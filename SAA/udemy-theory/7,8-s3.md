## S3

<br>

- s3는 `무한 확장 스토리` -> EBS처럼 따로 용량을 설정 안해도됨

-----------------
## s3 사용 케이스

- 백업 또는 일반 저장 스토리지
- 재해복구 스토리지
- 아카이브
- 하이브리드 클라우드 스토리지
- 애플리케이션 호스팅
- 미디어 호스팅 -> s3에서 영상도 볼 수 있음
- 데이터 레이크, 빅데이터 분석용
- 소프트웨어 배포
- 정적 웹사이트

### `핵심은 객체를 저장하는 것임!!!!`

<br>
<br>

----------------

## s3 버킷 특징

- S3는 오브젝트(파일)이 버킷(디렉토리)에 저장 된다는 것이다
- 버킷 이름은 전역적으로 고유해야 된다. -> 웹사이트 도메인 같은 것임
- 버킷은 `리전 수준`에서 정의해야된다
  - s3 버킷은 `특정지역(리전)에 생성`되는 것이다!!!!!
- 객체를 저장하기 위해서는 키가 필요하고, `키는 객체의 전체 경로`를 나타낸다.
- 실제 s3에는 디렉터리라는 개념은 없고, 슬러쉬를 포함한 아주 긴 이름일 뿐이다 (UI에서 보기에는)
- 객체의 최대 크기는 `5TB`이다. (스노우 패밀리 사용시...)
- 또한 `5기가가 넘는 파일은 여러부분으로 나누어서 업로드 해야` 된다.
- S3 ACL 버킷에 넣은 객체의 `소유권을 설정` 할 수 있다
- www에서 엑세스 가능하고, 정적 웹호스팅 하기 좋다.

<br>
<br>

------

## S3 버킷 정책

<br>

- 왜 객체 URL로 이미지가 안열리나?
  - 퍼블릭 엑세스가 차단되서 열리지 않음,
  - 그냥 열었을때는 서명된 url을 이용해서 열엇음(`aws 자격증명이 포함된 url임`)

<br>
<br>

-----------


## s3 버킷 보안
- 사용자 기반의 보안 (iam 사용)
- 리소스 기반 -> 버킷 자체에 보안 정책을 적용 할 수 있음
  - `오브젝트 기반`의 ACL 엑시스 컨트롤 리스트
  - `버킷 기반`의 ACL 엑시스 컨트롤 리스트
  - s3버킷에는 사용자 기반의 보안이든 리소스 기반의 보안이든, `Deny(거부는) 실행 할 수 없음` (기본적으로 다 Deny이기 때문)
- S3에 저장할때, 암호화 가능
- 다른 계정의 IAM으로도, 내 S3 버킷 정책으로 접근 가능함 

<br>
<br>


---------
## s3 객체의 버전관리
- 버켓 수준에서 버젼관리가 가능 -> 객체를 업데이트 할때마다
- 기존것들을 덮어 씌우기 하지 않음 
- 의도치 않은 삭제를 방지, 복구 할 수 있음
- 기존 파일은 버젼이 `null`로 표시됨

<br>
<br>



-------------
## s3 엑세스 로깅

- s3에 접근한 로그들을 다른 버킷에 저장해 분석 할 수 있음

<br>
<br>


-------------------------
## s3 리플리케이션 (복제)

- 리전간 복제인 CRR (cross-region replication) - 크로스 리전 리플리케이션
- 같은 리전 복제인 SRR (same-region replication) - 세임 리전 리플리케이션


<br>
<br>


- S3 복제 프로세싱은 기본적으로 `비동기 방식`으로 진행됨,
버켓 복제를 위해서는 소스 버킷과, 대상 버킷에 S3 `버저닝`이 무조건 활성화 되어 있어야함!!!!!!!!!!


<br>
<br>


`CRR 케이스` <br>
한국 리전 -> 미국 리전 = 한국에 개쩌는 서비스가 있고 미국에 진출하려면 `짧은 엑세스 시간을 위해` 미국에도 똑같은 데이터 s3가 있엇으면 한다

<br>
<br>



`SRR 케이스` <br>
-> 프로덕션 과 테스트 계정간의 s3를 복제해서 `데이터의 이중화 또는 다른 워크로드로 데이터를 확인`하려고 하는것임!!!!


<br>
<br>


-------------------------------------------------

## s3 내구성과 가용성 개념

`내구성` <br>
s3로 인해 객체가 손실 될 수 있는 확률, 아마존에는 내구성 99.9999999999999999%라고 주장함
<br>
천만개 객체가 있을때, 만년후 하나의 객체를 읽어버릴 확률이라고 주장함
또한 모든 스토리지 클래스의 내구성은 동일함

<br><br>

`유효성, 가용성` <br>
얼마나 쉽게 또는 장애없이 서비스를 이용할 수 있냐... <br>
이건 스토리지 클래스 마다 다름. <br>
s3 스탠다드는 99.99%로 , 약 1년에 53분동안 서비스를 이용 할 수 없다<br>
이는 서비스를 처리 할 때, 오류가 생길수도 있다는 의미


<br><br>

----------------------------

## s3 스토리지 클래스들

<br><br>

`s3 스탠다드` <br>
- 자주 액세스 하는 데이터 용
- 지연시간이 짧고, 처리량이 많음
- 빅데이터 분석과, 모바일 게임 애플리케이션 등 쓰이기 쉬움, 콘텐츠 배포
- 유효성,가용성은 99.99%

<br><br><br>

`s3 infrequent access` (ia) - 드문 엑세스
- 자주 엑세스 하지는 않지만, 필요할 때 빠르게 액세스 해야되는 데이터에 적합
- 요금은 스탠다드 보다 저렴하지만, 검색 요금이 추가로 발생
- 그리고 유효성,가용성은  스탠다드보다 약간 낮음 99.9% 
- 재해 복구와 백업에 이상적임

<br><br><br>

`s3 one zone-infrequent access`
- 위 Ia랑 똑같고, 하나의 az에만 있는 저장소
- 내구성은 99.999999%로 높지만 가용영역이 파괴되면 데이터 손실됨
- 가용성은 99.5%로 일반 ia보다 더 낮네;;;
- 온프레미스 데이터나 다시 생성 가능한 데이터의 보조 백업 복사본 저장에 적합함

<br><br><br>

`s3 글래셔` <br>
아카이브와 백업에 적합한 저비용 스토리지,스토리지 비용에 검색 비용이 포함됨!!!!
  <br><br>

  - `s3 glacier instant retrieval` - 글래셔 즉시 검색
    - 밀리초 단위의 검색이 가능
    - 분기에 한번 엑세스 할때 적합
    - 최소 스토리지 유지기간은 90일

  - `s3 glacier flexible retrieval`
    - 유연한 3가지 검색 기능을 제공 
    - 1분에서 5분의 빠른검색 - expedited
    - 3~5시간의 검색 - standard
    - 5~12시간 검색 - bulk 무료
    - 최소 스토리지 유지기간은 90일

  - `s3 glacier deep archive`
    - 장기보관 스토리지
    - 12시간 검색 - standard
    - 48시간 검색 - bulk
    - 최소 유지기간은 180일

<br><br>

`s3 intelligent tiering`
- 사용자 패턴을 기반으로 엑세스 계층간에 객체를 이동시킴
- `매월 모니터링과 자동화 요금이 발생`
- `검색 요금은 발생 X`
- frequent access 계층에 자동으로 저장되고, 30일동안 엑세스 하지 않은 객체는 infrequent access 계층으로 이동됨
- 90일동안 액세스 하지 않으면, 아카이브 인스탄스 계층으로 이동함
- 90일에서 700+일 이면 아카이브 엑세스 티어로 이동
- 180일에서 700+일이면 딥아카이스 엑세스 티어로 이동

<br><br>


-------------------------
## S3 수명 주기

- 수명주기는 일부 버킷 경로에만 적용 가능 함 (와일드 카드 이용해서)
- `객체의 태그`를 매칭을 통해서도 적용 가능



`S3 Analytis` 분석기
- `객체를 (생성날짜, 기간) 분석해 최적의 스토리지를 추천해줌`
- `s3 standard와 standard IA`에서 권장 사항
- one-zone ia 와 glacier에서는 해당 안됨
- 보고서는 매일 업데이트됨
- s3 분석까지 `24~48`시간 걸림
- csv 파일로 떨굼



-----------------------------------------

## aws s3 - Requester pays 요청자 지불

- 원래 S3 비용은, 내 버킷 디스크 크기,다운받을때 트래픽 비용으로 비용이 청구 됨
- `S3 다운로드 트래픽 비용`을, 버킷 공유받은 계정에서 냄
- 대량의 데이터를 AWS에서 공유할때 사용
- `요청자가 익명이여서는 안됨` (AWS 인증받은 사람이여야함)

![Alt text](../../etc/image2/s3%EC%9A%94%EC%B2%AD%EC%9E%90%EC%A7%80%EB%B6%88.png)




---------------------------------------------------------

## AWS s3 - 이벤트 알림

이벤트 생성
- 객체 생성, 제거, 복원, 복제, (이름 필터링 가능)
- 원하는 만큼 만들고, 원하는 대상으로 send 가능
- 이벤트 전송에 초~1분도 걸림
- `AWS Event Bridge`와 통합도 가능
  - 이벤트 브릿지와 함께라면, `고급 필터링도 가능`
- 단, s3 이벤트를 다른 서비스로(SQS나 SNS 등) 전송하려면 `해당 서비스의 정책 연결`해줘야함

![Alt text](../../etc/image2/s3%EC%9D%B4%EB%B2%A4%ED%8A%B8%EC%95%8C%EB%A6%BC.png)


----------------------------------------------

## AWS S3 퍼포먼스

- S3는 요청이 아주 많을때, 자동적으로 확장됨
- 접두사당 -> 초당 3500개의 put/copy/post/delete, 5500개의 GET/HEAD 요청을 지원한다 
  - /temp/file* 같이 prefix 했을떄 객체 찾는 것
  - 만약 아래와같이 GET/HEAD 요청을 동시에 하면, 초당 22,000개의 GET/HEAD 요청을 처리한다
    - /temp/file1*
    - /temp/file2*
    - /temp/file3*
    - /temp/file4*



S3 multi-part upload
-` AWS CLI 또는 고수준 API`로 S3에 업로드할 경우 멀티파트 업로드 기능이 `자동`으로 적용되지만, 저수준 API로 업로드 작업을 수행할 경우 `직접 객체를 여러 부분으로 분할`할 수 있다.
- `100MB 이상`의 파일은 멀티파트로 업로드 - `권장사항`
- 5GB 넘는 파일은 `반드시` 멀티 파트업로드 사용 해야됨

S3 Transger Acceleration
- 파일을 AWS `엣지 로케이션`으로 전송해서 속도를 높임,이 후 데이터를 `대상 리전` S3에 전달
- 멀티파트 업로드와 `같이 사용 가능`


`S3 Byte-Range Fetches`(바이트 범위 가져오기)
- 파일에서 특정 바이트 범위를 가져와서 GET요청을 병렬화 하는 방법
- 특정 바이트 범위를 실패해도, 더작은 바이트범위에서 재시도함
- 사례
  - 다운로드 속도를 높일때 사용함 (`병렬 바이트 나눠받기`)
  - 파일 검색 (파일의 앞부분 바이트 체크)




-------------------------------------------
## AWS S3 select & Glacier select

- S3에서 파일을 검색할때, 그냥하면 너무 많은 데이터를 검색하게됨, `SQL 문법을 이용해 필터링 할 수 있음`
- S3 select는 이전에서 모든데이터를 검색한 다음 애플리케이션측에서 필터링을 통해 필요한걸 찾음
  
------------------
## AWS Batch operations

- 단일 요청으로 S3 객체에서 대량 작업을 수행하는 서비스
- 예) 
  - 모든 객체의 메타데이터와 속성을 수정
  - `암호화 되지않은 모든 객체를 암호화`
  - 특정 데이터 복사, 복원
  - ACL이나, tag 수정
  - 람다 함수를 호출해, 사용자 지정작업도 가능함


배치 작업 플로우
1. `S3 inventory`라는 기능을 이용해, 배치작업에 대상이 되는 `객체 목록`을 가져옴
2. S3 select를 통해 객체 필터링
3. Batch 오퍼레이션을 통해 오브젝트 처리


![Alt text](../../etc/image2/s3%EB%B0%B0%EC%B9%98.png)





















