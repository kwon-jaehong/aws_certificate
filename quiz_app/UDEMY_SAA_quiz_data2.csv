exam_category,exam_type,quiz_id,en_quiz_title,most_vote,correct,en_A,en_B,en_C,en_D,en_E,en_F,en_G,ko_quiz_title,ko_A,ko_B,ko_C,ko_D,ko_E,ko_G,subject,choice_count,ko_F
udemy,CLF-01,1,A company is using a combination of API Gateway and Lambda for the web services of the online web portal that is being accessed by hundreds of thousands of clients each day. They will be announcing a new revolutionary product and it is expected that the web portal will receive a massive number of visitors all around the globe.How can you protect the backend systems and applications from traffic spikes?,B,B,Deploy Multi-AZ in API Gateway with Read Replica,Use throttling limits in API Gateway,Manually upgrade the EC2 instances being used by API Gateway,API Gateway will automatically scale and handle massive traffic spikes so you do not have to do anything.,,,,한 회사에서 매일 수십만 명의 클라이언트가 액세스하는 온라인 웹 포털의 웹 서비스를 위해 API 게이트웨이와 Lambda의 조합을 사용하고 있습니다. 그들은 새롭고 혁신적인 제품을 발표할 예정이며 웹 포털은 전 세계적으로 엄청난 수의 방문자를 받을 것으로 예상됩니다.트래픽 급증으로부터 백엔드 시스템과 애플리케이션을 어떻게 보호할 수 있습니까?,읽기 전용 복제본으로 API Gateway에 다중 AZ 배포,API Gateway에서 제한 제한 사용,API Gateway에서 사용 중인 EC2 인스턴스를 수동으로 업그레이드,API Gateway는 대규모 트래픽 급증을 자동으로 확장하고 처리하므로 아무 것도 할 필요가 없습니다.,,,0,,
udemy,CLF-01,2,"A company collects atmospheric data such as temperature, air pressure, and humidity from different countries. Each site location is equipped with various weather instruments and a high-speed Internet connection. The average collected data in each location is around 500 GB and will be analyzed by a weather forecasting application hosted in Northern Virginia. As the Solutions Architect, you need to aggregate all the data in the fastest way.Which of the following options can satisfy the given requirement?",A,A,Enable Transfer Acceleration in the destination bucket and upload the collected data using Multipart Upload.,Set up a Site-to-Site VPN connection.,Upload the data to the closest S3 bucket. Set up a cross-region replication and copy the objects to the destination bucket.,Use AWS Snowball Edge to transfer large amounts of data.,,,,"한 회사가 여러 국가에서 온도, 기압 및 습도와 같은 대기 데이터를 수집합니다. 각 사이트 위치에는 다양한 기상 장비와 고속 인터넷 연결이 장착되어 있습니다. 각 위치에서 수집된 평균 데이터는 약 500GB이며 북부 버지니아에서 호스팅되는 일기 예보 애플리케이션에서 분석됩니다. 솔루션 설계자는 모든 데이터를 가장 빠른 방법으로 집계해야 합니다.다음 옵션 중 주어진 요구 사항을 충족할 수 있는 옵션은 무엇입니까?",대상 버킷에서 Transfer Acceleration을 활성화하고 수집된 데이터를 멀티파트 업로드를 사용하여 업로드합니다.,사이트 간 VPN 연결을 설정합니다.,가장 가까운 S3 버킷에 데이터를 업로드합니다. 지역 간 복제를 설정하고 개체를 대상 버킷에 복사합니다.,AWS Snowball Edge를 사용하여 대량의 데이터를 전송하십시오.,,,0,,
udemy,CLF-01,3,A Solutions Architect needs to make sure that the On-Demand EC2 instance can only be accessed from this IP address (110.238.98.71) via an SSH connection. Which configuration below will satisfy this requirement?,D,D,"Security Group Outbound Rule: Protocol – UDP, Port Range – 22, Destination 0.0.0.0/0","Security Group Inbound Rule: Protocol – UDP, Port Range – 22, Source 110.238.98.71/32","Security Group Outbound Rule: Protocol – TCP, Port Range – 22, Destination 110.238.98.71/32","Security Group Inbound Rule: Protocol – TCP. Port Range – 22, Source 110.238.98.71/32",,,,Solutions Architect는 SSH 연결을 통해서만 이 IP 주소(110.238.98.71)에서만 온디맨드 EC2 인스턴스에 액세스할 수 있는지 확인해야 합니다. 아래에서 이 요구 사항을 충족하는 구성은 무엇입니까?,"보안 그룹 아웃바운드 규칙: 프로토콜 – UDP, 포트 범위 – 22, 대상 0.0.0.0/0","보안 그룹 인바운드 규칙: 프로토콜 – UDP, 포트 범위 – 22, 소스 110.238.98.71/32","보안 그룹 아웃바운드 규칙: 프로토콜 – TCP, 포트 범위 – 22, 대상 110.238.98.71/32","보안 그룹 인바운드 규칙: 프로토콜 – TCP. 포트 범위 – 22, 소스 110.238.98.71/32",,,0,,
udemy,CLF-01,4,"A company uses an Application Load Balancer (ALB) for its public-facing multi-tier web applications. The security team has recently reported that there has been a surge of SQL injection attacks lately, which causes critical data discrepancy issues. The same issue is also encountered by its other web applications in other AWS accounts that are behind an ALB. An immediate solution is required to prevent the remote injection of unauthorized SQL queries and protect their applications hosted across multiple accounts.As a Solutions Architect, what solution would you recommend?",B,B,Use Amazon Macie to scan for vulnerabilities and unintended network exposure. Refactor the web application to be less susceptible to SQL injection attacks based on the security assessment. Utilize the AWS Audit Manager to reuse the security assessment across all AWS accounts.,"Use AWS WAF and set up a managed rule to block request patterns associated with the exploitation of SQL databases, like SQL injection attacks. Associate it with the Application Load Balancer. Integrate AWS WAF with AWS Firewall Manager to reuse the rules across all the AWS accounts.",Use AWS Network Firewall to filter web vulnerabilities and brute force attacks using stateful rule groups across all Application Load Balancers on all AWS accounts. Refactor the web application to be less susceptible to SQL injection attacks based on the security assessment.,"Use Amazon GuardDuty and set up a managed rule to block request patterns associated with the exploitation of SQL databases, like SQL injection attacks. Associate it with the Application Load Balancer and utilize the AWS Security Hub service to reuse the managed rules across all the AWS accounts",,,,회사는 공용 다중 계층 웹 애플리케이션에 ALB(Application Load Balancer)를 사용합니다. 보안팀은 최근 SQL 주입 공격이 급증하여 중요한 데이터 불일치 문제가 발생했다고 보고했습니다. ALB 뒤에 있는 다른 AWS 계정의 다른 웹 애플리케이션에서도 동일한 문제가 발생합니다. 무단 SQL 쿼리의 원격 주입을 방지하고 여러 계정에서 호스팅되는 애플리케이션을 보호하려면 즉각적인 솔루션이 필요합니다.솔루션 아키텍트로서 어떤 솔루션을 추천하시겠습니까?,Amazon Macie를 사용하여 취약성 및 의도하지 않은 네트워크 노출을 스캔합니다. 보안 평가를 기반으로 SQL 삽입 공격에 덜 취약하도록 웹 애플리케이션을 리팩터링합니다. AWS Audit Manager를 활용하여 모든 AWS 계정에서 보안 평가를 재사용합니다.,AWS WAF를 사용하고 관리형 규칙을 설정하여 SQL 주입 공격과 같은 SQL 데이터베이스 악용과 관련된 요청 패턴을 차단합니다. Application Load Balancer와 연결합니다. AWS WAF를 AWS Firewall Manager와 통합하여 모든 AWS 계정에서 규칙을 재사용합니다.,AWS Network Firewall을 사용하여 모든 AWS 계정의 모든 Application Load Balancer에서 상태 저장 규칙 그룹을 사용하여 웹 취약성과 무차별 암호 대입 공격을 필터링합니다. 보안 평가를 기반으로 SQL 삽입 공격에 덜 취약하도록 웹 애플리케이션을 리팩터링합니다.,Amazon GuardDuty를 사용하고 관리형 규칙을 설정하여 SQL 주입 공격과 같은 SQL 데이터베이스 악용과 관련된 요청 패턴을 차단합니다. 이를 Application Load Balancer와 연결하고 AWS Security Hub 서비스를 활용하여 모든 AWS 계정에서 관리형 규칙을 재사용합니다.,,,0,,
udemy,CLF-01,5,"There was an incident in your production environment where the user data stored in the S3 bucket has been accidentally deleted by one of the Junior DevOps Engineers. The issue was escalated to your manager and after a few days, you were instructed to improve the security and protection of your AWS resources.    What combination of the following options will protect the S3 objects in your bucket from both accidental deletion and overwriting? (Select TWO.)",CE,CE,Provide access to S3 data strictly through pre-signed URL only,Enable Amazon S3 Intelligent-Tiering,Enable Versioning,Disallow S3 Delete using an IAM bucket policy,Enable Multi-Factor Authentication Delete,,,프로덕션 환경에서 주니어 DevOps 엔지니어 중 한 명이 실수로 S3 버킷에 저장된 사용자 데이터를 삭제한 사고가 발생했습니다. 이 문제는 관리자에게 에스컬레이션되었으며 며칠 후 AWS 리소스의 보안 및 보호를 개선하라는 지시를 받았습니다.    버킷의 S3 객체를 우발적인 삭제 및 덮어쓰기로부터 보호하는 다음 옵션의 조합은 무엇입니까? (2개를 선택하세요.),사전 서명된 URL을 통해서만 S3 데이터에 대한 액세스를 엄격하게 제공합니다.,Amazon S3 Intelligent-Tiering 활성화,버전 관리 활성화,IAM 버킷 정책을 사용하여 S3 삭제를 허용하지 않음,다단계 인증 삭제 활성화,,0,,
udemy,CLF-01,6,"A company conducted a surprise IT audit on all of the AWS resources being used in the production environment. During the audit activities, it was noted that you are using a combination of Standard and Convertible Reserved EC2 instances in your applications.Which of the following are the characteristics and benefits of using these two types of Reserved EC2 instances? (Select TWO.)",AB,AB,Unused Standard Reserved Instances can later be sold at the Reserved Instance Marketplace.,Convertible Reserved Instances allow you to exchange for another convertible reserved instance of a different instance family.,It runs in a VPC on hardware that's dedicated to a single customer.,It can enable you to reserve capacity for your Amazon EC2 instances in multiple Availability Zones and multiple AWS Regions for any duration.,Unused Convertible Reserved Instances can later be sold at the Reserved Instance Marketplace.,,,한 회사가 프로덕션 환경에서 사용 중인 모든 AWS 리소스에 대해 기습 IT 감사를 실시했습니다. 감사 활동 중에 애플리케이션에서 표준 및 전환형 예약 EC2 인스턴스의 조합을 사용하고 있다는 사실이 확인되었습니다.다음 중 이 두 가지 유형의 예약 EC2 인스턴스를 사용할 때의 특징과 이점은 무엇입니까? (2개를 선택하세요.),미사용 표준 예약 인스턴스는 나중에 예약 인스턴스 마켓플레이스에서 판매할 수 있습니다.,전환형 예약 인스턴스를 사용하면 다른 인스턴스 패밀리의 다른 전환형 예약 인스턴스로 교환할 수 있습니다.,단일 고객 전용 하드웨어의 VPC에서 실행됩니다.,기간에 관계없이 여러 가용 영역과 여러 AWS 리전에서 Amazon EC2 인스턴스의 용량을 예약할 수 있습니다.,사용하지 않은 전환형 예약 인스턴스는 나중에 예약 인스턴스 마켓플레이스에서 판매할 수 있습니다.,,0,,
udemy,CLF-01,7,"A Docker application, which is running on an Amazon ECS cluster behind a load balancer, is heavily using DynamoDB. You are instructed to improve the database performance by distributing the workload evenly and using the provisioned throughput efficiently.    Which of the following would you consider to implement for your DynamoDB table?",A,A,"Use partition keys with high-cardinality attributes, which have a large number of distinct values for each item.","Avoid using a composite primary key, which is composed of a partition key and a sort key.","Use partition keys with low-cardinality attributes, which have a few number of distinct values for each item.",Reduce the number of partition keys in the DynamoDB table.,,,,로드 밸런서 뒤의 Amazon ECS 클러스터에서 실행 중인 Docker 애플리케이션은 DynamoDB를 많이 사용하고 있습니다. 워크로드를 고르게 분산하고 프로비저닝된 처리량을 효율적으로 사용하여 데이터베이스 성능을 개선하라는 지시를 받았습니다.    다음 중 DynamoDB 테이블에 대해 구현할 것을 고려하는 것은 무엇입니까?,각 항목에 대해 고유한 값이 많은 높은 카디널리티 속성이 있는 파티션 키를 사용합니다.,파티션 키와 정렬 키로 구성된 복합 기본 키를 사용하지 마십시오.,각 항목에 대해 몇 가지 고유 값이 있는 낮은 카디널리티 속성이 있는 파티션 키를 사용합니다.,DynamoDB 테이블의 파티션 키 수를 줄입니다.,,,0,,
udemy,CLF-01,8,"A pharmaceutical company has resources hosted on both their on-premises network and in AWS cloud. They want all of their Software Architects to access resources on both environments using their on-premises credentials, which is stored in Active Directory. In this scenario, which of the following can be used to fulfill this requirement?",D,D,Set up SAML 2.0-Based Federation by using a Web Identity Federation.,Use IAM users,Use Amazon VPC,Set up SAML 2.0-Based Federation by using a Microsoft Active Directory Federation Service (AD FS).,,,,제약 회사는 온프레미스 네트워크와 AWS 클라우드 모두에서 리소스를 호스팅하고 있습니다. 그들은 모든 소프트웨어 설계자가 Active Directory에 저장된 온프레미스 자격 증명을 사용하여 두 환경의 리소스에 액세스하기를 원합니다.이 시나리오에서 다음 중 이 요구 사항을 충족하는 데 사용할 수 있는 것은 무엇입니까?,웹 ID 페더레이션을 사용하여 SAML 2.0 기반 페더레이션을 설정합니다.,IAM 사용자 사용,아마존 VPC 사용,Microsoft AD FS(Active Directory Federation Service)를 사용하여 SAML 2.0 기반 페더레이션을 설정합니다.,,,0,,
udemy,CLF-01,9,A payment processing company plans to migrate its on-premises application to an Amazon EC2 instance. An IPv6 CIDR block is attached to the company’s Amazon VPC. Strict security policy mandates that the production VPC must only allow outbound communication over IPv6 between the instance and the internet but should prevent the internet from initiating an inbound IPv6 connection. The new architecture should also allow traffic flow inspection and traffic filtering.What should a solutions architect do to meet these requirements?,A,A,Launch the EC2 instance to a private subnet and attach an Egress-Only Internet Gateway to the VPC to allow outbound IPv6 communication to the internet. Use AWS Network Firewall to set up the required rules for traffic inspection and traffic filtering.,Launch the EC2 instance to a private subnet and attach a NAT Gateway to the VPC to allow outbound IPv6 communication to the internet. Use AWS Firewall Manager to set up the required rules for traffic inspection and traffic filtering.,Launch the EC2 instance to a public subnet and attach an Internet Gateway to the VPC to allow outbound IPv6 communication to the internet. Use Traffic Mirroring to set up the required rules for traffic inspection and traffic filtering.,Launch the EC2 instance to a private subnet and attach AWS PrivateLink interface endpoint to the VPC to control outbound IPv6 communication to the internet. Use Amazon GuardDuty to set up the required rules for traffic inspection and traffic filtering.,,,,결제 처리 회사는 온프레미스 애플리케이션을 Amazon EC2 인스턴스로 마이그레이션할 계획입니다. IPv6 CIDR 블록은 회사의 Amazon VPC에 연결됩니다. 엄격한 보안 정책에 따르면 프로덕션 VPC는 ​​인스턴스와 인터넷 간에 IPv6를 통한 아웃바운드 통신만 허용해야 하지만 인터넷이 인바운드 IPv6 연결을 시작하는 것은 방지해야 합니다. 새로운 아키텍처는 또한 트래픽 흐름 검사 및 트래픽 필터링을 허용해야 합니다.솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까?,EC2 인스턴스를 프라이빗 서브넷으로 시작하고 외부 전용 인터넷 게이트웨이를 VPC에 연결하여 인터넷에 대한 아웃바운드 IPv6 통신을 허용합니다. AWS Network Firewall을 사용하여 트래픽 검사 및 트래픽 필터링에 필요한 규칙을 설정합니다.,EC2 인스턴스를 프라이빗 서브넷으로 시작하고 NAT 게이트웨이를 VPC에 연결하여 인터넷에 대한 아웃바운드 IPv6 통신을 허용합니다. AWS Firewall Manager를 사용하여 트래픽 검사 및 트래픽 필터링에 필요한 규칙을 설정합니다.,EC2 인스턴스를 퍼블릭 서브넷으로 시작하고 인터넷 게이트웨이를 VPC에 연결하여 인터넷에 대한 아웃바운드 IPv6 통신을 허용합니다. 트래픽 미러링을 사용하여 트래픽 검사 및 트래픽 필터링에 필요한 규칙을 설정합니다.,EC2 인스턴스를 프라이빗 서브넷으로 시작하고 AWS PrivateLink 인터페이스 엔드포인트를 VPC에 연결하여 인터넷에 대한 아웃바운드 IPv6 통신을 제어합니다. Amazon GuardDuty를 사용하여 트래픽 검사 및 트래픽 필터링에 필요한 규칙을 설정합니다.,,,0,,
udemy,CLF-01,10,A popular social network is hosted in AWS and is using a DynamoDB table as its database. There is a requirement to implement a 'follow' feature where users can subscribe to certain updates made by a particular user and be notified via email. Which of the following is the most suitable solution that you should implement to meet the requirement?,C,C,"Using the Kinesis Client Library (KCL), write an application that leverages on DynamoDB Streams Kinesis Adapter that will fetch data from the DynamoDB Streams endpoint. When there are updates made by a particular user, notify the subscribers via email using SNS.","Set up a DAX cluster to access the source DynamoDB table. Create a new DynamoDB trigger and a Lambda function. For every update made in the user data, the trigger will send data to the Lambda function which will then notify the subscribers via email using SNS.","Enable DynamoDB Stream and create an AWS Lambda trigger, as well as the IAM role which contains all of the permissions that the Lambda function will need at runtime. The data from the stream record will be processed by the Lambda function which will then publish a message to SNS Topic that will notify the subscribers via email.",Create a Lambda function that uses DynamoDB Streams Kinesis Adapter which will fetch data from the DynamoDB Streams endpoint. Set up an SNS Topic that will notify the subscribers via email when there is an update made by a particular user.,,,,널리 사용되는 소셜 네트워크는 AWS에서 호스팅되며 DynamoDB 테이블을 데이터베이스로 사용하고 있습니다. 사용자가 특정 사용자가 만든 특정 업데이트를 구독하고 이메일을 통해 알림을 받을 수 있는 '팔로우' 기능을 구현하기 위한 요구 사항이 있습니다. 다음 중 요구 사항을 충족하기 위해 구현해야 하는 가장 적합한 솔루션은 무엇입니까?,Kinesis 클라이언트 라이브러리(KCL)를 사용하여 DynamoDB 스트림 엔드포인트에서 데이터를 가져오는 DynamoDB 스트림 Kinesis 어댑터를 활용하는 애플리케이션을 작성합니다. 특정 사용자의 업데이트가 있는 경우 SNS를 사용하여 이메일을 통해 구독자에게 알립니다.,원본 DynamoDB 테이블에 액세스하도록 DAX 클러스터를 설정합니다. 새 DynamoDB 트리거와 Lambda 함수를 생성합니다. 사용자 데이터가 업데이트될 때마다 트리거는 데이터를 Lambda 함수로 전송한 다음 SNS를 사용하여 이메일을 통해 구독자에게 알립니다.,DynamoDB 스트림을 활성화하고 AWS Lambda 트리거와 Lambda 함수가 런타임에 필요한 모든 권한을 포함하는 IAM 역할을 생성합니다. 스트림 레코드의 데이터는 Lambda 함수에 의해 처리된 후 이메일을 통해 구독자에게 알리는 SNS 주제에 메시지를 게시합니다.,DynamoDB 스트림 엔드포인트에서 데이터를 가져오는 DynamoDB 스트림 Kinesis 어댑터를 사용하는 Lambda 함수를 생성합니다. 특정 사용자의 업데이트가 있을 때 이메일을 통해 구독자에게 알리는 SNS 주제를 설정합니다.,,,0,,
udemy,CLF-01,11,"An online learning company hosts its Microsoft .NET e-Learning application on a Windows Server in its on-premises data center. The application uses an Oracle Database Standard Edition as its backend database.The company wants a high-performing solution to migrate this workload to the AWS cloud to take advantage of the cloud’s high availability. The migration process should minimize development changes, and the environment should be easier to manage.Which of the following options should be implemented to meet the company requirements? (Select TWO.)",CD,CD,Refactor the application to .NET Core and run it as a serverless container service using Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate.,Provision and replatform the application to Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes. Use the Windows Server Amazon Machine Image (AMI) and deploy the .NET application using to the ECS cluster via the Amazon ECS Anywhere service.,Rehost the on-premises .NET application to an AWS Elastic Beanstalk Multi-AZ environment which runs in multiple Availability Zones.,Migrate the Oracle database to Amazon RDS for Oracle in a Multi-AZ deployment by using AWS Database Migration Service (AWS DMS).,Use AWS Application Migration Service (AWS MGN) to migrate the on-premises Oracle database server to a new Amazon EC2 instance.,,,온라인 학습 회사는 온프레미스 데이터 센터의 Windows Server에서 Microsoft .NET e-Learning 애플리케이션을 호스팅합니다. 이 애플리케이션은 Oracle Database Standard Edition을 백엔드 데이터베이스로 사용합니다.회사는 클라우드의 고가용성을 활용하기 위해 이 워크로드를 AWS 클라우드로 마이그레이션하는 고성능 솔루션을 원합니다. 마이그레이션 프로세스는 개발 변경을 최소화해야 하며 환경은 관리하기 쉬워야 합니다.다음 중 회사 요구 사항을 충족하기 위해 구현해야 하는 옵션은 무엇입니까? (2개를 선택하세요.),애플리케이션을 .NET Core로 리팩터링하고 AWS Fargate와 함께 Amazon Elastic Kubernetes Service(Amazon EKS)를 사용하여 서버리스 컨테이너 서비스로 실행합니다.,Amazon EC2 작업자 노드를 사용하여 애플리케이션을 Amazon Elastic Container Service(Amazon ECS)에 프로비저닝하고 플랫폼을 변경합니다. Windows Server Amazon 머신 이미지(AMI)를 사용하고 Amazon ECS Anywhere 서비스를 통해 ECS 클러스터에 .NET 애플리케이션을 배포합니다.,온프레미스 .NET 애플리케이션을 여러 가용 영역에서 실행되는 AWS Elastic Beanstalk 다중 AZ 환경으로 다시 호스팅합니다.,AWS Database Migration Service(AWS DMS)를 사용하여 다중 AZ 배포에서 Oracle 데이터베이스를 Oracle용 Amazon RDS로 마이그레이션합니다.,AWS Application Migration Service(AWS MGN)를 사용하여 온프레미스 Oracle 데이터베이스 서버를 새로운 Amazon EC2 인스턴스로 마이그레이션합니다.,,0,,
udemy,CLF-01,12,An online shopping platform is hosted on an Auto Scaling group of Spot EC2 instances and uses Amazon Aurora PostgreSQL as its database. There is a requirement to optimize your database workloads in your cluster where you have to direct the write operations of the production traffic to your high-capacity instances and point the reporting queries sent by your internal staff to the low-capacity instances. Which is the most suitable configuration for your application as well as your Aurora database cluster to achieve this requirement?,C,C,"Configure your application to use the reader endpoint for both production traffic and reporting queries, which will enable your Aurora database to automatically perform load-balancing among all the Aurora Replicas.","In your application, use the instance endpoint of your Aurora database to handle the incoming production traffic and use the cluster endpoint to handle reporting queries.",Create a custom endpoint in Aurora based on the specified criteria for the production traffic and another custom endpoint to handle the reporting queries.,"Do nothing since by default, Aurora will automatically direct the production traffic to your high-capacity instances and the reporting queries to your low-capacity instances.",,,,온라인 쇼핑 플랫폼은 Spot EC2 인스턴스의 Auto Scaling 그룹에서 호스팅되며 Amazon Aurora PostgreSQL을 데이터베이스로 사용합니다. 프로덕션 트래픽의 쓰기 작업을 고용량 인스턴스로 지정하고 내부 직원이 보낸 보고 쿼리를 저용량 인스턴스로 지정해야 하는 클러스터에서 데이터베이스 워크로드를 최적화해야 한다는 요구 사항이 있습니다.이 요구 사항을 충족하기 위해 애플리케이션과 Aurora 데이터베이스 클러스터에 가장 적합한 구성은 무엇입니까?,프로덕션 트래픽과 보고 쿼리 모두에 대해 리더 엔드포인트를 사용하도록 애플리케이션을 구성하면 Aurora 데이터베이스가 모든 Aurora 복제본 간에 로드 밸런싱을 자동으로 수행할 수 있습니다.,애플리케이션에서 Aurora 데이터베이스의 인스턴스 엔드포인트를 사용하여 들어오는 프로덕션 트래픽을 처리하고 클러스터 엔드포인트를 사용하여 보고 쿼리를 처리합니다.,프로덕션 트래픽에 대해 지정된 기준에 따라 Aurora에서 사용자 지정 엔드포인트를 생성하고 보고 쿼리를 처리하기 위한 또 다른 사용자 지정 엔드포인트를 생성합니다.,"기본적으로 Aurora는 프로덕션 트래픽을 대용량 인스턴스로, 보고 쿼리를 저용량 인스턴스로 자동으로 전달하므로 아무 작업도 수행하지 마십시오.",,,0,,
udemy,CLF-01,13,"A startup is using Amazon RDS to store data from a web application. Most of the time, the application has low user activity but it receives bursts of traffic within seconds whenever there is a new product announcement. The Solutions Architect needs to create a solution that will allow users around the globe to access the data using an API.What should the Solutions Architect do meet the above requirement?",D,D,Create an API using Amazon API Gateway and use the Amazon ECS cluster with Service Auto Scaling to handle the bursts of traffic in seconds.,Create an API using Amazon API Gateway and use Amazon Elastic Beanstalk with Auto Scaling to handle the bursts of traffic in seconds.,Create an API using Amazon API Gateway and use an Auto Scaling group of Amazon EC2 instances to handle the bursts of traffic in seconds.,Create an API using Amazon API Gateway and use AWS Lambda to handle the bursts of traffic in seconds.,,,,스타트업에서 Amazon RDS를 사용하여 웹 애플리케이션의 데이터를 저장하고 있습니다. 대부분의 경우 애플리케이션은 사용자 활동이 적지만 신제품 발표가 있을 때마다 몇 초 안에 트래픽 폭주를 받습니다. Solutions Architect는 전 세계 사용자가 API를 사용하여 데이터에 액세스할 수 있는 솔루션을 만들어야 합니다.솔루션 아키텍트가 위의 요구 사항을 충족해야 하는 것은 무엇입니까?,Amazon API Gateway를 사용하여 API를 생성하고 Service Auto Scaling과 함께 Amazon ECS 클러스터를 사용하여 폭발적인 트래픽을 몇 초 만에 처리하십시오.,Amazon API Gateway를 사용하여 API를 생성하고 Auto Scaling과 함께 Amazon Elastic Beanstalk를 사용하여 트래픽 급증을 몇 초 만에 처리하십시오.,Amazon API Gateway를 사용하여 API를 생성하고 Amazon EC2 인스턴스의 Auto Scaling 그룹을 사용하여 폭발적인 트래픽을 몇 초 만에 처리합니다.,Amazon API Gateway를 사용하여 API를 생성하고 AWS Lambda를 사용하여 몇 초 안에 폭발적인 트래픽을 처리합니다.,,,0,,
udemy,CLF-01,14,"The company that you are working for has a highly available architecture consisting of an elastic load balancer and several EC2 instances configured with auto-scaling in three Availability Zones. You want to monitor your EC2 instances based on a particular metric, which is not readily available in CloudWatch.   Which of the following is a custom metric in CloudWatch which you have to manually set up?",B,B,CPU Utilization of an EC2 instance,Memory Utilization of an EC2 instance,Disk Reads activity of an EC2 instance,Network packets out of an EC2 instance,,,,귀하가 근무하고 있는 회사에는 3개의 가용 영역에서 자동 확장으로 구성된 여러 EC2 인스턴스와 탄력적 로드 밸런서로 구성된 고가용성 아키텍처가 있습니다. CloudWatch에서 쉽게 사용할 수 없는 특정 지표를 기반으로 EC2 인스턴스를 모니터링하려고 합니다.   다음 중 수동으로 설정해야 하는 CloudWatch의 사용자 지정 지표는 무엇입니까?,EC2 인스턴스의 CPU 사용률,EC2 인스턴스의 메모리 사용률,EC2 인스턴스의 디스크 읽기 활동,EC2 인스턴스에서 나가는 네트워크 패킷,,,0,,
udemy,CLF-01,15,"A company needs to deploy at least 2 EC2 instances to support the normal workloads of its application and automatically scale up to 6 EC2 instances to handle the peak load. The architecture must be highly available and fault-tolerant as it is processing mission-critical workloads.As the Solutions Architect of the company, what should you do to meet the above requirement?",B,B,Create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 6. Use 2 Availability Zones and deploy 1 instance for each AZ.,Create an Auto Scaling group of EC2 instances and set the minimum capacity to 4 and the maximum capacity to 6. Deploy 2 instances in Availability Zone A and another 2 instances in Availability Zone B.,Create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 6. Deploy 4 instances in Availability Zone A.,Create an Auto Scaling group of EC2 instances and set the minimum capacity to 2 and the maximum capacity to 4. Deploy 2 instances in Availability Zone A and 2 instances in Availability Zone B.,,,,회사는 애플리케이션의 일반 워크로드를 지원하기 위해 최소 2개의 EC2 인스턴스를 배포해야 하며 최대 로드를 처리하기 위해 최대 6개의 EC2 인스턴스를 자동으로 확장해야 합니다. 아키텍처는 미션 크리티컬 워크로드를 처리하므로 가용성이 높고 내결함성이 있어야 합니다.회사의 솔루션 설계자로서 위의 요구 사항을 충족하려면 어떻게 해야 합니까?,"EC2 인스턴스의 Auto Scaling 그룹을 생성하고 최소 용량을 2로, 최대 용량을 6으로 설정합니다. 2개의 가용 영역을 사용하고 각 AZ에 대해 1개의 인스턴스를 배포합니다.","EC2 인스턴스의 Auto Scaling 그룹을 생성하고 최소 용량을 4로, 최대 용량을 6으로 설정합니다. 가용 영역 A에 2개의 인스턴스를 배포하고 가용 영역 B에 또 다른 2개의 인스턴스를 배포합니다.","EC2 인스턴스의 Auto Scaling 그룹을 생성하고 최소 용량을 2로, 최대 용량을 6으로 설정합니다. 가용 영역 A에 인스턴스 4개를 배포합니다.","EC2 인스턴스의 Auto Scaling 그룹을 생성하고 최소 용량을 2로, 최대 용량을 4로 설정합니다. 가용 영역 A에 인스턴스 2개, 가용 영역 B에 인스턴스 2개를 배포합니다.",,,0,,
udemy,CLF-01,16,A company has a hybrid cloud architecture that connects their on-premises data center and cloud infrastructure in AWS. They require a durable storage backup for their corporate documents stored on-premises and a local cache that provides low latency access to their recently accessed data to reduce data egress charges. The documents must be stored to and retrieved from AWS via the Server Message Block (SMB) protocol. These files must immediately be accessible within minutes for six months and archived for another decade to meet the data compliance. Which of the following is the best and most cost-effective approach to implement in this scenario?,A,A,Launch a new file gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the file gateway and set up a lifecycle policy to move the data into Glacier for data archival.,Launch a new tape gateway that connects to your on-premises data center using AWS Storage Gateway. Upload the documents to the tape gateway and set up a lifecycle policy to move the data into Glacier for archival.,"Establish a Direct Connect connection to integrate your on-premises network to your VPC. Upload the documents on Amazon EBS Volumes and use a lifecycle policy to automatically move the EBS snapshots to an S3 bucket, and then later to Glacier for archival.",Use AWS Snowmobile to migrate all of the files from the on-premises network. Upload the documents to an S3 bucket and set up a lifecycle policy to move the data into Glacier for archival.,,,,회사는 온프레미스 데이터 센터와 AWS의 클라우드 인프라를 연결하는 하이브리드 클라우드 아키텍처를 보유하고 있습니다. 온프레미스에 저장된 회사 문서를 위한 내구성 있는 스토리지 백업과 데이터 송신 비용을 줄이기 위해 최근에 액세스한 데이터에 대한 짧은 대기 시간 액세스를 제공하는 로컬 캐시가 필요합니다. 문서는 SMB(Server Message Block) 프로토콜을 통해 AWS에 저장하고 AWS에서 검색해야 합니다. 이러한 파일은 6개월 동안 몇 분 안에 즉시 액세스할 수 있어야 하며 데이터 규정 준수를 위해 향후 10년 동안 보관해야 합니다.다음 중 이 시나리오에서 구현할 수 있는 가장 비용 효율적인 접근 방식은 무엇입니까?,AWS Storage Gateway를 사용하여 온프레미스 데이터 센터에 연결하는 새 파일 게이트웨이를 시작합니다. 문서를 파일 게이트웨이에 업로드하고 수명 주기 정책을 설정하여 데이터 보관을 위해 데이터를 Glacier로 이동합니다.,AWS Storage Gateway를 사용하여 온프레미스 데이터 센터에 연결하는 새로운 테이프 게이트웨이를 시작하십시오. 문서를 테이프 게이트웨이에 업로드하고 보관을 위해 데이터를 Glacier로 옮기는 수명 주기 정책을 설정합니다.,온프레미스 네트워크를 VPC에 통합하기 위해 Direct Connect 연결을 설정합니다. Amazon EBS 볼륨에 문서를 업로드하고 수명 주기 정책을 사용하여 EBS 스냅샷을 자동으로 S3 버킷으로 이동한 다음 나중에 보관을 위해 Glacier로 이동합니다.,AWS Snowmobile을 사용하여 온프레미스 네트워크에서 모든 파일을 마이그레이션합니다. 문서를 S3 버킷에 업로드하고 보관을 위해 데이터를 Glacier로 옮기는 수명 주기 정책을 설정합니다.,,,0,,
udemy,CLF-01,17,"A Forex trading platform, which frequently processes and stores global financial data every minute, is hosted in your on-premises data center and uses an Oracle database. Due to a recent cooling problem in their data center, the company urgently needs to migrate their infrastructure to AWS to improve the performance of their applications. As the Solutions Architect, you are responsible in ensuring that the database is properly migrated and should remain available in case of database server failure in the future.  Which of the following is the most suitable solution to meet the requirement?",B,B,Launch an Oracle database instance in RDS with Recovery Manager (RMAN) enabled.,Create an Oracle database in RDS with Multi-AZ deployments.,Launch an Oracle Real Application Clusters (RAC) in RDS.,Convert the database schema using the AWS Schema Conversion Tool and AWS Database Migration Service. Migrate the Oracle database to a non-cluster Amazon Aurora with a single instance.,,,,1분마다 글로벌 금융 데이터를 자주 처리하고 저장하는 Forex 거래 플랫폼은 온프레미스 데이터 센터에서 호스팅되며 Oracle 데이터베이스를 사용합니다. 최근 데이터 센터의 냉각 문제로 인해 회사는 애플리케이션의 성능을 개선하기 위해 인프라를 AWS로 긴급하게 마이그레이션해야 합니다. Solutions Architect는 데이터베이스가 적절하게 마이그레이션되고 향후 데이터베이스 서버 오류가 발생할 경우에도 계속 사용할 수 있도록 보장할 책임이 있습니다.  다음 중 요구 사항을 충족하는 가장 적합한 솔루션은 무엇입니까?,RMAN(복구 관리자)이 활성화된 RDS에서 Oracle 데이터베이스 인스턴스를 시작합니다.,다중 AZ 배포를 통해 RDS에서 Oracle 데이터베이스를 생성합니다.,RDS에서 Oracle RAC(Real Application Clusters)를 시작합니다.,AWS Schema Conversion Tool 및 AWS Database Migration Service를 사용하여 데이터베이스 스키마를 변환합니다. 단일 인스턴스를 사용하여 Oracle 데이터베이스를 비클러스터 Amazon Aurora로 마이그레이션합니다.,,,0,,
udemy,CLF-01,18,An application consists of multiple EC2 instances in private subnets in different availability zones. The application uses a single NAT Gateway for downloading software patches from the Internet to the instances. There is a requirement to protect the application from a single point of failure when the NAT Gateway encounters a failure or if its availability zone goes down.How should the Solutions Architect redesign the architecture to be more highly available and cost-effective,C,C,Create a NAT Gateway in each availability zone. Configure the route table in each public subnet to ensure that instances use the NAT Gateway in the same availability zone.,Create three NAT Gateways in each availability zone. Configure the route table in each private subnet to ensure that instances use the NAT Gateway in the same availability zone.,Create a NAT Gateway in each availability zone. Configure the route table in each private subnet to ensure that instances use the NAT Gateway in the same availability zone,Create two NAT Gateways in each availability zone. Configure the route table in each public subnet to ensure that instances use the NAT Gateway in the same availability zone.,,,,애플리케이션은 서로 다른 가용 영역의 프라이빗 서브넷에 있는 여러 EC2 인스턴스로 구성됩니다. 애플리케이션은 인터넷에서 인스턴스로 소프트웨어 패치를 다운로드하기 위해 단일 NAT 게이트웨이를 사용합니다. NAT 게이트웨이에 장애가 발생하거나 해당 가용성 영역이 다운되는 경우 단일 장애 지점으로부터 애플리케이션을 보호해야 하는 요구 사항이 있습니다.Solutions Architect는 가용성과 비용 효율성을 높이기 위해 아키텍처를 어떻게 재설계해야 합니까?,각 가용 영역에서 NAT 게이트웨이를 생성합니다. 인스턴스가 동일한 가용 영역에서 NAT 게이트웨이를 사용하도록 각 퍼블릭 서브넷에서 라우팅 테이블을 구성합니다.,각 가용 영역에 3개의 NAT 게이트웨이를 만듭니다. 인스턴스가 동일한 가용 영역에서 NAT 게이트웨이를 사용하도록 각 프라이빗 서브넷에서 라우팅 테이블을 구성합니다.,각 가용 영역에서 NAT 게이트웨이를 생성합니다. 인스턴스가 동일한 가용 영역에서 NAT 게이트웨이를 사용하도록 각 프라이빗 서브넷에서 라우팅 테이블을 구성합니다.,각 가용 영역에 두 개의 NAT 게이트웨이를 만듭니다. 인스턴스가 동일한 가용 영역에서 NAT 게이트웨이를 사용하도록 각 퍼블릭 서브넷에서 라우팅 테이블을 구성합니다.,,,0,,
udemy,CLF-01,19,"A company is using Amazon S3 to store frequently accessed data. When an object is created or deleted, the S3 bucket will send an event notification to the Amazon SQS queue. A solutions architect needs to create a solution that will notify the development and operations team about the created or deleted objects.Which of the following would satisfy this requirement?",C,C,Set up an Amazon SNS topic and configure two Amazon SQS queues to poll the SNS topic. Grant Amazon S3 permission to send notifications to Amazon SNS and update the bucket to use the new SNS topic.,Create a new Amazon SNS FIFO topic for the other team. Grant Amazon S3 permission to send the notification to the second SNS topic.,Create an Amazon SNS topic and configure two Amazon SQS queues to subscribe to the topic. Grant Amazon S3 permission to send notifications to Amazon SNS and update the bucket to use the new SNS topic.,Set up another Amazon SQS queue for the other team. Grant Amazon S3 permission to send a notification to the second SQS queue.,,,,한 회사에서 Amazon S3를 사용하여 자주 액세스하는 데이터를 저장하고 있습니다. 객체가 생성되거나 삭제되면 S3 버킷은 Amazon SQS 대기열에 이벤트 알림을 보냅니다. 솔루션 설계자는 생성되거나 삭제된 개체에 대해 개발 및 운영 팀에 알릴 솔루션을 만들어야 합니다.다음 중 이 요구 사항을 충족하는 것은 무엇입니까?,Amazon SNS 주제를 설정하고 두 개의 Amazon SQS 대기열을 구성하여 SNS 주제를 폴링합니다. Amazon SNS에 알림을 보낼 수 있는 권한을 Amazon S3에 부여하고 새 SNS 주제를 사용하도록 버킷을 업데이트합니다.,다른 팀을 위한 새로운 Amazon SNS FIFO 주제를 생성합니다. 두 번째 SNS 주제에 알림을 보낼 수 있는 권한을 Amazon S3에 부여합니다.,Amazon SNS 주제를 생성하고 두 개의 Amazon SQS 대기열을 구성하여 주제를 구독합니다. Amazon SNS에 알림을 보낼 수 있는 권한을 Amazon S3에 부여하고 새 SNS 주제를 사용하도록 버킷을 업데이트합니다.,다른 팀을 위해 다른 Amazon SQS 대기열을 설정합니다. 두 번째 SQS 대기열에 알림을 보낼 수 있는 권한을 Amazon S3에 부여합니다.,,,0,,
udemy,CLF-01,20,"A company plans to launch an Amazon EC2 instance in a private subnet for its internal corporate web portal. For security purposes, the EC2 instance must send data to Amazon DynamoDB and Amazon S3 via private endpoints that don't pass through the public Internet.Which of the following can meet the above requirements?",A,A,Use VPC endpoints to route all access to S3 and DynamoDB via private endpoints.,Use AWS Direct Connect to route all access to S3 and DynamoDB via private endpoints.,Use AWS Transit Gateway to route all access to S3 and DynamoDB via private endpoints.,Use AWS VPN CloudHub to route all access to S3 and DynamoDB via private endpoints.,,,,회사는 내부 회사 웹 포털의 프라이빗 서브넷에서 Amazon EC2 인스턴스를 시작할 계획입니다. 보안을 위해 EC2 인스턴스는 퍼블릭 인터넷을 통과하지 않는 프라이빗 엔드포인트를 통해 Amazon DynamoDB 및 Amazon S3로 데이터를 전송해야 합니다.다음 중 위의 요구 사항을 충족할 수 있는 것은 무엇입니까?,VPC 엔드포인트를 사용하여 프라이빗 엔드포인트를 통해 S3 및 DynamoDB에 대한 모든 액세스를 라우팅합니다.,AWS Direct Connect를 사용하여 프라이빗 엔드포인트를 통해 S3 및 DynamoDB에 대한 모든 액세스를 라우팅합니다.,AWS Transit Gateway를 사용하여 프라이빗 엔드포인트를 통해 S3 및 DynamoDB에 대한 모든 액세스를 라우팅합니다.,AWS VPN CloudHub를 사용하여 프라이빗 엔드포인트를 통해 S3 및 DynamoDB에 대한 모든 액세스를 라우팅합니다.,,,0,,
udemy,CLF-01,21,"An e-commerce company operates a highly scalable web application that relies on an Amazon Aurora database. As their users multiply, they've noticed that the read replica struggles to keep up with the increasing read traffic, leading to performance bottlenecks during peak periods.As a solutions architect, which of the following will address the issue with the most cost-effective solution?",D,D,Increase the size of the Amazon Aurora DB cluster.,Implement read scaling with Amazon Aurora Global Database.,Set up a read replica that can operate across different regions.,Use automatic scaling for the Amazon Aurora read replica using Aurora Auto Scaling.,,,,전자 상거래 회사는 Amazon Aurora 데이터베이스에 의존하는 확장성이 뛰어난 웹 애플리케이션을 운영합니다. 사용자가 늘어남에 따라 읽기 전용 복제본이 증가하는 읽기 트래픽을 따라잡기 어려워 피크 기간 동안 성능 병목 현상이 발생한다는 사실을 알게 되었습니다.솔루션 설계자로서 다음 중 가장 비용 효율적인 솔루션으로 문제를 해결하는 것은 무엇입니까?,Amazon Aurora DB 클러스터의 크기를 늘립니다.,Amazon Aurora 글로벌 데이터베이스로 읽기 확장을 구현하십시오.,여러 리전에서 작동할 수 있는 읽기 전용 복제본을 설정합니다.,Aurora Auto Scaling을 사용하여 Amazon Aurora 읽기 전용 복제본에 대해 자동 조정을 사용합니다.,,,0,,
udemy,CLF-01,22,A tech company that you are working for has undertaken a Total Cost Of Ownership (TCO) analysis evaluating the use of Amazon S3 versus acquiring more storage hardware. The result was that all 1200 employees would be granted access to use Amazon S3 for the storage of their personal documents.Which of the following will you need to consider so you can set up a solution that incorporates a single sign-on feature from your corporate AD or LDAP directory and also restricts access for each individual user to a designated user folder in an S3 bucket? (Select TWO.),BC,BC,Map each individual user to a designated user folder in S3 using Amazon WorkDocs to access their personal documents.,"Set up a Federation proxy or an Identity provider, and use AWS Security Token Service to generate temporary tokens.",Configure an IAM role and an IAM Policy to access the bucket.,"Use 3rd party Single Sign-On solutions such as Atlassian Crowd, OKTA, OneLogin and many others.",Set up a matching IAM user for each of the 1200 users in your corporate directory that needs access to a folder in the S3 bucket.,,,귀하가 근무하는 기술 회사에서 Amazon S3 사용과 추가 스토리지 하드웨어 구입을 평가하는 TCO(총 소유 비용) 분석을 수행했습니다. 그 결과 1200명의 직원 모두가 개인 문서 저장을 위해 Amazon S3를 사용할 수 있는 액세스 권한을 부여받았습니다.다음 중 회사 AD 또는 LDAP 디렉터리의 싱글 사인온 기능을 통합하고 각 개별 사용자의 액세스를 S3 버킷의 지정된 사용자 폴더로 제한하는 솔루션을 설정하기 위해 고려해야 할 사항은 무엇입니까? (2개를 선택하세요.),Amazon WorkDocs를 사용하여 각 개별 사용자를 S3의 지정된 사용자 폴더에 매핑하여 개인 문서에 액세스합니다.,페더레이션 프록시 또는 자격 증명 공급자를 설정하고 AWS Security Token Service를 사용하여 임시 토큰을 생성합니다.,버킷에 액세스하도록 IAM 역할 및 IAM 정책을 구성합니다.,"Atlassian Crowd, OKTA, OneLogin 등의 타사 Single Sign-On 솔루션을 사용하십시오.",S3 버킷의 폴더에 액세스해야 하는 회사 디렉터리의 사용자 1200명 각각에 대해 일치하는 IAM 사용자를 설정합니다.,,0,,
udemy,CLF-01,23,"A Solutions Architect is working for a company which has multiple VPCs in various AWS regions. The Architect is assigned to set up a logging system which will track all of the changes made to their AWS resources in all regions, including the configurations made in IAM, CloudFront, AWS WAF, and Route 53. In order to pass the compliance requirements, the solution must ensure the security, integrity, and durability of the log data. It should also provide an event history of all API calls made in AWS Management Console and AWS CLI. Which of the following solutions is the best fit for this scenario?",C,C,Set up a new CloudWatch trail in a new S3 bucket using the AWS CLI and also pass both the --is-multi-region-trail and --include-global-service-events parameters then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.,Set up a new CloudTrail trail in a new S3 bucket using the AWS CLI and also pass both the --is-multi-region-trail and --no-include-global-service-events parameters then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.,Set up a new CloudTrail trail in a new S3 bucket using the AWS CLI and also pass both the --is-multi-region-trail and --include-global-service-events parameters then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.,Set up a new CloudWatch trail in a new S3 bucket using the CloudTrail console and also pass the --is-multi-region-trail parameter then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.,,,,"Solutions Architect는 다양한 AWS 지역에 여러 VPC가 있는 회사에서 근무하고 있습니다. 설계자는 IAM, CloudFront, AWS WAF 및 Route 53에서 이루어진 구성을 포함하여 모든 리전에서 AWS 리소스에 대한 모든 변경 사항을 추적할 로깅 시스템을 설정하도록 지정되었습니다. 규정 준수 요구 사항을 통과하기 위해 솔루션은 로그 데이터의 보안, 무결성 및 내구성을 보장해야 합니다. 또한 AWS Management Console 및 AWS CLI에서 이루어진 모든 API 호출의 이벤트 기록을 제공해야 합니다.다음 중 이 시나리오에 가장 적합한 솔루션은 무엇입니까?",AWS CLI를 사용하여 새 S3 버킷에 새 CloudWatch 추적을 설정하고 및 매개변수를 모두 전달한 --is-multi-region-trail다음 --include-global-service-eventsKMS 암호화를 사용하여 로그 파일을 암호화합니다. S3 버킷에 MFA(Multi Factor Authentication) 삭제를 적용하고 버킷 정책을 구성하여 승인된 사용자만 로그에 액세스할 수 있도록 합니다.,AWS CLI를 사용하여 새 S3 버킷에 새 CloudTrail 추적을 설정하고 및 매개변수를 모두 전달한 --is-multi-region-trail다음 --no-include-global-service-eventsKMS 암호화를 사용하여 로그 파일을 암호화합니다. S3 버킷에 MFA(Multi Factor Authentication) 삭제를 적용하고 버킷 정책을 구성하여 승인된 사용자만 로그에 액세스할 수 있도록 합니다.,AWS CLI를 사용하여 새 S3 버킷에 새 CloudTrail 추적을 설정하고 및 매개변수를 모두 전달한 --is-multi-region-trail다음 --include-global-service-eventsKMS 암호화를 사용하여 로그 파일을 암호화합니다. S3 버킷에 MFA(Multi Factor Authentication) 삭제를 적용하고 버킷 정책을 구성하여 승인된 사용자만 로그에 액세스할 수 있도록 합니다.,CloudTrail 콘솔을 사용하여 새 S3 버킷에 새 CloudWatch 추적을 설정하고 --is-multi-region-trail파라미터를 전달한 다음 KMS 암호화를 사용하여 로그 파일을 암호화합니다. S3 버킷에 MFA(Multi Factor Authentication) 삭제를 적용하고 버킷 정책을 구성하여 승인된 사용자만 로그에 액세스할 수 있도록 합니다.,,,0,,
udemy,CLF-01,24,"A company requires all the data stored in the cloud to be encrypted at rest. To easily integrate this with other AWS services, they must have full control over the encryption of the created keys and also the ability to immediately remove the key material from AWS KMS. The solution should also be able to audit the key usage independently of AWS CloudTrail.Which of the following options will meet this requirement?",B,B,Use AWS Key Management Service to create a CMK in a custom key store and store the non-extractable key material in Amazon S3.,Use AWS Key Management Service to create a CMK in a custom key store and store the non-extractable key material in AWS CloudHSM.,Use AWS Key Management Service to create AWS-owned CMKs and store the non-extractable key material in AWS CloudHSM.,Use AWS Key Management Service to create AWS-managed CMKs and store the non-extractable key material in AWS CloudHSM.,,,,회사는 클라우드에 저장된 모든 데이터를 유휴 상태에서 암호화해야 합니다. 이를 다른 AWS 서비스와 쉽게 통합하려면 생성된 키의 암호화를 완전히 제어하고 AWS KMS에서 키 자료를 즉시 제거할 수 있어야 합니다. 또한 솔루션은 AWS CloudTrail과 독립적으로 키 사용을 감사할 수 있어야 합니다.다음 중 이 요구 사항을 충족하는 옵션은 무엇입니까?,AWS Key Management Service를 사용하여 사용자 지정 키 스토어에 CMK를 생성하고 추출할 수 없는 키 구성 요소를 Amazon S3에 저장합니다.,AWS Key Management Service를 사용하여 사용자 지정 키 스토어에 CMK를 생성하고 추출할 수 없는 키 구성 요소를 AWS CloudHSM에 저장합니다.,AWS Key Management Service를 사용하여 AWS 소유 CMK를 생성하고 추출할 수 없는 키 구성 요소를 AWS CloudHSM에 저장합니다.,AWS Key Management Service를 사용하여 AWS 관리형 CMK를 생성하고 추출할 수 없는 키 구성 요소를 AWS CloudHSM에 저장합니다.,,,0,,
udemy,CLF-01,25,"A company plans to host a web application in an Auto Scaling group of Amazon EC2 instances. The application will be used globally by users to upload and store several types of files. Based on user trends, files that are older than 2 years must be stored in a different storage class. The Solutions Architect of the company needs to create a cost-effective and scalable solution to store the old files yet still provide durability and high availability.Which of the following approach can be used to fulfill this requirement? (Select TWO.)",BC,BC,Use Amazon EBS volumes to store the files. Configure the Amazon Data Lifecycle Manager (DLM) to schedule snapshots of the volumes after 2 years.,Use Amazon S3 and create a lifecycle policy that will move the objects to Amazon S3 Standard-IA after 2 years.,Use Amazon S3 and create a lifecycle policy that will move the objects to Amazon S3 Glacier after 2 years.,Use a RAID 0 storage configuration that stripes multiple Amazon EBS volumes together to store the files. Configure the Amazon Data Lifecycle Manager (DLM) to schedule snapshots of the volumes after 2 years.,Use Amazon EFS and create a lifecycle policy that will move the objects to Amazon EFS-IA after 2 years.,,,회사는 Amazon EC2 인스턴스의 Auto Scaling 그룹에서 웹 애플리케이션을 호스팅할 계획입니다. 이 응용 프로그램은 여러 유형의 파일을 업로드하고 저장하기 위해 사용자가 전 세계적으로 사용합니다. 사용자 경향에 따라 2년 이상 된 파일은 다른 스토리지 클래스에 저장해야 합니다. 회사의 Solutions Architect는 이전 파일을 저장하면서도 여전히 내구성과 고가용성을 제공하는 비용 효율적이고 확장 가능한 솔루션을 만들어야 합니다.다음 중 이 요구 사항을 충족하는 데 사용할 수 있는 방법은 무엇입니까? (2개를 선택하세요.),Amazon EBS 볼륨을 사용하여 파일을 저장합니다. 2년 후 볼륨의 스냅샷을 예약하도록 Amazon DLM(Data Lifecycle Manager)을 구성합니다.,Amazon S3를 사용하고 2년 후에 객체를 Amazon S3 Standard-IA로 이동하는 수명 주기 정책을 생성합니다.,Amazon S3를 사용하고 2년 후에 객체를 Amazon S3 Glacier로 이동하는 수명 주기 정책을 생성합니다.,여러 Amazon EBS 볼륨을 함께 스트라이프하는 RAID 0 스토리지 구성을 사용하여 파일을 저장합니다. 2년 후 볼륨의 스냅샷을 예약하도록 Amazon DLM(Data Lifecycle Manager)을 구성합니다.,Amazon EFS를 사용하고 2년 후에 객체를 Amazon EFS-IA로 이동하는 수명 주기 정책을 생성합니다.,,0,,
udemy,CLF-01,26,A company is in the process of migrating their applications to AWS. One of their systems requires a database that can scale globally and handle frequent schema changes. The application should not have any downtime or performance issues whenever there is a schema change in the database. It should also provide a low latency response to high-traffic queries.Which is the most suitable database solution to use to achieve this requirement?,D,D,An Amazon Aurora database with Read Replicas,Redshift,An Amazon RDS instance in Multi-AZ Deployments configuration,Amazon DynamoDB,,,,회사에서 애플리케이션을 AWS로 마이그레이션하는 과정에 있습니다. 그들의 시스템 중 하나는 전역적으로 확장되고 빈번한 스키마 변경을 처리할 수 있는 데이터베이스가 필요합니다. 데이터베이스에서 스키마가 변경될 때마다 응용 프로그램에 중단 시간이나 성능 문제가 없어야 합니다. 또한 트래픽이 많은 쿼리에 대해 대기 시간이 짧은 응답을 제공해야 합니다.이 요구 사항을 충족하는 데 사용하기에 가장 적합한 데이터베이스 솔루션은 무엇입니까?,읽기 전용 복제본이 있는 Amazon Aurora 데이터베이스,적색편이,다중 AZ 배포 구성의 Amazon RDS 인스턴스,아마존 다이나모DB,,,0,,
udemy,CLF-01,27,"A company has a cloud architecture that is composed of Linux and Windows EC2 instances that process high volumes of financial data 24 hours a day, 7 days a week. To ensure high availability of the systems, the Solutions Architect needs to create a solution that allows them to monitor the memory and disk utilization metrics of all the instances.Which of the following is the most suitable monitoring solution to implement?",B,B,Enable the Enhanced Monitoring option in EC2 and install CloudWatch agent to all the EC2 instances to be able to view the memory and disk utilization in the CloudWatch dashboard.,Install the CloudWatch agent to all the EC2 instances that gathers the memory and disk utilization data. View the custom metrics in the Amazon CloudWatch console.,Use Amazon Inspector and install the Inspector agent to all EC2 instances.,Use the default CloudWatch configuration to EC2 instances where the memory and disk utilization metrics are already available. Install the AWS Systems Manager (SSM) Agent to all the EC2 instances.,,,,회사에는 하루 24시간 연중무휴 대량의 금융 데이터를 처리하는 Linux 및 Windows EC2 인스턴스로 구성된 클라우드 아키텍처가 있습니다. 시스템의 고가용성을 보장하기 위해 Solutions Architect는 모든 인스턴스의 메모리 및 디스크 사용 메트릭을 모니터링할 수 있는 솔루션을 생성해야 합니다.다음 중 구현하기에 가장 적합한 모니터링 솔루션은 무엇입니까?,EC2에서 Enhanced Monitoring 옵션을 활성화하고 CloudWatch 대시보드에서 메모리 및 디스크 사용률을 볼 수 있도록 모든 EC2 인스턴스에 CloudWatch 에이전트를 설치합니다.,메모리 및 디스크 사용률 데이터를 수집하는 모든 EC2 인스턴스에 CloudWatch 에이전트를 설치합니다. Amazon CloudWatch 콘솔에서 사용자 지정 지표를 봅니다.,Amazon Inspector를 사용하고 Inspector 에이전트를 모든 EC2 인스턴스에 설치합니다.,메모리 및 디스크 사용 지표를 이미 사용할 수 있는 EC2 인스턴스에 기본 CloudWatch 구성을 사용합니다. 모든 EC2 인스턴스에 AWS Systems Manager(SSM) 에이전트를 설치합니다.,,,0,,
udemy,CLF-01,28,"An application that records weather data every minute is deployed in a fleet of Spot EC2 instances and uses a MySQL RDS database instance. Currently, there is only one RDS instance running in one Availability Zone. You plan to improve the database to ensure high availability by synchronous data replication to another RDS instance. Which of the following performs synchronous data replication in RDS?",B,B,CloudFront running as a Multi-AZ deployment,RDS DB instance running as a Multi-AZ deployment,DynamoDB Read Replica,RDS Read Replica,,,,1분마다 날씨 데이터를 기록하는 애플리케이션은 Spot EC2 인스턴스 플릿에 배포되며 MySQL RDS 데이터베이스 인스턴스를 사용합니다. 현재 하나의 가용 영역에서 실행되는 RDS 인스턴스는 하나만 있습니다. 다른 RDS 인스턴스에 동기식 데이터 복제를 통해 고가용성을 보장하도록 데이터베이스를 개선할 계획입니다.다음 중 RDS에서 동기식 데이터 복제를 수행하는 것은 무엇입니까?,다중 AZ 배포로 실행되는 CloudFront,다중 AZ 배포로 실행되는 RDS DB 인스턴스,DynamoDB 읽기 전용 복제본,RDS 읽기 복제본,,,0,,
udemy,CLF-01,29,"A car dealership website hosted in Amazon EC2 stores car listings in an Amazon Aurora database managed by Amazon RDS. Once a vehicle has been sold, its data must be removed from the current listings and forwarded to a distributed processing system.Which of the following options can satisfy the given requirement?",D,D,Create an RDS event subscription and send the notifications to AWS Lambda. Configure the Lambda function to fan out the event notifications to multiple Amazon SQS queues to update the processing system.,Create an RDS event subscription and send the notifications to Amazon SNS. Configure the SNS topic to fan out the event notifications to multiple Amazon SQS queues. Process the data using Lambda functions.,Create an RDS event subscription and send the notifications to Amazon SQS. Configure the SQS queues to fan out the event notifications to multiple Amazon SNS topics. Process the data using Lambda functions.,Create a native function or a stored procedure that invokes a Lambda function. Configure the Lambda function to send event notifications to an Amazon SQS queue for the processing system to consume.,,,,Amazon EC2에서 호스팅되는 자동차 대리점 웹 사이트는 Amazon RDS에서 관리하는 Amazon Aurora 데이터베이스에 자동차 목록을 저장합니다. 차량이 판매되면 해당 데이터를 현재 목록에서 제거하고 분산 처리 시스템으로 전달해야 합니다.다음 옵션 중 주어진 요구 사항을 충족할 수 있는 옵션은 무엇입니까?,RDS 이벤트 구독을 생성하고 알림을 AWS Lambda로 보냅니다. 이벤트 알림을 여러 Amazon SQS 대기열로 팬아웃하여 처리 시스템을 업데이트하도록 Lambda 함수를 구성합니다.,RDS 이벤트 구독을 생성하고 알림을 Amazon SNS로 보냅니다. 이벤트 알림을 여러 Amazon SQS 대기열로 팬아웃하도록 SNS 주제를 구성합니다. Lambda 함수를 사용하여 데이터를 처리합니다.,RDS 이벤트 구독을 생성하고 알림을 Amazon SQS로 보냅니다. 이벤트 알림을 여러 Amazon SNS 주제로 팬아웃하도록 SQS 대기열을 구성합니다. Lambda 함수를 사용하여 데이터를 처리합니다.,Lambda 함수를 호출하는 기본 함수 또는 저장 프로시저를 생성합니다. 처리 시스템이 사용할 Amazon SQS 대기열에 이벤트 알림을 보내도록 Lambda 함수를 구성합니다.,,,0,,
udemy,CLF-01,30,"A company is designing a banking portal that uses Amazon ElastiCache for Redis as its distributed session management component. Since the other Cloud Engineers in your department have access to your ElastiCache cluster, you have to secure the session data in the portal by requiring them to enter a password before they are granted permission to execute Redis commands.As the Solutions Architect, which of the following should you do to meet the above requirement?",B,B,Set up an IAM Policy and MFA which requires the Cloud Engineers to enter their IAM credentials and token before they can access the ElastiCache cluster.,Authenticate the users using Redis AUTH by creating a new Redis Cluster with both the --transit-encryption-enabled and --auth-token parameters enabled.,Set up a Redis replication group and enable the AtRestEncryptionEnabled parameter.,Enable the in-transit encryption for Redis replication groups.,,,,한 회사에서 Redis용 Amazon ElastiCache를 분산 세션 관리 구성 요소로 사용하는 뱅킹 포털을 설계하고 있습니다. 부서의 다른 클라우드 엔지니어가 ElastiCache 클러스터에 액세스할 수 있으므로 Redis 명령을 실행할 수 있는 권한을 부여받기 전에 암호를 입력하도록 요구하여 포털의 세션 데이터를 보호해야 합니다.솔루션 설계자로서 위의 요구 사항을 충족하려면 다음 중 무엇을 해야 합니까?,클라우드 엔지니어가 ElastiCache 클러스터에 액세스하기 전에 IAM 자격 증명과 토큰을 입력해야 하는 IAM 정책 및 MFA를 설정합니다.,--transit-encryption-enabled및 매개변수가 활성화된 새 Redis 클러스터를 생성하여 Redis AUTH를 사용하여 사용자를 인증합니다 --auth-token.,Redis 복제 그룹을 설정하고 AtRestEncryptionEnabled매개변수를 활성화합니다.,Redis 복제 그룹에 대해 전송 중 암호화를 활성화합니다.,,,0,,
udemy,CLF-01,31,"A newly hired Solutions Architect is assigned to manage a set of CloudFormation templates that are used in the company's cloud architecture in AWS. The Architect accessed the templates and tried to analyze the configured IAM policy for an S3 bucket.{  ""Version"": ""2012-10-17"",  ""Statement"": [   {    ""Effect"": ""Allow"",    ""Action"": [     ""s3:Get*"",     ""s3:List*""    ],    ""Resource"": ""*""   },   {    ""Effect"": ""Allow"",    ""Action"": ""s3:PutObject"",    ""Resource"": ""arn:aws:s3:::boracay/*""   }  ] }What does the above IAM policy allow? (Select THREE.)",ABE,ABE,An IAM user with this IAM policy is allowed to write objects into the boracay S3 bucket.,An IAM user with this IAM policy is allowed to read objects from the boracay S3 bucket.,An IAM user with this IAM policy is allowed to change access rights for the boracay S3 bucket.,An IAM user with this IAM policy is allowed to read and delete objects from the boracay S3 bucket.,An IAM user with this IAM policy is allowed to read objects from all S3 buckets owned by the account.,An IAM user with this IAM policy is allowed to read objects in the boracay S3 bucket but not allowed to list the objects in the bucket.,,"새로 고용된 솔루션 설계자는 AWS에서 회사의 클라우드 아키텍처에 사용되는 CloudFormation 템플릿 세트를 관리하도록 지정됩니다. Architect는 템플릿에 액세스하고 S3 버킷에 대해 구성된 IAM 정책을 분석하려고 했습니다.{  ""버전"" : ""2012-10-17"" ,   ""진술서"" : [    {    ""효과"" : ""허용"" ,     ""액션"" : [      ""s3:Get*"" ,     ""s3:목록*""    ],    ""자원"" : ""*""    },   {    ""효과"" : ""허용"" ,     ""작업"" : ""s3:PutObject"" ,     ""리소스"" : ""arn:aws:s3:::boracay/*""    }  ] }위의 IAM 정책은 무엇을 허용합니까? (3개를 선택하세요.)",이 IAM 정책이 있는 IAM 사용자는 S3 버킷에 객체를 쓸 수 있습니다 boracay.,이 IAM 정책이 있는 IAM 사용자는 S3 버킷에서 객체를 읽을 수 있습니다 boracay.,이 IAM 정책이 있는 IAM 사용자는 S3 버킷에 대한 액세스 권한을 변경할 수 있습니다 boracay.,이 IAM 정책이 있는 IAM 사용자는 S3 버킷에서 객체를 읽고 삭제할 수 있습니다 boracay.,이 IAM 정책이 있는 IAM 사용자는 계정이 소유한 모든 S3 버킷에서 객체를 읽을 수 있습니다.,,0,,이 IAM 정책이 있는 IAM 사용자는 S3 버킷의 객체를 읽을 수 boracay있지만 버킷의 객체를 나열할 수는 없습니다.
udemy,CLF-01,32,"A government agency plans to store confidential tax documents on AWS. Due to the sensitive information in the files, the Solutions Architect must restrict the data access requests made to the storage solution to a specific Amazon VPC only. The solution should also prevent the files from being deleted or overwritten to meet the regulatory requirement of having a write-once-read-many (WORM) storage model.Which combination of the following options should the Architect implement? (Select TWO.)",BD,BD,Set up a new Amazon S3 bucket to store the tax documents and integrate it with AWS Network Firewall. Configure the Network Firewall to only accept data access requests from a specific Amazon VPC.,Create a new Amazon S3 bucket with the S3 Object Lock feature enabled. Store the documents in the bucket and set the Legal Hold option for object retention.,Enable Object Lock but disable Object Versioning on the new Amazon S3 bucket to comply with the write-once-read-many (WORM) storage model requirement.,Configure an Amazon S3 Access Point for the S3 bucket to restrict data access to a particular Amazon VPC only.,Store the tax documents in the Amazon S3 Glacier Instant Retrieval storage class to restrict fast data retrieval to a particular Amazon VPC of your choice.,,,정부 기관에서 기밀 세금 문서를 AWS에 저장할 계획입니다. 파일의 민감한 정보로 인해 Solutions Architect는 스토리지 솔루션에 대한 데이터 액세스 요청을 특정 Amazon VPC로만 제한해야 합니다. 이 솔루션은 WORM(Write Once Read Many) 스토리지 모델에 대한 규정 요구 사항을 충족하기 위해 파일이 삭제되거나 덮어쓰이는 것을 방지해야 합니다.Architect는 다음 옵션 중 어떤 조합을 구현해야 합니까? (2개를 선택하세요.),새 Amazon S3 버킷을 설정하여 세금 문서를 저장하고 AWS Network Firewall과 통합하십시오. 특정 Amazon VPC의 데이터 액세스 요청만 수락하도록 네트워크 방화벽을 구성합니다.,S3 객체 잠금 기능이 활성화된 새 Amazon S3 버킷을 생성합니다. 버킷에 문서를 저장하고 개체 보존에 대한 법적 보존 옵션을 설정합니다.,객체 잠금을 활성화하지만 WORM(Write-Once-Read-Many) 스토리지 모델 요구 사항을 준수하기 위해 새 Amazon S3 버킷에서 객체 버전 관리를 비활성화합니다.,특정 Amazon VPC에 대한 데이터 액세스만 제한하도록 S3 버킷에 대한 Amazon S3 액세스 포인트를 구성합니다.,Amazon S3 Glacier Instant Retrieval 스토리지 클래스에 세금 문서를 저장하여 선택한 특정 Amazon VPC로 빠른 데이터 검색을 제한합니다.,,0,,
udemy,CLF-01,33,A Solutions Architect needs to set up a relational database and come up with a disaster recovery plan to mitigate multi-region failure. The solution requires a Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of less than 1 minute.Which of the following AWS services can fulfill this requirement?,C,C,Amazon RDS for PostgreSQL with cross-region read replicas,Amazon Timestream,Amazon Aurora Global Database,Amazon Quantum Ledger Database (Amazon QLDB),,,,Solutions Architect는 관계형 데이터베이스를 설정하고 재해 복구 계획을 세워 다중 지역 장애를 완화해야 합니다. 이 솔루션에는 1초의 RPO(복구 지점 목표)와 1분 미만의 RTO(복구 시간 목표)가 필요합니다.다음 중 이 요구 사항을 충족할 수 있는 AWS 서비스는 무엇입니까?,지역 간 읽기 전용 복제본이 있는 PostgreSQL용 Amazon RDS,아마존 타임스트림,Amazon Aurora 글로벌 데이터베이스,Amazon Quantum Ledger 데이터베이스(Amazon QLDB),,,0,,
udemy,CLF-01,34,"A company has a web application that uses Amazon CloudFront to distribute its images, videos, and other static contents stored in its S3 bucket to its users around the world. The company has recently introduced a new member-only access feature to some of its high-quality media files. There is a requirement to provide access to multiple private media files only to their paying subscribers without having to change their current URLs.Which of the following is the most suitable solution that you should implement to satisfy this requirement?",A,A,"Use Signed Cookies to control who can access the private files in your CloudFront distribution by modifying your application to determine whether a user should have access to your content. For members, send the required Set-Cookie headers to the viewer which will unlock the content only to them.",Create a Signed URL with a custom policy which only allows the members to see the private files.,Configure your CloudFront distribution to use Field-Level Encryption to protect your private data and only allow access to members.,Configure your CloudFront distribution to use Match Viewer as its Origin Protocol Policy which will automatically match the user request. This will allow access to the private content if the request is a paying member and deny it if it is not a member.,,,,"회사에는 Amazon CloudFront를 사용하여 S3 버킷에 저장된 이미지, 비디오 및 기타 정적 콘텐츠를 전 세계 사용자에게 배포하는 웹 애플리케이션이 있습니다. 이 회사는 최근 일부 고품질 미디어 파일에 대한 새로운 회원 전용 액세스 기능을 도입했습니다. 현재 URL을 변경할 필요 없이 유료 구독자에게만 여러 비공개 미디어 파일에 대한 액세스를 제공해야 한다는 요구 사항이 있습니다.다음 중 이 요구 사항을 충족하기 위해 구현해야 하는 가장 적합한 솔루션은 무엇입니까?",서명된 쿠키를 사용하면 사용자가 콘텐츠에 액세스해야 하는지 여부를 결정하도록 애플리케이션을 수정하여 CloudFront 배포의 개인 파일에 액세스할 수 있는 사람을 제어할 수 있습니다. 회원의 경우 필요한 Set-Cookie헤더를 뷰어에게 보내면 해당 회원만 콘텐츠를 잠금 해제할 수 있습니다.,구성원만 개인 파일을 볼 수 있도록 허용하는 사용자 지정 정책으로 서명된 URL을 만듭니다.,필드 수준 암호화를 사용하여 비공개 데이터를 보호하고 구성원에게만 액세스를 허용하도록 CloudFront 배포를 구성합니다.,Match Viewer를 사용자 요청과 자동으로 일치시키는 오리진 프로토콜 정책으로 사용하도록 CloudFront 배포를 구성합니다. 이렇게 하면 요청이 유료 회원인 경우 비공개 콘텐츠에 대한 액세스가 허용되고 회원이 아닌 경우 거부됩니다.,,,0,,
udemy,CLF-01,35,"A travel photo sharing website is using Amazon S3 to serve high-quality photos to visitors of your website. After a few days, you found out that there are other travel websites linking and using your photos. This resulted in financial losses for your business. What is the MOST effective method to mitigate this issue?",B,B,Block the IP addresses of the offending websites using NACL.,Configure your S3 bucket to remove public read access and use pre-signed URLs with expiry dates.,Use CloudFront distributions for your photos.,Store and privately serve the high-quality photos on Amazon WorkDocs instead.,,,,"여행 사진 공유 웹사이트는 Amazon S3를 사용하여 웹사이트 방문자에게 고품질 사진을 제공합니다. 며칠 후, 당신은 당신의 사진을 연결하고 사용하는 다른 여행 웹사이트가 있다는 것을 알게 되었습니다. 이로 인해 비즈니스에 재정적 손실이 발생했습니다.이 문제를 완화하는 가장 효과적인 방법은 무엇입니까?",NACL을 사용하여 공격 웹사이트의 IP 주소를 차단합니다.,공개 읽기 액세스를 제거하고 만료일이 있는 미리 서명된 URL을 사용하도록 S3 버킷을 구성하십시오.,사진에 CloudFront 배포를 사용하십시오.,대신 Amazon WorkDocs에 고품질 사진을 저장하고 비공개로 제공하십시오.,,,0,,
udemy,CLF-01,36,"A company hosted an e-commerce website on an Auto Scaling group of EC2 instances behind an Application Load Balancer. The Solutions Architect noticed that the website is receiving a large number of illegitimate external requests from multiple systems with IP addresses that constantly change. To resolve the performance issues, the Solutions Architect must implement a solution that would block the illegitimate requests with minimal impact on legitimate traffic.Which of the following options fulfills this requirement?",D,D,Create a regular rule in AWS WAF and associate the web ACL to an Application Load Balancer.,Create a custom network ACL and associate it with the subnet of the Application Load Balancer to block the offending requests.,Create a custom rule in the security group of the Application Load Balancer to block the offending requests.,Create a rate-based rule in AWS WAF and associate the web ACL to an Application Load Balancer.,,,,한 회사가 Application Load Balancer 뒤에 있는 EC2 인스턴스의 Auto Scaling 그룹에서 전자상거래 웹사이트를 호스팅했습니다. Solutions Architect는 웹 사이트가 지속적으로 변경되는 IP 주소를 사용하는 여러 시스템에서 많은 수의 불법 외부 요청을 받고 있음을 확인했습니다. 성능 문제를 해결하기 위해 Solutions Architect는 합법적인 트래픽에 미치는 영향을 최소화하면서 불법적인 요청을 차단하는 솔루션을 구현해야 합니다.다음 중 이 요구 사항을 충족하는 옵션은 무엇입니까?,AWS WAF에서 일반 규칙을 생성하고 웹 ACL을 Application Load Balancer에 연결합니다.,사용자 지정 네트워크 ACL을 생성하고 이를 Application Load Balancer의 서브넷과 연결하여 잘못된 요청을 차단합니다.,잘못된 요청을 차단하기 위해 Application Load Balancer의 보안 그룹에 사용자 지정 규칙을 만듭니다.,AWS WAF에서 속도 기반 규칙을 생성하고 웹 ACL을 Application Load Balancer에 연결합니다.,,,0,,
udemy,CLF-01,37,"A global IT company with offices around the world has multiple AWS accounts. To improve efficiency and drive costs down, the Chief Information Officer (CIO) wants to set up a solution that centrally manages their AWS resources. This will allow them to procure AWS resources centrally and share resources such as AWS Transit Gateways, AWS License Manager configurations, or Amazon Route 53 Resolver rules across their various accounts. As the Solutions Architect, which combination of options should you implement in this scenario? (Select TWO.)",AB,AB,Use the AWS Resource Access Manager (RAM) service to easily and securely share your resources with your AWS accounts.,Consolidate all of the company's accounts using AWS Organizations.,Use AWS Control Tower to easily and securely share your resources with your AWS accounts.,Use the AWS Identity and Access Management service to set up cross-account access that will easily and securely share your resources with your AWS accounts.,Consolidate all of the company's accounts using AWS ParallelCluster.,,,"전 세계에 지사를 둔 글로벌 IT 회사는 여러 AWS 계정을 보유하고 있습니다. 효율성을 개선하고 비용을 절감하기 위해 최고 정보 책임자(CIO)는 AWS 리소스를 중앙에서 관리하는 솔루션을 설정하려고 합니다. 이를 통해 AWS 리소스를 중앙에서 조달하고 다양한 계정에서 AWS Transit Gateway, AWS License Manager 구성 또는 Amazon Route 53 Resolver 규칙과 같은 리소스를 공유할 수 있습니다.솔루션 설계자로서 이 시나리오에서 어떤 옵션 조합을 구현해야 합니까? (2개를 선택하세요.)",AWS Resource Access Manager(RAM) 서비스를 사용하여 리소스를 AWS 계정과 쉽고 안전하게 공유하십시오.,AWS Organizations를 사용하여 회사의 모든 계정을 통합합니다.,AWS Control Tower를 사용하여 리소스를 AWS 계정과 쉽고 안전하게 공유하십시오.,AWS Identity and Access Management 서비스를 사용하여 AWS 계정과 리소스를 쉽고 안전하게 공유하는 교차 계정 액세스를 설정하십시오.,AWS ParallelCluster를 사용하여 회사의 모든 계정을 통합합니다.,,0,,
udemy,CLF-01,38,There are a lot of outages in the Availability Zone of your RDS database instance to the point that you have lost access to the database. What could you do to prevent losing access to your database in case that this event happens again?,C,C,Create a read replica,Make a snapshot of the database,Enabled Multi-AZ failover,Increase the database instance size,,,,RDS 데이터베이스 인스턴스의 가용 영역에서 데이터베이스에 대한 액세스 권한을 잃을 정도로 많은 중단이 발생했습니다. 이 이벤트가 다시 발생하는 경우 데이터베이스에 대한 액세스 권한을 잃지 않으려면 어떻게 해야 합니까?,읽기 복제본 만들기,데이터베이스의 스냅샷 만들기,활성화된 다중 AZ 장애 조치,데이터베이스 인스턴스 크기 늘리기,,,0,,
udemy,CLF-01,39,"A company has a web application that uses Internet Information Services (IIS) for Windows Server. A file share is used to store the application data on the network-attached storage of the company’s on-premises data center. To achieve a highly available system, they plan to migrate the application and file share to AWS.Which of the following can be used to fulfill this requirement?",D,D,Migrate the existing file share configuration to Amazon EBS.,Migrate the existing file share configuration to Amazon EFS.,Migrate the existing file share configuration to AWS Storage Gateway.,Migrate the existing file share configuration to Amazon FSx for Windows File Server.,,,,회사에 Windows Server용 IIS(인터넷 정보 서비스)를 사용하는 웹 응용 프로그램이 있습니다. 파일 공유는 회사 온프레미스 데이터 센터의 네트워크 연결 스토리지에 애플리케이션 데이터를 저장하는 데 사용됩니다. 고가용성 시스템을 달성하기 위해 애플리케이션 및 파일 공유를 AWS로 마이그레이션할 계획입니다.다음 중 이 요구 사항을 충족하는 데 사용할 수 있는 것은 무엇입니까?,기존 파일 공유 구성을 Amazon EBS로 마이그레이션합니다.,기존 파일 공유 구성을 Amazon EFS로 마이그레이션합니다.,기존 파일 공유 구성을 AWS Storage Gateway로 마이그레이션합니다.,기존 파일 공유 구성을 Amazon FSx for Windows File Server로 마이그레이션합니다.,,,0,,
udemy,CLF-01,40,A business has recently migrated its applications to AWS. The audit team must be able to assess whether the services the company is using meet common security and regulatory standards. A solutions architect needs to provide the team with a report of all compliance-related documents for their account.Which action should a solutions architect consider?,A,A,Use AWS Artifact to view the security reports as well as other AWS compliance-related information.,"Run an Amazon Macie job to view the Service Organization Control (SOC), Payment Card Industry (PCI), and other compliance reports from AWS Certificate Manager (ACM).",Run an Amazon Inspector assessment job to download all of the AWS compliance-related information.,View all of the AWS security compliance reports from AWS Security Hub.,,,,한 기업이 최근 애플리케이션을 AWS로 마이그레이션했습니다. 감사 팀은 회사에서 사용하는 서비스가 공통 보안 및 규제 표준을 충족하는지 여부를 평가할 수 있어야 합니다. 솔루션 설계자는 계정에 대한 모든 규정 준수 관련 문서 보고서를 팀에 제공해야 합니다.솔루션 설계자는 어떤 조치를 고려해야 합니까?,AWS Artifact를 사용하여 보안 보고서와 기타 AWS 규정 준수 관련 정보를 볼 수 있습니다.,"Amazon Macie 작업을 실행하여 SOC(Service Organization Control), PCI(Payment Card Industry) 및 AWS Certificate Manager(ACM)의 기타 규정 준수 보고서를 봅니다.",Amazon Inspector 평가 작업을 실행하여 모든 AWS 규정 준수 관련 정보를 다운로드합니다.,AWS Security Hub에서 모든 AWS 보안 규정 준수 보고서를 봅니다.,,,0,,
udemy,CLF-01,41,"An AI-powered Forex trading application consumes thousands of data sets to train its machine learning model. The application’s workload requires a high-performance, parallel hot storage to process the training datasets concurrently. It also needs cost-effective cold storage to archive those datasets that yield low profit.Which of the following Amazon storage services should the developer use?",B,B,Use Amazon FSx For Windows File Server and Amazon S3 for hot and cold storage respectively.,Use Amazon FSx For Lustre and Amazon S3 for hot and cold storage respectively.,Use Amazon Elastic File System and Amazon S3 for hot and cold storage respectively.,Use Amazon FSx For Lustre and Amazon EBS Provisioned IOPS SSD (io1) volumes for hot and cold storage respectively.,,,,AI 기반 Forex 거래 애플리케이션은 기계 학습 모델을 교육하기 위해 수천 개의 데이터 세트를 사용합니다. 애플리케이션의 워크로드에는 교육 데이터 세트를 동시에 처리하기 위한 고성능 병렬 핫 스토리지가 필요합니다. 또한 수익이 낮은 데이터 세트를 보관하려면 비용 효율적인 콜드 스토리지가 필요합니다.다음 중 개발자가 사용해야 하는 Amazon 스토리지 서비스는 무엇입니까?,핫 스토리지와 콜드 스토리지에는 각각 Amazon FSx For Windows File Server 및 Amazon S3를 사용하십시오.,핫 스토리지와 콜드 스토리지에는 각각 Amazon FSx For Lustre와 Amazon S3를 사용하십시오.,핫 스토리지와 콜드 스토리지에 각각 Amazon Elastic File System과 Amazon S3를 사용합니다.,핫 스토리지와 콜드 스토리지에 각각 Amazon FSx For Lustre 및 Amazon EBS 프로비저닝된 IOPS SSD(io1) 볼륨을 사용합니다.,,,0,,
udemy,CLF-01,42,"An online cryptocurrency exchange platform is hosted in AWS which uses ECS Cluster and RDS in Multi-AZ Deployments configuration. The application is heavily using the RDS instance to process complex read and write database operations. To maintain the reliability, availability, and performance of your systems, you have to closely monitor how the different processes or threads on a DB instance use the CPU, including the percentage of the CPU bandwidth and total memory consumed by each process.   Which of the following is the most suitable solution to properly monitor your database?",D,D,"Create a script that collects and publishes custom metrics to CloudWatch, which tracks the real-time CPU Utilization of the RDS instance, and then set up a custom CloudWatch dashboard to view the metrics.",Use Amazon CloudWatch to monitor the CPU Utilization of your database.,Check the CPU% and MEM% metrics which are readily available in the Amazon RDS console that shows the percentage of the CPU bandwidth and total memory consumed by each database process of your RDS instance.,Enable Enhanced Monitoring in RDS.,,,,"온라인 암호화폐 교환 플랫폼은 다중 AZ 배포 구성에서 ECS 클러스터 및 RDS를 사용하는 AWS에서 호스팅됩니다. 애플리케이션은 RDS 인스턴스를 많이 사용하여 복잡한 읽기 및 쓰기 데이터베이스 작업을 처리합니다. 시스템의 안정성, 가용성 및 성능을 유지하려면 CPU 대역폭의 비율과 각 프로세스에서 사용하는 총 메모리를 포함하여 DB 인스턴스의 여러 프로세스 또는 스레드가 CPU를 사용하는 방식을 면밀히 모니터링해야 합니다.   다음 중 데이터베이스를 올바르게 모니터링하는 데 가장 적합한 솔루션은 무엇입니까?",RDS 인스턴스의 실시간 CPU 사용률을 추적하는 CloudWatch에 사용자 지정 지표를 수집하고 게시하는 스크립트를 만든 다음 사용자 지정 CloudWatch 대시보드를 설정하여 지표를 봅니다.,Amazon CloudWatch를 사용하여 데이터베이스의 CPU 사용률을 모니터링합니다.,RDS 인스턴스의 각 데이터베이스 프로세스에서 소비하는 총 메모리와 CPU 대역폭의 백분율을 보여주는 Amazon RDS 콘솔에서 쉽게 사용할 수 있는 CPU%및 지표를 확인하십시오 .MEM%,RDS에서 향상된 모니터링을 활성화합니다.,,,0,,
udemy,CLF-01,43,A tech company has a CRM application hosted on an Auto Scaling group of On-Demand EC2 instances with different instance types and sizes. The application is extensively used during office hours from 9 in the morning to 5 in the afternoon. Their users are complaining that the performance of the application is slow during the start of the day but then works normally after a couple of hours.Which of the following is the MOST operationally efficient solution to implement to ensure the application works properly at the beginning of the day?,C,C,Configure a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the CPU utilization.,Configure a Dynamic scaling policy for the Auto Scaling group to launch new instances based on the Memory utilization.,Configure a Scheduled scaling policy for the Auto Scaling group to launch new instances before the start of the day.,Configure a Predictive scaling policy for the Auto Scaling group to automatically adjust the number of Amazon EC2 instances,,,,기술 회사에는 인스턴스 유형과 크기가 다른 온디맨드 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅되는 CRM 애플리케이션이 있습니다. 오전 9시부터 오후 5시까지 근무 시간 동안 애플리케이션이 광범위하게 사용됩니다. 그들의 사용자는 하루를 시작할 때 응용 프로그램의 성능이 느리지만 몇 시간 후에는 정상적으로 작동한다고 불평하고 있습니다.다음 중 하루를 시작할 때 애플리케이션이 제대로 작동하도록 구현하는 가장 운영상 효율적인 솔루션은 무엇입니까?,Auto Scaling 그룹이 CPU 사용률에 따라 새 인스턴스를 시작하도록 동적 조정 정책을 구성합니다.,메모리 사용률을 기반으로 새 인스턴스를 시작하도록 Auto Scaling 그룹에 대한 동적 조정 정책을 구성합니다.,하루가 시작되기 전에 새 인스턴스를 시작하도록 Auto Scaling 그룹에 대한 예약 조정 정책을 구성합니다.,Amazon EC2 인스턴스 수를 자동으로 조정하도록 Auto Scaling 그룹에 대한 예측 조정 정책 구성,,,0,,
udemy,CLF-01,44,A Solutions Architect identified a series of DDoS attacks while monitoring the VPC. The Architect needs to fortify the current cloud infrastructure to protect the data of the clients.Which of the following is the most suitable solution to mitigate these kinds of attacks?,B,B,"Using the AWS Firewall Manager, set up a security layer that will prevent SYN floods, UDP reflection attacks, and other DDoS attacks.",Use AWS Shield Advanced to detect and mitigate DDoS attacks.,A combination of Security Groups and Network Access Control Lists to only allow authorized traffic to access your VPC.,"Set up a web application firewall using AWS WAF to filter, monitor, and block HTTP traffic.",,,,Solutions Architect는 VPC를 모니터링하는 동안 일련의 DDoS 공격을 식별했습니다. Architect는 클라이언트의 데이터를 보호하기 위해 현재 클라우드 인프라를 강화해야 합니다.다음 중 이러한 종류의 공격을 완화하는 데 가장 적합한 솔루션은 무엇입니까?,"AWS Firewall Manager를 사용하여 SYN 플러드, UDP 반사 공격 및 기타 DDoS 공격을 방지하는 보안 계층을 설정합니다.",AWS Shield Advanced를 사용하여 DDoS 공격을 탐지하고 완화하십시오.,승인된 트래픽만 VPC에 액세스하도록 허용하는 보안 그룹과 네트워크 액세스 제어 목록의 조합입니다.,"AWS WAF를 사용하여 웹 애플리케이션 방화벽을 설정하여 HTTP 트래픽을 필터링, 모니터링 및 차단합니다.",,,0,,
udemy,CLF-01,45,A company wishes to query data that resides in multiple AWS accounts from a central data lake. Each account has its own Amazon S3 bucket that stores data unique to its business function. Users from different accounts must be granted access to the data lake based on their roles.Which solution will minimize overhead and costs while meeting the required access patterns?,C,C,Use AWS Control Tower to centrally manage each account's S3 buckets.,Use AWS Kinesis Firehose to consolidate data from multiple accounts into a single account.,Use AWS Lake Formation to consolidate data from multiple accounts into a single account.,Create a scheduled Lambda function for transferring data from multiple accounts to the S3 buckets of a central account,,,,회사는 중앙 데이터 레이크에서 여러 AWS 계정에 있는 데이터를 쿼리하려고 합니다. 각 계정에는 비즈니스 기능에 고유한 데이터를 저장하는 자체 Amazon S3 버킷이 있습니다. 다른 계정의 사용자는 역할에 따라 데이터 레이크에 대한 액세스 권한을 부여받아야 합니다.필요한 액세스 패턴을 충족하면서 오버헤드와 비용을 최소화하는 솔루션은 무엇입니까?,AWS Control Tower를 사용하여 각 계정의 S3 버킷을 중앙에서 관리합니다.,AWS Kinesis Firehose를 사용하여 여러 계정의 데이터를 단일 계정으로 통합합니다.,AWS Lake Formation을 사용하여 여러 계정의 데이터를 단일 계정으로 통합합니다.,여러 계정에서 중앙 계정의 S3 버킷으로 데이터를 전송하기 위한 예약된 Lambda 함수 생성,,,0,,
udemy,CLF-01,46,A popular social media website uses a CloudFront web distribution to serve their static contents to their millions of users around the globe. They are receiving a number of complaints recently that their users take a lot of time to log into their website. There are also occasions when their users are getting HTTP 504 errors. You are instructed by your manager to significantly reduce the user's login time to further optimize the system. Which of the following options should you use together to set up a cost-effective solution that can improve your application's performance? (Select TWO.),CE,CE,Deploy your application to multiple AWS regions to accommodate your users around the world. Set up a Route 53 record with latency routing policy to route incoming traffic to the region that provides the best latency to the user.,"Use multiple and geographically disperse VPCs to various AWS regions then create a transit VPC to connect all of your resources. In order to handle the requests faster, set up Lambda functions in each region using the AWS Serverless Application Model (SAM) service.","Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your Lambda functions to execute the authentication process in AWS locations closer to the users.","Configure your origin to add a Cache-Control max-age directive to your objects, and specify the longest practical value for max-age to increase the cache hit ratio of your CloudFront distribution.",Set up an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses.,,,인기 있는 소셜 미디어 웹 사이트는 CloudFront 웹 배포를 사용하여 전 세계 수백만 명의 사용자에게 정적 콘텐츠를 제공합니다. 최근 사용자가 웹 사이트에 로그인하는 데 많은 시간이 걸린다는 불만이 많이 접수되고 있습니다. 사용자에게 HTTP 504 오류가 발생하는 경우도 있습니다. 시스템을 더욱 최적화하기 위해 사용자의 로그인 시간을 크게 줄이도록 관리자로부터 지시를 받았습니다.애플리케이션의 성능을 향상시킬 수 있는 비용 효율적인 솔루션을 설정하기 위해 함께 사용해야 하는 옵션은 다음 중 무엇입니까? (2개를 선택하세요.),전 세계 사용자를 수용할 수 있도록 여러 AWS 리전에 애플리케이션을 배포합니다. 수신 트래픽을 사용자에게 최상의 지연 시간을 제공하는 리전으로 라우팅하도록 지연 시간 라우팅 정책으로 Route 53 레코드를 설정합니다.,다양한 AWS 지역에 여러 개의 지리적으로 분산된 VPC를 사용한 다음 전송 VPC를 생성하여 모든 리소스를 연결합니다. 요청을 더 빠르게 처리하려면 AWS Serverless Application Model(SAM) 서비스를 사용하여 각 리전에서 Lambda 함수를 설정하십시오.,CloudFront 웹 배포가 Lambda@Edge를 사용하여 사용자에게 제공하는 콘텐츠를 사용자 지정하면 Lambda 함수가 사용자에게 더 가까운 AWS 위치에서 인증 프로세스를 실행할 수 있습니다.,Cache-Control max-age객체에 지시문을 추가하도록 오리진을 구성 하고 max-ageCloudFront 배포의 캐시 적중률을 높이기 위해 가장 긴 실제 값을 지정합니다.,오리진이 두 개인 오리진 그룹을 생성하여 오리진 장애 조치를 설정합니다. 하나는 기본 오리진으로 지정하고 다른 하나는 기본 오리진이 특정 HTTP 상태 코드 실패 응답을 반환할 때 CloudFront가 자동으로 전환하는 두 번째 오리진으로 지정합니다.,,0,,
udemy,CLF-01,47,A company hosted a web application in an Auto Scaling group of EC2 instances. The IT manager is concerned about the over-provisioning of the resources that can cause higher operating costs. A Solutions Architect has been instructed to create a cost-effective solution without affecting the performance of the application.Which dynamic scaling policy should be used to satisfy this requirement?,A,A,Use target tracking scaling.,Use scheduled scaling.,Use suspend and resume scaling.,Use simple scaling.,,,,한 회사가 EC2 인스턴스의 Auto Scaling 그룹에서 웹 애플리케이션을 호스팅했습니다. IT 관리자는 더 높은 운영 비용을 초래할 수 있는 리소스의 과잉 프로비저닝에 대해 우려하고 있습니다. Solutions Architect는 애플리케이션의 성능에 영향을 미치지 않으면서 비용 효율적인 솔루션을 만들라는 지시를 받았습니다.이 요구 사항을 충족하려면 어떤 동적 조정 정책을 사용해야 합니까?,대상 추적 스케일링을 사용하십시오.,예약된 조정을 사용합니다.,일시 중지 및 재개 조정을 사용하십시오.,간단한 스케일링을 사용하십시오.,,,0,,
udemy,CLF-01,48,"A media company has an Amazon ECS Cluster, which uses the Fargate launch type, to host its news website. The application data are all stored in Amazon Keyspaces (for Apache Cassandra) with data-at-rest encryption enabled. The database credentials should be supplied using environment variables, to comply with strict security compliance. As the Solutions Architect, you have to ensure that the credentials are secure and that they cannot be viewed in plaintext on the cluster itself.Which of the following is the most suitable solution in this scenario that you can implement with minimal effort?",C,C,Store the database credentials in the ECS task definition file of the ECS Cluster and encrypt it with KMS. Store the task definition JSON file in Amazon Quantum Ledger Database (Amazon QLDB). Create an IAM role to the ECS task definition script that allows access to the Amazon QLDB and then pass the --cli-input-json parameter when calling the ECS register-task-definition action. Reference the task definition JSON file in the Amazon QLDB which contains the database credentials.,"Use the AWS Secrets Manager to store the database credentials and then encrypt them using AWS Certificate Manager (ACM). Create a resource-based policy for your Amazon ECS task execution role (taskRoleArn) and reference it with your task definition which allows access to both ACM and AWS Secrets Manager. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Secrets Manager secret which contains the sensitive data, to present to the container.","Use the AWS Systems Manager Parameter Store to keep the database credentials and then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role (taskRoleArn) and reference it with your task definition, which allows access to both KMS and the Parameter Store. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Systems Manager Parameter Store parameter containing the sensitive data to present to the container.","In the ECS task definition file of the ECS Cluster, store the database credentials to Amazon ECS Anywhere to centrally manage these sensitive data and securely transmit it to only those containers that need access to it. Allocate an IAM Role to the cluster to ensure that the passwords are only accessible by the ECS service tasks. Run the AWS IAM Access Analyzer to verify that the credentials can’t be viewed in plaintext.",,,,미디어 회사에는 Fargate 시작 유형을 사용하여 뉴스 웹 사이트를 호스팅하는 Amazon ECS 클러스터가 있습니다. 애플리케이션 데이터는 저장 데이터 암호화가 활성화된 Amazon Keyspaces(Apache Cassandra용)에 모두 저장됩니다. 엄격한 보안 규정 준수를 위해 환경 변수를 사용하여 데이터베이스 자격 증명을 제공해야 합니다. Solutions Architect는 자격 증명이 안전하고 클러스터 자체에서 일반 텍스트로 볼 수 없도록 해야 합니다.다음 중 이 시나리오에서 최소한의 노력으로 구현할 수 있는 가장 적합한 솔루션은 무엇입니까?,ECS 클러스터의 ECS 작업 정의 파일에 데이터베이스 자격 증명을 저장하고 KMS로 암호화합니다. 작업 정의 JSON 파일을 Amazon Quantum Ledger Database(Amazon QLDB)에 저장합니다. Amazon QLDB에 대한 액세스를 허용하는 ECS 작업 정의 스크립트에 대한 IAM 역할을 생성한 다음 --cli-input-jsonECS register-task-definition작업을 호출할 때 파라미터를 전달합니다. 데이터베이스 자격 증명이 포함된 Amazon QLDB에서 작업 정의 JSON 파일을 참조하십시오.,AWS Secrets Manager를 사용하여 데이터베이스 자격 증명을 저장한 다음 AWS Certificate Manager(ACM)를 사용하여 암호화합니다. Amazon ECS 작업 실행 역할( taskRoleArn)에 대한 리소스 기반 정책을 생성하고 ACM 및 AWS Secrets Manager 모두에 대한 액세스를 허용하는 작업 정의로 참조합니다. 컨테이너 정의 내에서 컨테이너에 설정할 환경 변수의 이름과 중요한 데이터가 포함된 Secrets Manager 암호의 전체 ARN으로 암호를 지정하여 컨테이너에 제공합니다.,AWS Systems Manager Parameter Store를 사용하여 데이터베이스 자격 증명을 유지한 다음 AWS KMS를 사용하여 암호화합니다. Amazon ECS 작업 실행 역할( taskRoleArn)에 대한 IAM 역할을 생성하고 KMS와 Parameter Store 모두에 대한 액세스를 허용하는 작업 정의로 참조합니다. 컨테이너 정의 내에서 컨테이너에 설정할 환경 변수의 이름과 컨테이너에 표시할 중요한 데이터가 포함된 Systems Manager Parameter Store 파라미터의 전체 ARN으로 비밀을 지정합니다.,ECS 클러스터의 ECS 작업 정의 파일에서 데이터베이스 자격 증명을 Amazon ECS Anywhere에 저장하여 이러한 민감한 데이터를 중앙에서 관리하고 액세스가 필요한 컨테이너에만 안전하게 전송합니다. ECS 서비스 작업에서만 암호에 액세스할 수 있도록 클러스터에 IAM 역할을 할당합니다. AWS IAM Access Analyzer를 실행하여 자격 증명을 일반 텍스트로 볼 수 없는지 확인합니다.,,,0,,
udemy,CLF-01,49,"A healthcare organization wants to build a system that can predict drug prescription abuse. They will gather real-time data from multiple sources, which includes Personally Identifiable Information (PII). It's crucial that this sensitive information is anonymized prior to landing in a NoSQL database for further processing.Which solution would meet the requirements?",A,A,"Ingest real-time data using Amazon Kinesis Data Stream. Use a Lambda function to anonymize the PII, then store it in Amazon DynamoDB.","Stream the data in an Amazon DynamoDB table. Enable DynamoDB Streams, and configure a function that performs anonymization on newly written items.",Create a data lake in Amazon S3 and use it as the primary storage for patient health data. Use an S3 trigger to run a Lambda function that performs anonymization. Send the anonymized data to Amazon DynamoDB,Deploy an Amazon Kinesis Data Firehose stream to capture and transform the streaming data. Deliver the anonymized data to Amazon Redshift for analysis.,,,,의료 기관에서 약물 처방 남용을 예측할 수 있는 시스템을 구축하려고 합니다. 그들은 개인 식별 정보(PII)를 포함하는 여러 소스로부터 실시간 데이터를 수집할 것입니다. 추가 처리를 위해 NoSQL 데이터베이스에 도달하기 전에 이 민감한 정보를 익명화하는 것이 중요합니다.요구 사항을 충족하는 솔루션은 무엇입니까?,Amazon Kinesis Data Stream을 사용하여 실시간 데이터를 수집합니다. Lambda 함수를 사용하여 PII를 익명화한 다음 Amazon DynamoDB에 저장합니다.,Amazon DynamoDB 테이블에서 데이터를 스트리밍합니다. DynamoDB 스트림을 활성화하고 새로 작성된 항목에 대해 익명화를 수행하는 기능을 구성합니다.,Amazon S3에서 데이터 레이크를 생성하고 이를 환자 건강 데이터의 기본 스토리지로 사용합니다. S3 트리거를 사용하여 익명화를 수행하는 Lambda 함수를 실행합니다. 익명화된 데이터를 Amazon DynamoDB로 전송,Amazon Kinesis Data Firehose 스트림을 배포하여 스트리밍 데이터를 캡처하고 변환합니다. 분석을 위해 익명화된 데이터를 Amazon Redshift로 전달합니다.,,,0,,
udemy,CLF-01,50,"A telecommunications company is planning to give AWS Console access to developers. Company policy mandates the use of identity federation and role-based access control. Currently, the roles are already assigned using groups in the corporate Active Directory. In this scenario, what combination of the following services can provide developers access to the AWS console? (Select TWO.)",BE,BE,AWS Directory Service Simple AD,IAM Roles,IAM Groups,Lambda,AWS Directory Service AD Connector,,,통신 회사는 개발자에게 AWS 콘솔 액세스 권한을 부여할 계획입니다. 회사 정책은 ID 페더레이션 및 역할 기반 액세스 제어를 사용하도록 규정하고 있습니다. 현재 회사 Active Directory의 그룹을 사용하여 역할이 이미 할당되었습니다. 이 시나리오에서 개발자에게 AWS 콘솔에 대한 액세스를 제공할 수 있는 다음 서비스의 조합은 무엇입니까? (2개를 선택하세요.),AWS 디렉터리 서비스 단순 AD,IAM 역할,IAM 그룹,람다,AWS 디렉터리 서비스 AD 커넥터,,0,,
udemy,CLF-01,51,"A content management system (CMS) is hosted on a fleet of auto-scaled, On-Demand EC2 instances that use Amazon Aurora as its database. Currently, the system stores the file documents that the users upload in one of the attached EBS Volumes. Your manager noticed that the system performance is quite slow and he has instructed you to improve the architecture of the system.In this scenario, what will you do to implement a scalable, high-available POSIX-compliant shared file system?",D,D,Create an S3 bucket and use this as the storage for the CMS,Use ElastiCache,Upgrading your existing EBS volumes to Provisioned IOPS SSD Volumes,Use EFS,,,,콘텐츠 관리 시스템(CMS)은 Amazon Aurora를 데이터베이스로 사용하는 자동 확장 온디맨드 EC2 인스턴스 플릿에서 호스팅됩니다. 현재 시스템은 연결된 EBS 볼륨 중 하나에 사용자가 업로드한 파일 문서를 저장합니다. 당신의 관리자는 시스템 성능이 매우 느리다는 것을 알아채고 당신에게 시스템 아키텍처를 개선하라고 지시했습니다.이 시나리오에서 확장 가능하고 가용성이 높은 POSIX 호환 공유 파일 시스템을 구현하기 위해 무엇을 하시겠습니까?,S3 버킷을 생성하고 이를 CMS용 스토리지로 사용,ElastiCache 사용,기존 EBS 볼륨을 프로비저닝된 IOPS SSD 볼륨으로 업그레이드,EFS 사용,,,0,,
udemy,CLF-01,52,A company has 3 DevOps engineers that are handling its software development and infrastructure management processes. One of the engineers accidentally deleted a file hosted in Amazon S3 which has caused disruption of service.What can the DevOps engineers do to prevent this from happening again?,A,A,Enable S3 Versioning and Multi-Factor Authentication Delete on the bucket.,Use S3 Infrequently Accessed storage to store the data.,Create an IAM bucket policy that disables delete operation.,Set up a signed URL for all users.,,,,회사에는 소프트웨어 개발 및 인프라 관리 프로세스를 처리하는 3명의 DevOps 엔지니어가 있습니다. 엔지니어 중 한 명이 Amazon S3에 호스팅된 파일을 실수로 삭제하여 서비스 중단을 초래했습니다.이러한 일이 다시 발생하지 않도록 DevOps 엔지니어는 무엇을 할 수 있습니까?,버킷에서 S3 버전 관리 및 Multi-Factor Authentication 삭제를 활성화합니다.,S3 Infrequently Accessed 스토리지를 사용하여 데이터를 저장합니다.,삭제 작업을 비활성화하는 IAM 버킷 정책을 생성합니다.,모든 사용자에 대해 서명된 URL을 설정합니다.,,,0,,
udemy,CLF-01,53,A government entity is conducting a population and housing census in the city. Each household information uploaded on their online portal is stored in encrypted files in Amazon S3. The government assigned its Solutions Architect to set compliance policies that verify data containing personally identifiable information (PII) in a manner that meets their compliance standards. They should also be alerted if there are potential policy violations with the privacy of their S3 buckets.Which of the following should the Architect implement to satisfy this requirement?,D,D,Set up and configure Amazon Fraud Detector to send out alert notifications whenever a security violation is detected on their Amazon S3 data.,Set up and configure Amazon Kendra to monitor malicious activity on their Amazon S3 data,Set up and configure Amazon Polly to scan for usage patterns on Amazon S3 data,Set up and configure Amazon Macie to monitor their Amazon S3 data.,,,,정부 기관이 도시에서 인구 및 주택 조사를 실시하고 있습니다. 온라인 포털에 업로드된 각 가구 정보는 Amazon S3에 암호화된 파일로 저장됩니다. 정부는 규정 준수 표준을 충족하는 방식으로 PII(개인 식별 정보)가 포함된 데이터를 확인하는 규정 준수 정책을 설정하도록 솔루션 아키텍트를 지정했습니다. 또한 S3 버킷의 프라이버시와 관련하여 잠재적인 정책 위반이 있는 경우 경고를 받아야 합니다.다음 중 Architect가 이 요구 사항을 충족하기 위해 구현해야 하는 것은 무엇입니까?,Amazon S3 데이터에서 보안 위반이 감지될 때마다 경고 알림을 보내도록 Amazon Fraud Detector를 설정하고 구성합니다.,Amazon S3 데이터에 대한 악의적인 활동을 모니터링하도록 Amazon Kendra 설정 및 구성,Amazon S3 데이터에서 사용 패턴을 스캔하도록 Amazon Polly 설정 및 구성,Amazon S3 데이터를 모니터링하도록 Amazon Macie를 설정하고 구성합니다.,,,0,,
udemy,CLF-01,54,"A cryptocurrency trading platform is using an API built in AWS Lambda and API Gateway. Due to the recent news and rumors about the upcoming price surge of Bitcoin, Ethereum and other cryptocurrencies, it is expected that the trading platform would have a significant increase in site visitors and new users in the coming days ahead. In this scenario, how can you protect the backend systems of the platform from traffic spikes?",B,B,"Switch from using AWS Lambda and API Gateway to a more scalable and highly available architecture using EC2 instances, ELB, and Auto Scaling.",Enable throttling limits and result caching in API Gateway.,Use CloudFront in front of the API Gateway to act as a cache.,Move the Lambda function in a VPC.,,,,"암호화폐 거래 플랫폼은 AWS Lambda 및 API Gateway에 내장된 API를 사용하고 있습니다. 비트코인, 이더리움 및 기타 암호화폐의 향후 가격 급등에 대한 최근 뉴스와 소문으로 인해 거래 플랫폼은 앞으로 며칠 동안 사이트 방문자와 신규 사용자가 크게 증가할 것으로 예상됩니다.이 시나리오에서 트래픽 급증으로부터 플랫폼의 백엔드 시스템을 어떻게 보호할 수 있습니까?","AWS Lambda 및 API Gateway 사용에서 EC2 인스턴스, ELB 및 Auto Scaling을 사용하여 보다 확장 가능하고 가용성이 높은 아키텍처로 전환하십시오.",API Gateway에서 제한 제한 및 결과 캐싱을 활성화합니다.,API 게이트웨이 앞에서 CloudFront를 사용하여 캐시 역할을 합니다.,VPC에서 Lambda 함수를 이동합니다.,,,0,,
udemy,CLF-01,55,"A Solutions Architect is hosting a website in an Amazon S3 bucket named tutorialsdojo. The users load the website using the following URL: http://tutorialsdojo.s3-website-us-east-1.amazonaws.com and there is a new requirement to add a JavaScript on the webpages in order to make authenticated HTTP GET requests against the same bucket by using the Amazon S3 API endpoint (tutorialsdojo.s3.amazonaws.com). Upon testing, you noticed that the web browser blocks JavaScript from allowing those requests. Which of the following options is the MOST suitable solution that you should implement for this scenario?",B,B,Enable cross-account access.,Enable Cross-origin resource sharing (CORS) configuration in the bucket.,Enable Cross-Region Replication (CRR).,Enable Cross-Zone Load Balancing.,,,,Solutions Architect는 이름이 Amazon S3 버킷인 웹 사이트를 호스팅하고 있습니다 tutorialsdojo. 사용자는 다음 URL을 사용하여 웹 사이트를 로드합니다. Amazon S3 API 엔드포인트( )를 사용하여 동일한 버킷에 대해 http://tutorialsdojo.s3-website-us-east-1.amazonaws.com인증된 HTTP 요청을 만들기 위해 웹 페이지에 JavaScript를 추가해야 하는 새로운 요구 사항이 있습니다 . 테스트에서 웹 브라우저가 JavaScript가 이러한 요청을 허용하지 못하도록 차단하는 것을 발견했습니다.GETtutorialsdojo.s3.amazonaws.com다음 옵션 중 이 시나리오에 구현해야 하는 가장 적합한 솔루션은 무엇입니까?,교차 계정 액세스를 활성화합니다.,버킷에서 교차 출처 리소스 공유(CORS) 구성을 활성화합니다.,지역 간 복제(CRR)를 활성화합니다.,교차 영역 로드 밸런싱을 활성화합니다.,,,0,,
udemy,CLF-01,56,"An application hosted in EC2 consumes messages from an SQS queue and is integrated with SNS to send out an email to you once the process is complete. The Operations team received 5 orders but after a few hours, they saw 20 email notifications in their inbox. Which of the following could be the possible culprit for this issue?",D,D,The web application is set for long polling so the messages are being sent twice.,The web application is set to short polling so some messages are not being picked up,The web application does not have permission to consume messages in the SQS queue.,The web application is not deleting the messages in the SQS queue after it has processed them.,,,,EC2에서 호스팅되는 애플리케이션은 SQS 대기열의 메시지를 사용하고 SNS와 통합되어 프로세스가 완료되면 이메일을 보냅니다. 운영 팀은 5개의 주문을 받았지만 몇 시간 후 받은 편지함에 20개의 이메일 알림이 표시되었습니다.다음 중 이 문제의 원인일 수 있는 것은 무엇입니까?,웹 애플리케이션은 긴 폴링으로 설정되어 메시지가 두 번 전송됩니다.,웹 애플리케이션이 짧은 폴링으로 설정되어 일부 메시지가 선택되지 않습니다.,웹 애플리케이션에 SQS 대기열의 메시지를 사용할 수 있는 권한이 없습니다.,웹 애플리케이션은 메시지를 처리한 후 SQS 대기열의 메시지를 삭제하지 않습니다.,,,0,,
udemy,CLF-01,57,An online medical system hosted in AWS stores sensitive Personally Identifiable Information (PII) of the users in an Amazon S3 bucket. Both the master keys and the unencrypted data should never be sent to AWS to comply with the strict compliance and regulatory requirements of the company. Which S3 encryption technique should the Architect use?,C,C,Use S3 server-side encryption with a KMS managed key.,Use S3 client-side encryption with a KMS-managed customer master key.,Use S3 client-side encryption with a client-side master key.,Use S3 server-side encryption with customer provided key.,,,,AWS에서 호스팅되는 온라인 의료 시스템은 사용자의 민감한 개인 식별 정보(PII)를 Amazon S3 버킷에 저장합니다. 회사의 엄격한 규정 준수 및 규제 요구 사항을 준수하기 위해 마스터 키와 암호화되지 않은 데이터를 AWS로 보내서는 안 됩니다.Architect는 어떤 S3 암호화 기술을 사용해야 합니까?,KMS 관리형 키로 S3 서버 측 암호화를 사용합니다.,KMS 관리형 고객 마스터 키로 S3 클라이언트 측 암호화를 사용합니다.,클라이언트 측 마스터 키와 함께 S3 클라이언트 측 암호화를 사용합니다.,고객이 제공한 키로 S3 서버 측 암호화를 사용합니다.,,,0,,
udemy,CLF-01,58,"A software development company is using serverless computing with AWS Lambda to build and run applications without having to set up or manage servers. They have a Lambda function that connects to a MongoDB Atlas, which is a popular Database as a Service (DBaaS) platform and also uses a third party API to fetch certain data for their application. One of the developers was instructed to create the environment variables for the MongoDB database hostname, username, and password as well as the API credentials that will be used by the Lambda function for DEV, SIT, UAT, and PROD environments.Considering that the Lambda function is storing sensitive database and API credentials, how can this information be secured to prevent other developers in the team, or anyone, from seeing these credentials in plain text? Select the best option that provides maximum security.",B,B,"There is no need to do anything because, by default, AWS Lambda already encrypts the environment variables using the AWS Key Management Service.",Create a new KMS key and use it to enable encryption helpers that leverage on AWS Key Management Service to store and encrypt the sensitive information.,Enable SSL encryption that leverages on AWS CloudHSM to store and encrypt the sensitive information.,AWS Lambda does not provide encryption for the environment variables. Deploy your code to an EC2 instance instead.,,,,"소프트웨어 개발 회사는 서버를 설정하거나 관리할 필요 없이 애플리케이션을 구축하고 실행하기 위해 AWS Lambda와 함께 서버리스 컴퓨팅을 사용하고 있습니다. 그들은 대중적인 DBaaS(Database as a Service) 플랫폼인 MongoDB Atlas에 연결하고 타사 API를 사용하여 애플리케이션에 대한 특정 데이터를 가져오는 Lambda 함수를 가지고 있습니다. 개발자 중 한 명은 MongoDB 데이터베이스 호스트 이름, 사용자 이름 및 암호에 대한 환경 변수와 DEV, SIT, UAT 및 PROD 환경용 Lambda 함수에서 사용할 API 자격 증명을 생성하라는 지시를 받았습니다.Lambda 함수가 민감한 데이터베이스 및 API 자격 증명을 저장하고 있다는 점을 고려할 때, 팀의 다른 개발자 또는 누군가가 이러한 자격 증명을 일반 텍스트로 보지 못하도록 이 정보를 어떻게 보호할 수 있습니까? 최대 보안을 제공하는 최상의 옵션을 선택하십시오.",기본적으로 AWS Lambda는 이미 AWS Key Management Service를 사용하여 환경 변수를 암호화하기 때문에 아무 것도 할 필요가 없습니다.,새 KMS 키를 생성하고 이를 사용하여 AWS Key Management Service를 활용하여 중요한 정보를 저장하고 암호화하는 암호화 헬퍼를 활성화합니다.,중요한 정보를 저장하고 암호화하기 위해 AWS CloudHSM을 활용하는 SSL 암호화를 활성화합니다.,AWS Lambda는 환경 변수에 대한 암호화를 제공하지 않습니다. 대신 코드를 EC2 인스턴스에 배포하십시오.,,,0,,
udemy,CLF-01,59,"A medical records company is planning to store sensitive clinical trial data in an Amazon S3 repository with the object-level versioning feature enabled. The Solutions Architect is tasked with ensuring that no object can't be overwritten or deleted by any user in a period of one year only. To meet the strict compliance requirements, the root user of the company’s AWS account must also be restricted from making any changes to an object in the S3 bucket.Which of the following is the most secure way of storing the data in Amazon S3?",A,A,Enable S3 Object Lock in compliance mode with a retention period of one year.,Enable S3 Object Lock in governance mode with a retention period of one year.,Enable S3 Object Lock in governance mode with a legal hold of one year.,Enable S3 Object Lock in compliance mode with a legal hold of one year.,,,,의료 기록 회사는 객체 수준 버전 관리 기능이 활성화된 Amazon S3 리포지토리에 민감한 임상 시험 데이터를 저장할 계획입니다. Solutions Architect는 단 1년 동안 어떤 사용자도 개체를 덮어쓰거나 삭제할 수 없도록 하는 임무를 맡고 있습니다. 엄격한 규정 준수 요구 사항을 충족하려면 회사 AWS 계정의 루트 사용자가 S3 버킷의 객체를 변경하지 못하도록 제한해야 합니다.다음 중 Amazon S3에 데이터를 저장하는 가장 안전한 방법은 무엇입니까?,보존 기간이 1년인 규정 준수 모드에서 S3 객체 잠금을 활성화합니다.,보존 기간이 1년인 거버넌스 모드에서 S3 객체 잠금을 활성화합니다.,법적 보존 기간이 1년인 거버넌스 모드에서 S3 객체 잠금을 활성화합니다.,법적 보존 기간이 1년인 규정 준수 모드에서 S3 객체 잠금을 활성화합니다.,,,0,,
udemy,CLF-01,60,A company is using AWS Fargate to run a batch job whenever an object is uploaded to an Amazon S3 bucket. The minimum ECS task count is initially set to 1 to save on costs and should only be increased based on new objects uploaded to the S3 bucket.Which is the most suitable option to implement with the LEAST amount of effort?,A,A,Set up an Amazon EventBridge rule to detect S3 object PUT operations and set the target to the ECS cluster to run a new ECS task.,Set up an Amazon EventBridge rule to detect S3 object PUT operations and set the target to a Lambda function that will run the StartTask API command.,Set up an alarm in CloudWatch to monitor S3 object-level operations recorded on CloudTrail. Set two alarm actions to update the ECS task count to scale-out/scale-in depending on the S3 event.,Set up an alarm in Amazon CloudWatch to monitor S3 object-level operations that are recorded on CloudTrail. Create an Amazon EventBridge rule that triggers the ECS cluster when new CloudTrail events are detected.,,,,회사는 객체가 Amazon S3 버킷에 업로드될 때마다 AWS Fargate를 사용하여 배치 작업을 실행합니다. 최소 ECS 작업 수는 처음에 비용을 절약하기 위해 1로 설정되며 S3 버킷에 업로드된 새 객체에 따라 증가해야 합니다.최소한의 노력으로 구현하기에 가장 적합한 옵션은 무엇입니까?,Amazon EventBridge 규칙을 설정하여 S3 객체 PUT 작업을 감지하고 대상을 ECS 클러스터로 설정하여 새 ECS 작업을 실행합니다.,Amazon EventBridge 규칙을 설정하여 S3 객체 PUT 작업을 감지하고 대상을 API 명령을 실행할 Lambda 함수로 설정합니다 StartTask.,CloudTrail에 기록된 S3 개체 수준 작업을 모니터링하도록 CloudWatch에서 경보를 설정합니다. S3 이벤트에 따라 ECS 작업 수를 스케일 아웃/스케일 인으로 업데이트하도록 두 가지 경보 작업을 설정합니다.,Amazon CloudWatch에서 경보를 설정하여 CloudTrail에 기록된 S3 객체 수준 작업을 모니터링합니다. 새 CloudTrail 이벤트가 감지되면 ECS 클러스터를 트리거하는 Amazon EventBridge 규칙을 생성합니다.,,,0,,
udemy,CLF-01,61,A suite of web applications is hosted in an Auto Scaling group of EC2 instances across three Availability Zones and is configured with default settings. There is an Application Load Balancer that forwards the request to the respective target group on the URL path. The scale-in policy has been triggered due to the low number of incoming traffic to the application. Which EC2 instance will be the first one to be terminated by your Auto Scaling group?,C,C,The instance will be randomly selected by the Auto Scaling group,The EC2 instance which has the least number of user sessions,The EC2 instance launched from the oldest launch configuration,The EC2 instance which has been running for the longest time,,,,웹 애플리케이션 제품군은 3개의 가용 영역에 걸쳐 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅되며 기본 설정으로 구성됩니다. URL 경로의 각 대상 그룹에 요청을 전달하는 Application Load Balancer가 있습니다. 애플리케이션으로 들어오는 트래픽 수가 적어 축소 정책이 트리거되었습니다.Auto Scaling 그룹에서 가장 먼저 종료할 EC2 인스턴스는 무엇입니까?,인스턴스는 Auto Scaling 그룹에서 임의로 선택합니다.,사용자 세션 수가 가장 적은 EC2 인스턴스,가장 오래된 시작 구성에서 시작된 EC2 인스턴스,가장 오래 실행된 EC2 인스턴스,,,0,,
udemy,CLF-01,62,"An organization needs a persistent block storage volume that will be used for mission-critical workloads. The backup data will be stored in an object storage service and after 30 days, the data will be stored in a data archiving storage service.What should you do to meet the above requirement?",A,A,Attach an EBS volume in your EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to Amazon S3 Glacier.,Attach an EBS volume in your EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to Amazon S3 One Zone-IA.,Attach an instance store volume in your EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to Amazon S3 One Zone-IA.,Attach an instance store volume in your existing EC2 instance. Use Amazon S3 to store your backup data and configure a lifecycle policy to transition your objects to Amazon S3 Glacier.,,,,"조직에는 미션 크리티컬 워크로드에 사용할 영구 블록 스토리지 볼륨이 필요합니다. 백업 데이터는 오브젝트 스토리지 서비스에 저장되며, 30일 후에는 데이터 아카이빙 스토리지 서비스에 데이터가 저장됩니다.위의 요구 사항을 충족하려면 어떻게 해야 합니까?",EC2 인스턴스에 EBS 볼륨을 연결합니다. Amazon S3를 사용하여 백업 데이터를 저장하고 수명 주기 정책을 구성하여 객체를 Amazon S3 Glacier로 전환하십시오.,EC2 인스턴스에 EBS 볼륨을 연결합니다. Amazon S3를 사용하여 백업 데이터를 저장하고 수명 주기 정책을 구성하여 객체를 Amazon S3 One Zone-IA로 전환하십시오.,EC2 인스턴스에 인스턴스 스토어 볼륨을 연결합니다. Amazon S3를 사용하여 백업 데이터를 저장하고 수명 주기 정책을 구성하여 객체를 Amazon S3 One Zone-IA로 전환하십시오.,기존 EC2 인스턴스에 인스턴스 스토어 볼륨을 연결합니다. Amazon S3를 사용하여 백업 데이터를 저장하고 수명 주기 정책을 구성하여 객체를 Amazon S3 Glacier로 전환하십시오.,,,0,,
udemy,CLF-01,63,"A financial application is composed of an Auto Scaling group of EC2 instances, an Application Load Balancer, and a MySQL RDS instance in a Multi-AZ Deployments configuration. To protect the confidential data of your customers, you have to ensure that your RDS database can only be accessed using the profile credentials specific to your EC2 instances via an authentication token. As the Solutions Architect of the company, which of the following should you do to meet the above requirement?",C,C,Create an IAM Role and assign it to your EC2 instances which will grant exclusive access to your RDS instance.,Configure SSL in your application to encrypt the database connection to RDS.,Enable the IAM DB Authentication.,Use a combination of IAM and STS to restrict access to your RDS instance via a temporary token.,,,,"금융 애플리케이션은 다중 AZ 배포 구성에서 EC2 인스턴스의 Auto Scaling 그룹, 애플리케이션 로드 밸런서 및 MySQL RDS 인스턴스로 구성됩니다. 고객의 기밀 데이터를 보호하려면 인증 토큰을 통해 EC2 인스턴스에 특정한 프로필 자격 증명을 통해서만 RDS 데이터베이스에 액세스할 수 있도록 해야 합니다.회사의 솔루션 설계자로서 위의 요구 사항을 충족하려면 다음 중 무엇을 해야 합니까?",IAM 역할을 생성하고 RDS 인스턴스에 대한 독점 액세스 권한을 부여할 EC2 인스턴스에 할당합니다.,RDS에 대한 데이터베이스 연결을 암호화하도록 애플리케이션에서 SSL을 구성합니다.,IAM DB 인증을 활성화합니다.,IAM과 STS의 조합을 사용하여 임시 토큰을 통해 RDS 인스턴스에 대한 액세스를 제한합니다.,,,0,,
udemy,CLF-01,64,A company plans to migrate its on-premises workload to AWS. The current architecture is composed of a Microsoft SharePoint server that uses a Windows shared file storage. The Solutions Architect needs to use a cloud storage solution that is highly available and can be integrated with Active Directory for access control and authentication.Which of the following options can satisfy the given requirement?,B,B,Launch an Amazon EC2 Windows Server to mount a new S3 bucket as a file volume.,Create a file system using Amazon FSx for Windows File Server and join it to an Active Directory domain in AWS.,Create a file system using Amazon EFS and join it to an Active Directory domain.,Create a Network File System (NFS) file share using AWS Storage Gateway.,,,,회사에서 온프레미스 워크로드를 AWS로 마이그레이션할 계획입니다. 현재 아키텍처는 Windows 공유 파일 저장소를 사용하는 Microsoft SharePoint 서버로 구성되어 있습니다. Solutions Architect는 가용성이 높고 액세스 제어 및 인증을 위해 Active Directory와 통합할 수 있는 클라우드 스토리지 솔루션을 사용해야 합니다.다음 옵션 중 주어진 요구 사항을 충족할 수 있는 옵션은 무엇입니까?,Amazon EC2 Windows Server를 시작하여 새 S3 버킷을 파일 볼륨으로 탑재합니다.,Amazon FSx for Windows File Server를 사용하여 파일 시스템을 생성하고 AWS의 Active Directory 도메인에 조인합니다.,Amazon EFS를 사용하여 파일 시스템을 생성하고 Active Directory 도메인에 조인합니다.,AWS Storage Gateway를 사용하여 네트워크 파일 시스템(NFS) 파일 공유를 생성합니다.,,,0,,
udemy,CLF-01,65,"A logistics company plans to automate its order management application. The company wants to use SFTP file transfer in uploading business-critical documents. Since the files are confidential, the files need to be highly available and must be encrypted at rest. The files must also be automatically deleted a month after they are created.Which of the following options should be implemented to meet the company requirements with the least operation overhead?",B,B,Create an Amazon Elastic Filesystem (EFS) file system and enable encryption. Configure AWS Transfer for SFTP to securely upload files to the EFS file system. Apply an EFS lifecycle policy to delete files after 30 days.,Create an Amazon S3 bucket with encryption enabled. Launch an AWS Transfer for SFTP endpoint to securely upload files to the S3 bucket. Configure an S3 lifecycle rule to delete files after a month.,Provision an Amazon EC2 instance and install the SFTP service. Mount an encrypted EFS file system on the EC2 instance to store the uploaded files. Add a cron job to delete the files older than a month.,Create an Amazon S3 bucket with encryption enabled. Configure AWS Transfer for SFTP to securely upload files to the S3 bucket. Configure the retention policy on the SFTP server to delete files after a month.,,,,물류 회사는 주문 관리 애플리케이션을 자동화할 계획입니다. 회사는 업무상 중요한 문서를 업로드할 때 SFTP 파일 전송을 사용하려고 합니다. 파일은 기밀이므로 파일의 가용성이 높아야 하며 유휴 상태에서 암호화되어야 합니다. 또한 파일은 생성된 후 한 달 후에 자동으로 삭제되어야 합니다.최소한의 운영 오버헤드로 회사 요구 사항을 충족하려면 다음 중 어떤 옵션을 구현해야 합니까?,Amazon Elastic Filesystem(EFS) 파일 시스템을 생성하고 암호화를 활성화합니다. 파일을 EFS 파일 시스템에 안전하게 업로드하도록 AWS Transfer for SFTP를 구성합니다. 30일 후에 파일을 삭제하려면 EFS 수명 주기 정책을 적용합니다.,암호화가 활성화된 Amazon S3 버킷을 생성합니다. 파일을 S3 버킷에 안전하게 업로드하려면 SFTP용 AWS 전송 엔드포인트를 시작하십시오. 한 달 후에 파일을 삭제하도록 S3 수명 주기 규칙을 구성합니다.,Amazon EC2 인스턴스를 프로비저닝하고 SFTP 서비스를 설치합니다. 업로드된 파일을 저장하기 위해 EC2 인스턴스에 암호화된 EFS 파일 시스템을 탑재합니다. 크론 작업을 추가하여 한 달 이상 된 파일을 삭제하십시오.,암호화가 활성화된 Amazon S3 버킷을 생성합니다. 파일을 S3 버킷에 안전하게 업로드하도록 AWS Transfer for SFTP를 구성합니다. 한 달 후에 파일을 삭제하도록 SFTP 서버에서 보존 정책을 구성합니다.,,,0,,
udemy,CLF-01,66,A Solutions Architect is building a cloud infrastructure where EC2 instances require access to various AWS services such as S3 and Redshift. The Architect will also need to provide access to system administrators so they can deploy and test their changes.Which configuration should be used to ensure that the access to the resources is secured and not compromised? (Select TWO.),AD,AD,Assign an IAM role to the Amazon EC2 instance.,Store the AWS Access Keys in the EC2 instance.,Store the AWS Access Keys in ACM.,Enable Multi-Factor Authentication.,Assign an IAM user for each Amazon EC2 Instance.,,,Solutions Architect는 EC2 인스턴스가 S3 및 Redshift와 같은 다양한 AWS 서비스에 액세스해야 하는 클라우드 인프라를 구축하고 있습니다. Architect는 또한 시스템 관리자가 변경 사항을 배포하고 테스트할 수 있도록 액세스 권한을 제공해야 합니다. 리소스에 대한 액세스가 보호되고 손상되지 않도록 하려면 어떤 구성을 사용해야 합니까 ? (2개를 선택하세요.),Amazon EC2 인스턴스에 IAM 역할을 할당합니다.,EC2 인스턴스에 AWS 액세스 키를 저장합니다.,ACM에 AWS 액세스 키를 저장합니다.,다단계 인증을 활성화합니다.,각 Amazon EC2 인스턴스에 대해 IAM 사용자를 할당합니다.,,0,,
udemy,CLF-01,67,"A company is hosting its web application in an Auto Scaling group of EC2 instances behind an Application Load Balancer. Recently, the Solutions Architect identified a series of SQL injection attempts and cross-site scripting attacks to the application, which had adversely affected their production data. Which of the following should the Architect implement to mitigate this kind of attack?",C,C,"Using AWS Firewall Manager, set up security rules that block SQL injection and cross-site scripting attacks. Associate the rules to the Application Load Balancer.",Use Amazon Guard​Duty to prevent any further SQL injection and cross-site scripting attacks in your application.,Set up security rules that block SQL injection and cross-site scripting attacks in AWS Web Application Firewall (WAF). Associate the rules to the Application Load Balancer.,Block all the IP addresses where the SQL injection and cross-site scripting attacks originated using the Network Access Control List.,,,,"회사는 Application Load Balancer 뒤에 있는 EC2 인스턴스의 Auto Scaling 그룹에서 웹 애플리케이션을 호스팅하고 있습니다. 최근 Solutions Architect는 애플리케이션에 대한 일련의 SQL 삽입 시도 및 크로스 사이트 스크립팅 공격을 식별했으며, 이로 인해 생산 데이터에 악영향을 미쳤습니다.Architect는 이러한 종류의 공격을 완화하기 위해 다음 중 무엇을 구현해야 합니까?",AWS Firewall Manager를 사용하여 SQL 삽입 및 교차 사이트 스크립팅 공격을 차단하는 보안 규칙을 설정합니다. 규칙을 Application Load Balancer에 연결합니다.,Amazon Guard Duty를 사용하여 애플리케이션에서 추가적인 SQL 삽입 및 교차 사이트 스크립팅 공격을 방지하십시오.,AWS Web Application Firewall(WAF)에서 SQL 삽입 및 교차 사이트 스크립팅 공격을 차단하는 보안 규칙을 설정합니다. 규칙을 Application Load Balancer에 연결합니다.,네트워크 액세스 제어 목록을 사용하여 SQL 주입 및 교차 사이트 스크립팅 공격이 시작된 모든 IP 주소를 차단합니다.,,,0,,
udemy,CLF-01,68,An organization needs to control the access for several S3 buckets. They plan to use a gateway endpoint to allow access to trusted buckets.Which of the following could help you achieve this requirement?,D,D,Generate a bucket policy for trusted S3 buckets.,Generate a bucket policy for trusted VPCs.,Generate an endpoint policy for trusted VPCs.,Generate an endpoint policy for trusted S3 buckets.,,,,조직은 여러 S3 버킷에 대한 액세스를 제어해야 합니다. 게이트웨이 엔드포인트를 사용하여 신뢰할 수 있는 버킷에 대한 액세스를 허용할 계획입니다.다음 중 이 요건을 충족하는 데 도움이 되는 것은 무엇입니까?,신뢰할 수 있는 S3 버킷에 대한 버킷 정책을 생성합니다.,신뢰할 수 있는 VPC에 대한 버킷 정책을 생성합니다.,신뢰할 수 있는 VPC에 대한 엔드포인트 정책을 생성합니다.,신뢰할 수 있는 S3 버킷에 대한 엔드포인트 정책을 생성합니다.,,,0,,
udemy,CLF-01,69,"A media company is setting up an ECS batch architecture for its image processing application. It will be hosted in an Amazon ECS Cluster with two ECS tasks that will handle image uploads from the users and image processing. The first ECS task will process the user requests, store the image in an S3 input bucket, and push a message to a queue. The second task reads from the queue, parses the message containing the object name, and then downloads the object. Once the image is processed and transformed, it will upload the objects to the S3 output bucket. To complete the architecture, the Solutions Architect must create a queue and the necessary IAM permissions for the ECS tasks.Which of the following should the Architect do next?",D,D,Launch a new Amazon MQ queue and configure the second ECS task to read from it. Create an IAM role that the ECS tasks can assume in order to get access to the S3 buckets and Amazon MQ queue. Set the (EnableTaskIAMRole) option to true in the task definition.,Launch a new Amazon Kinesis Data Firehose and configure the second ECS task to read from it. Create an IAM role that the ECS tasks can assume in order to get access to the S3 buckets and Kinesis Data Firehose. Specify the ARN of the IAM Role in the (taskDefinitionArn) field of the task definition.,Launch a new Amazon AppStream 2.0 queue and configure the second ECS task to read from it. Create an IAM role that the ECS tasks can assume in order to get access to the S3 buckets and AppStream 2.0 queue. Declare the IAM Role (taskRoleArn) in the task definition.,Launch a new Amazon SQS queue and configure the second ECS task to read from it. Create an IAM role that the ECS tasks can assume in order to get access to the S3 buckets and SQS queue. Declare the IAM Role (taskRoleArn) in the task definition.,,,,미디어 회사는 이미지 처리 애플리케이션을 위한 ECS 배치 아키텍처를 설정하고 있습니다. 사용자의 이미지 업로드 및 이미지 처리를 처리하는 두 개의 ECS 작업이 있는 Amazon ECS 클러스터에서 호스팅됩니다. 첫 번째 ECS 작업은 사용자 요청을 처리하고 이미지를 S3 입력 버킷에 저장하고 메시지를 대기열에 푸시합니다. 두 번째 작업은 큐에서 읽고 개체 이름이 포함된 메시지를 구문 분석한 다음 개체를 다운로드합니다. 이미지가 처리 및 변환되면 객체를 S3 출력 버킷에 업로드합니다. 아키텍처를 완료하려면 Solutions Architect가 대기열과 ECS 작업에 필요한 IAM 권한을 생성해야 합니다.Architect는 다음 중 무엇을 해야 합니까?,새 Amazon MQ 대기열을 시작하고 여기에서 읽을 두 번째 ECS 작업을 구성합니다. S3 버킷 및 Amazon MQ 대기열에 대한 액세스 권한을 얻기 위해 ECS 작업이 맡을 수 있는 IAM 역할을 생성합니다. EnableTaskIAMRole작업 정의에서 ( ) 옵션을 true로 설정합니다 .,새 Amazon Kinesis Data Firehose를 시작하고 여기에서 읽을 두 번째 ECS 작업을 구성합니다. S3 버킷 및 Kinesis Data Firehose에 대한 액세스 권한을 얻기 위해 ECS 작업이 맡을 수 있는 IAM 역할을 생성합니다. taskDefinitionArn작업 정의의 ( ) 필드에 IAM 역할의 ARN을 지정합니다 .,새 Amazon AppStream 2.0 대기열을 시작하고 여기에서 읽을 두 번째 ECS 작업을 구성합니다. S3 버킷 및 AppStream 2.0 대기열에 대한 액세스 권한을 얻기 위해 ECS 작업이 맡을 수 있는 IAM 역할을 생성합니다. taskRoleArn작업 정의에서 IAM 역할( )을 선언합니다 .,새 Amazon SQS 대기열을 시작하고 여기에서 읽을 두 번째 ECS 작업을 구성합니다. S3 버킷 및 SQS 대기열에 대한 액세스 권한을 얻기 위해 ECS 작업이 맡을 수 있는 IAM 역할을 생성합니다. taskRoleArn작업 정의에서 IAM 역할( )을 선언합니다 .,,,0,,
udemy,CLF-01,70,"A business has a network of surveillance cameras installed within the premises of its data center. Management wants to leverage Artificial Intelligence to monitor and detect unauthorized personnel entering restricted areas. Should an unauthorized person be detected, the security team must be alerted via SMS.Which solution satisfies the requirement?",C,C,Replace the existing cameras with AWS IoT. Upload a face detection model to the AWS IoT devices and send them over to AWS Control Tower for checking and notification,Set up Amazon Managed Service for Prometheus to stream live feeds from the cameras. Use Amazon Fraud Detector to detect unauthorized personnel. Set the phone numbers of the security as subscribers to an SNS topic.,Use Amazon Kinesis Video to stream live feeds from the cameras. Use Amazon Rekognition to detect authorized personnel. Set the phone numbers of the security as subscribers to an SNS topic.,Configure Amazon Elastic Transcoder to stream live feeds from the cameras. Use Amazon Kendra to detect authorized personnel. Set the phone numbers of the security as subscribers to an SNS topic.,,,,기업에는 데이터 센터 구내에 감시 카메라 네트워크가 설치되어 있습니다. 경영진은 인공 지능을 활용하여 제한 구역에 무단으로 진입하는 사람을 모니터링하고 감지하기를 원합니다. 승인되지 않은 사람이 감지되면 보안 팀에 SMS를 통해 경고해야 합니다.어떤 솔루션이 요구 사항을 충족합니까?,기존 카메라를 AWS IoT로 교체합니다. 얼굴 인식 모델을 AWS IoT 장치에 업로드하고 확인 및 알림을 위해 AWS Control Tower로 보냅니다.,Prometheus용 Amazon Managed Service를 설정하여 카메라에서 라이브 피드를 스트리밍합니다. Amazon Fraud Detector를 사용하여 승인되지 않은 직원을 감지합니다. 보안의 전화번호를 SNS 주제에 대한 가입자로 설정합니다.,Amazon Kinesis Video를 사용하여 카메라에서 라이브 피드를 스트리밍합니다. Amazon Rekognition을 사용하여 승인된 직원을 감지합니다. 보안의 전화번호를 SNS 주제에 대한 가입자로 설정합니다.,카메라에서 라이브 피드를 스트리밍하도록 Amazon Elastic Transcoder를 구성합니다. Amazon Kendra를 사용하여 승인된 직원을 탐지합니다. 보안의 전화번호를 SNS 주제에 대한 가입자로 설정합니다.,,,0,,
udemy,CLF-01,71,"A startup has multiple AWS accounts that are assigned to its development teams. Since the company is projected to grow rapidly, the management wants to consolidate all of its AWS accounts into a multi-account setup. To simplify the login process on the AWS accounts, the management wants to utilize its existing directory service for user authenticationWhich combination of actions should a solutions architect recommend to meet these requirements? (Select TWO.)",AD,AD,"On the master account, use AWS Organizations to create a new organization with all features turned on. Invite the child accounts to this new organization.",Create an identity pool on Amazon Cognito and configure it to use the company’s directory service. Configure AWS IAM Identity Center (AWS Single Sign-On) to accept Cognito authentication.,"On the master account, use AWS Organizations to create a new organization with all features turned on. Enable the organization’s external authentication and point it to use the company’s directory service.",Configure AWS IAM Identity Center (AWS Single Sign-On) for the organization and integrate it with the company’s directory service using the Active Directory Connector,Create Service Control Policies (SCP) in the organization to manage the child accounts. Configure AWS IAM Identity Center (AWS Single Sign-On) to use AWS Directory Service.,,,스타트업에는 개발 팀에 할당된 여러 AWS 계정이 있습니다. 회사가 빠르게 성장할 것으로 예상되므로 경영진은 모든 AWS 계정을 다중 계정 설정으로 통합하려고 합니다. AWS 계정의 로그인 프로세스를 단순화하기 위해 경영진은 사용자 인증을 위해 기존 디렉토리 서비스를 활용하려고 합니다.이러한 요구 사항을 충족하기 위해 솔루션 설계자가 권장해야 하는 작업 조합은 무엇입니까? (2개를 선택하세요.),마스터 계정에서 AWS Organizations를 사용하여 모든 기능이 켜진 새 조직을 생성합니다. 이 새 조직에 하위 계정을 초대합니다.,Amazon Cognito에서 자격 증명 풀을 생성하고 회사의 디렉터리 서비스를 사용하도록 구성합니다. Cognito 인증을 수락하도록 AWS IAM Identity Center(AWS Single Sign-On)를 구성합니다.,마스터 계정에서 AWS Organizations를 사용하여 모든 기능이 켜진 새 조직을 생성합니다. 조직의 외부 인증을 활성화하고 회사의 디렉터리 서비스를 사용하도록 지정합니다.,조직에 대해 AWS IAM Identity Center(AWS Single Sign-On)를 구성하고 Active Directory Connector를 사용하여 회사의 디렉터리 서비스와 통합합니다.,하위 계정을 관리하기 위해 조직에서 서비스 제어 정책(SCP)을 만듭니다. AWS Directory Service를 사용하도록 AWS IAM Identity Center(AWS Single Sign-On)를 구성합니다.,,0,,
udemy,CLF-01,72,"A company is building an internal application that serves as a repository for images uploaded by a couple of users. Whenever a user uploads an image, it would be sent to Kinesis Data Streams for processing before it is stored in an S3 bucket. If the upload was successful, the application will return a prompt informing the user that the operation was successful. The entire processing typically takes about 5 minutes to finish.Which of the following options will allow you to asynchronously process the request to the application from upload request to Kinesis, S3, and return a reply in the most cost-effective manner?",C,C,Use a combination of Lambda and Step Functions to orchestrate service components and asynchronously process the requests.,Use a combination of SQS to queue the requests and then asynchronously process them using On-Demand EC2 Instances.,Replace the Kinesis Data Streams with an Amazon SQS queue. Create a Lambda function that will asynchronously process the requests.,Use a combination of SNS to buffer the requests and then asynchronously process them using On-Demand EC2 Instances.,,,,"한 회사에서 두 명의 사용자가 업로드한 이미지의 저장소 역할을 하는 내부 애플리케이션을 구축하고 있습니다. 사용자가 이미지를 업로드할 때마다 S3 버킷에 저장되기 전에 처리를 위해 Kinesis Data Streams로 전송됩니다. 업로드가 성공하면 애플리케이션은 사용자에게 작업이 성공했음을 알리는 프롬프트를 반환합니다. 전체 처리는 일반적으로 완료하는 데 약 5분이 걸립니다.다음 중 Kinesis, S3에 대한 업로드 요청에서 애플리케이션에 대한 요청을 비동기식으로 처리하고 가장 비용 효율적인 방식으로 응답을 반환할 수 있는 옵션은 무엇입니까?",Lambda와 Step Functions의 조합을 사용하여 서비스 구성 요소를 오케스트레이션하고 요청을 비동기식으로 처리합니다.,SQS 조합을 사용하여 요청을 대기열에 넣은 다음 온디맨드 EC2 인스턴스를 사용하여 비동기식으로 처리합니다.,Kinesis Data Streams를 Amazon SQS 대기열로 교체합니다. 요청을 비동기식으로 처리할 Lambda 함수를 생성합니다.,SNS 조합을 사용하여 요청을 버퍼링한 다음 온디맨드 EC2 인스턴스를 사용하여 비동기식으로 처리합니다.,,,0,,
udemy,CLF-01,73,A GraphQL API hosted is hosted in an Amazon EKS cluster with Fargate launch type and deployed using AWS SAM. The API is connected to an Amazon DynamoDB table with an Amazon DynamoDB Accelerator (DAX) as its data store. Both resources are hosted in the us-east-1 region.The AWS IAM authenticator for Kubernetes is integrated into the EKS cluster for role-based access control (RBAC) and cluster authentication. A solutions architect must improve network security by preventing database calls from traversing the public internet. An automated cross-account backup for the DynamoDB table is also required for long-term retention.Which of the following should the solutions architect implement to meet the requirement?,D,D,Create a DynamoDB interface endpoint. Associate the endpoint to the appropriate route table. Enable Point-in-Time Recovery (PITR) to restore the DynamoDB table to a particular point in time on the same or a different AWS account.,Create a DynamoDB gateway endpoint. Set up a Network Access Control List (NACL) rule that allows outbound traffic to the dynamodb.us-east-1.amazonaws.com gateway endpoint. Use the built-in on-demand DynamoDB backups for cross-account backup and recovery.,Create a DynamoDB interface endpoint. Set up a stateless rule using AWS Network Firewall to control all outbound traffic to only use the  dynamodb.us-east-1.amazonaws.com endpoint. Integrate the DynamoDB table with Amazon Timestream to allow point-in-time recovery from a different AWS account.,Create a DynamoDB gateway endpoint. Associate the endpoint to the appropriate route table. Use AWS Backup to automatically copy the on-demand DynamoDB backups to another AWS account for disaster recovery.,,,,호스팅되는 GraphQL API는 Fargate 시작 유형이 있는 Amazon EKS 클러스터에서 호스팅되고 AWS SAM을 사용하여 배포됩니다. API는 Amazon DynamoDB Accelerator(DAX)를 데이터 저장소로 사용하여 Amazon DynamoDB 테이블에 연결됩니다. 두 리소스 모두 us-east-1 리전에서 호스팅됩니다.Kubernetes용 AWS IAM 인증자는 역할 기반 액세스 제어(RBAC) 및 클러스터 인증을 위해 EKS 클러스터에 통합됩니다. 솔루션 설계자는 데이터베이스 호출이 공용 인터넷을 통과하지 못하도록 방지하여 네트워크 보안을 개선해야 합니다. 장기 보존을 위해서는 DynamoDB 테이블에 대한 자동 교차 계정 백업도 필요합니다.다음 중 요구 사항을 충족하기 위해 솔루션 설계자가 구현해야 하는 것은 무엇입니까?,DynamoDB 인터페이스 엔드포인트를 생성합니다. 엔드포인트를 적절한 라우팅 테이블에 연결합니다. PITR(특정 시점 복구)을 활성화하여 동일하거나 다른 AWS 계정의 특정 시점으로 DynamoDB 테이블을 복원합니다.,DynamoDB 게이트웨이 엔드포인트를 생성합니다. 게이트웨이 엔드포인트 에 대한 아웃바운드 트래픽을 허용하는 NACL(네트워크 액세스 제어 목록) 규칙을 설정합니다 dynamodb.us-east-1.amazonaws.com. 교차 계정 백업 및 복구를 위해 내장된 온디맨드 DynamoDB 백업을 사용합니다.,DynamoDB 인터페이스 엔드포인트를 생성합니다. AWS Network Firewall을 사용하여 상태 비저장 규칙을 설정하여   dynamodb.us-east-1.amazonaws.com엔드포인트만 사용하도록 모든 아웃바운드 트래픽을 제어합니다. DynamoDB 테이블을 Amazon Timestream과 통합하여 다른 AWS 계정에서 특정 시점으로 복구할 수 있습니다.,DynamoDB 게이트웨이 엔드포인트를 생성합니다. 엔드포인트를 적절한 라우팅 테이블에 연결합니다. 재해 복구를 위해 AWS Backup을 사용하여 온디맨드 DynamoDB 백업을 다른 AWS 계정에 자동으로 복사합니다.,,,0,,
udemy,CLF-01,74,A media company hosts large volumes of archive data that are about 250 TB in size on their internal servers. They have decided to move these data to S3 because of its durability and redundancy. The company currently has a 100 Mbps dedicated line connecting their head office to the Internet. Which of the following is the FASTEST and the MOST cost-effective way to import all these data to Amazon S3?,A,A,Order multiple AWS Snowball devices to upload the files to Amazon S3.,Establish an AWS Direct Connect connection then transfer the data over to S3.,Use AWS Snowmobile to transfer the data over to S3.,Upload it directly to S3,,,,미디어 회사는 내부 서버에서 크기가 약 250TB인 대용량 아카이브 데이터를 호스팅합니다. 그들은 내구성과 중복성 때문에 이러한 데이터를 S3로 옮기기로 결정했습니다. 이 회사는 현재 본사를 인터넷에 연결하는 100Mbps 전용 회선을 보유하고 있습니다.다음 중 이러한 모든 데이터를 Amazon S3로 가져오는 가장 빠르고 비용 효율적인 방법은 무엇입니까?,파일을 Amazon S3에 업로드하려면 여러 AWS Snowball 디바이스를 주문하십시오.,AWS Direct Connect 연결을 설정한 다음 데이터를 S3로 전송합니다.,AWS Snowmobile을 사용하여 데이터를 S3로 전송합니다.,S3에 직접 업로드,,,0,,
udemy,CLF-01,75,"A company has a cryptocurrency exchange portal that is hosted in an Auto Scaling group of EC2 instances behind an Application Load Balancer and is deployed across multiple AWS regions. The users can be found all around the globe, but the majority are from Japan and Sweden. Because of the compliance requirements in these two locations, you want the Japanese users to connect to the servers in the ap-northeast-1 Asia Pacific (Tokyo) region, while the Swedish users should be connected to the servers in the eu-west-1 EU (Ireland) region.Which of the following services would allow you to easily fulfill this requirement?",D,D,Set up a new CloudFront web distribution with the geo-restriction feature enabled.,Use Route 53 Weighted Routing policy.,Set up an Application Load Balancers that will automatically route the traffic to the proper AWS region.,Use Route 53 Geolocation Routing policy.,,,,회사에는 Application Load Balancer 뒤에 있는 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅되고 여러 AWS 지역에 배포되는 암호화폐 교환 포털이 있습니다. 사용자는 전 세계에서 찾을 수 있지만 대다수는 일본과 스웨덴 출신입니다. 이 두 위치의 규정 준수 요구 사항으로 인해 일본 사용자는 아시아 태평양(도쿄) 지역의 서버에 연결하고 ap-northeast-1스웨덴 사용자는 EU(아일랜드) 지역의 서버에 연결해야 합니다 eu-west-1.다음 중 이 요구 사항을 쉽게 충족할 수 있는 서비스는 무엇입니까?,지역 제한 기능이 활성화된 새로운 CloudFront 웹 배포를 설정합니다.,Route 53 가중 라우팅 정책을 사용합니다.,트래픽을 적절한 AWS 리전으로 자동 라우팅하는 Application Load Balancer를 설정합니다.,Route 53 지리적 위치 라우팅 정책을 사용합니다.,,,0,,
udemy,CLF-01,76,"A company has a serverless application made up of AWS Amplify, Amazon API Gateway and a Lambda function. The application is connected to an Amazon RDS MySQL database instance inside a private subnet. A Lambda Function URL is also implemented as the dedicated HTTPS endpoint for the function, which has the following value:https://12june1898pil1pinas.lambda-url.us-west-2.on.aws/ There are times during peak loads when the database throws a “too many connections” error preventing the users from accessing the application.Which solution could the company take to resolve the issue?",B,B,Increase the memory allocation of the Lambda function,Provision an RDS Proxy between the Lambda function and RDS database instance,Increase the concurrency limit of the Lambda function,Increase the rate limit of API Gateway,,,,"한 회사에 AWS Amplify, Amazon API Gateway 및 Lambda 함수로 구성된 서버리스 애플리케이션이 있습니다. 애플리케이션은 프라이빗 서브넷 내부의 Amazon RDS MySQL 데이터베이스 인스턴스에 연결됩니다. Lambda 함수 URL은 다음 값을 갖는 함수의 전용 HTTPS 엔드포인트로도 구현됩니다.https://12june1898pil1pinas.lambda-url.us-west-2.on.aws/ 최대 로드 중에 데이터베이스에서 ""너무 많은 연결"" 오류가 발생하여 사용자가 애플리케이션에 액세스하지 못하는 경우가 있습니다.회사가 문제를 해결하기 위해 취할 수 있는 솔루션은 무엇입니까?",Lambda 함수의 메모리 할당을 늘립니다.,Lambda 함수와 RDS 데이터베이스 인스턴스 간에 RDS 프록시 프로비저닝,Lambda 함수의 동시성 제한을 늘립니다.,API Gateway의 속도 제한을 늘립니다.,,,0,,
udemy,CLF-01,77,"An insurance company plans to implement a message filtering feature in their web application. To implement this solution, they need to create separate Amazon SQS queues for each type of quote request. The entire message processing should not exceed 24 hours.As the Solutions Architect of the company, which of the following should you do to meet the above requirement?",C,C,Create one Amazon SNS topic and configure the Amazon SQS queues to subscribe to the SNS topic. Publish the same messages to all SQS queues. Filter the messages in each queue based on the quote request type.,Create a data stream in Amazon Kinesis Data Streams. Use the Amazon Kinesis Client Library to deliver all the records to the designated SQS queues based on the quote request type.,Create one Amazon SNS topic and configure the Amazon SQS queues to subscribe to the SNS topic. Set the filter policies in the SNS subscriptions to publish the message to the designated SQS queue based on its quote request type.,Create multiple Amazon SNS topics and configure the Amazon SQS queues to subscribe to the SNS topics. Publish the message to the designated SQS queue based on the quote request type.,,,,보험 회사는 웹 애플리케이션에서 메시지 필터링 기능을 구현할 계획입니다. 이 솔루션을 구현하려면 각 유형의 견적 요청에 대해 별도의 Amazon SQS 대기열을 생성해야 합니다. 전체 메시지 처리는 24시간을 초과하지 않아야 합니다.회사의 솔루션 설계자로서 위의 요구 사항을 충족하려면 다음 중 무엇을 해야 합니까?,하나의 Amazon SNS 주제를 생성하고 SNS 주제를 구독하도록 Amazon SQS 대기열을 구성합니다. 모든 SQS 대기열에 동일한 메시지를 게시합니다. 견적 요청 유형에 따라 각 대기열의 메시지를 필터링합니다.,Amazon Kinesis Data Streams에서 데이터 스트림을 생성합니다. Amazon Kinesis Client Library를 사용하여 견적 요청 유형에 따라 지정된 SQS 대기열에 모든 레코드를 전달합니다.,하나의 Amazon SNS 주제를 생성하고 SNS 주제를 구독하도록 Amazon SQS 대기열을 구성합니다. 견적 요청 유형에 따라 지정된 SQS 대기열에 메시지를 게시하도록 SNS 구독에서 필터 정책을 설정합니다.,여러 Amazon SNS 주제를 생성하고 SNS 주제를 구독하도록 Amazon SQS 대기열을 구성합니다. 견적 요청 유형에 따라 지정된 SQS 대기열에 메시지를 게시합니다.,,,0,,
udemy,CLF-01,78,A company is running a custom application in an Auto Scaling group of Amazon EC2 instances. Several instances are failing due to insufficient swap space. The Solutions Architect has been instructed to troubleshoot the issue and effectively monitor the available swap space of each EC2 instance.Which of the following options fulfills this requirement?,A,A,Install the CloudWatch agent on each instance and monitor the SwapUtilization metric.,Create a CloudWatch dashboard and monitor the SwapUsed metric.,Create a new trail in AWS CloudTrail and configure Amazon CloudWatch Logs to monitor your trail logs.,Enable detailed monitoring on each instance and monitor the SwapUtilization metric.,,,,회사는 Amazon EC2 인스턴스의 Auto Scaling 그룹에서 사용자 지정 애플리케이션을 실행하고 있습니다. 스왑 공간이 부족하여 여러 인스턴스가 실패하고 있습니다. Solutions Architect는 문제를 해결하고 각 EC2 인스턴스의 사용 가능한 스왑 공간을 효과적으로 모니터링하라는 지시를 받았습니다.다음 중 이 요구 사항을 충족하는 옵션은 무엇입니까?,각 인스턴스에 CloudWatch 에이전트를 설치하고 SwapUtilization지표를 모니터링합니다.,CloudWatch 대시보드를 생성하고 SwapUsed지표를 모니터링합니다.,AWS CloudTrail에서 새 추적을 생성하고 추적 로그를 모니터링하도록 Amazon CloudWatch Logs를 구성합니다.,각 인스턴스에 대한 세부 모니터링을 활성화하고 SwapUtilization메트릭을 모니터링합니다.,,,0,,
udemy,CLF-01,79,"A company plans to migrate all of their applications to AWS. The Solutions Architect suggested to store all the data to EBS volumes. The Chief Technical Officer is worried that EBS volumes are not appropriate for the existing workloads due to compliance requirements, downtime scenarios, and IOPS performance.Which of the following are valid points in proving that EBS is the best service to use for migration? (Select TWO.)",AB,AB,"EBS volumes support live configuration changes while in production which means that you can modify the volume type, volume size, and IOPS capacity without service interruptions.",An EBS volume is off-instance storage that can persist independently from the life of an instance.,"Amazon EBS provides the ability to create snapshots (backups) of any EBS volume and write a copy of the data in the volume to Amazon RDS, where it is stored redundantly in multiple Availability Zones","When you create an EBS volume in an Availability Zone, it is automatically replicated on a separate AWS region to prevent data loss due to a failure of any single hardware component.",EBS volumes can be attached to any EC2 Instance in any Availability Zone.,,,"회사에서 모든 애플리케이션을 AWS로 마이그레이션할 계획입니다. Solutions Architect는 모든 데이터를 EBS 볼륨에 저장할 것을 제안했습니다. 최고 기술 책임자는 규정 준수 요구 사항, 다운타임 시나리오 및 IOPS 성능으로 인해 EBS 볼륨이 기존 워크로드에 적합하지 않다고 걱정합니다.다음 중 EBS가 마이그레이션에 사용하기에 가장 적합한 서비스임을 증명하는 유효한 포인트는 무엇입니까? (2개를 선택하세요.)","EBS 볼륨은 프로덕션 중에 라이브 구성 변경을 지원하므로 서비스 중단 없이 볼륨 유형, 볼륨 크기 및 IOPS 용량을 수정할 수 있습니다.",EBS 볼륨은 인스턴스 수명과 독립적으로 지속될 수 있는 인스턴스 외부 스토리지입니다.,Amazon EBS는 모든 EBS 볼륨의 스냅샷(백업)을 생성하고 볼륨의 데이터 복사본을 Amazon RDS에 기록하여 여러 가용 영역에 중복 저장하는 기능을 제공합니다.,가용 영역에서 EBS 볼륨을 생성하면 단일 하드웨어 구성 요소의 장애로 인한 데이터 손실을 방지하기 위해 별도의 AWS 리전에 자동으로 복제됩니다.,EBS 볼륨은 모든 가용 영역의 모든 EC2 인스턴스에 연결할 수 있습니다.,,0,,
udemy,CLF-01,80,"For data privacy, a healthcare company has been asked to comply with the Health Insurance Portability and Accountability Act (HIPAA). The company stores all its backups on an Amazon S3 bucket. It is required that data stored on the S3 bucket must be encrypted.What is the best option to do this? (Select TWO.)",CD,CD,Store the data in encrypted EBS snapshots.,Store the data on EBS volumes with encryption enabled instead of using Amazon S3.,Enable Server-Side Encryption on an S3 bucket to make use of AES-256 encryption.,"Before sending the data to Amazon S3 over HTTPS, encrypt the data locally first using your own encryption keys.",Enable Server-Side Encryption on an S3 bucket to make use of AES-128 encryption.,,,데이터 프라이버시를 위해 의료 회사는 HIPAA(Health Insurance Portability and Accountability Act)를 준수하도록 요청받았습니다. 회사는 모든 백업을 Amazon S3 버킷에 저장합니다. S3 버킷에 저장된 데이터는 암호화되어야 합니다.이 작업을 수행하는 가장 좋은 방법은 무엇입니까? (2개를 선택하세요.),암호화된 EBS 스냅샷에 데이터를 저장합니다.,Amazon S3를 사용하는 대신 암호화가 활성화된 EBS 볼륨에 데이터를 저장합니다.,AES-256 암호화를 사용하려면 S3 버킷에서 서버 측 암호화를 활성화하십시오.,HTTPS를 통해 Amazon S3로 데이터를 전송하기 전에 먼저 자체 암호화 키를 사용하여 로컬에서 데이터를 암호화합니다.,AES-128 암호화를 사용하려면 S3 버킷에서 서버 측 암호화를 활성화하십시오.,,0,,
udemy,CLF-01,81,A company has an enterprise web application hosted on Amazon ECS Docker containers that use an Amazon FSx for Lustre filesystem for its high-performance computing workloads. A warm standby environment is running in another AWS region for disaster recovery. A Solutions Architect was assigned to design a system that will automatically route the live traffic to the disaster recovery (DR) environment only in the event that the primary application stack experiences an outage.What should the Architect do to satisfy this requirement?,B,B,Set up a CloudWatch Alarm to monitor the primary Route 53 DNS endpoint and create a custom Lambda function. Execute the ChangeResourceRecordSets API call using the function to initiate the failover to the secondary DNS record.,Set up a failover routing policy configuration in Route 53 by adding a health check on the primary service endpoint. Configure Route 53 to direct the DNS queries to the secondary record when the primary resource is unhealthy. Configure the network access control list and the route table to allow Route 53 to send requests to the endpoints specified in the health checks. Enable the Evaluate Target Health option by setting it to Yes.,Set up a Weighted routing policy configuration in Route 53 by adding health checks on both the primary stack and the DR environment. Configure the network access control list and the route table to allow Route 53 to send requests to the endpoints specified in the health checks. Enable the Evaluate Target Health option by setting it to Yes.,Set up a CloudWatch Events rule to monitor the primary Route 53 DNS endpoint and create a custom Lambda function. Execute the ChangeResourceRecordSets API call using the function to initiate the failover to the secondary DNS record.,,,,회사에는 고성능 컴퓨팅 워크로드에 Amazon FSx for Lustre 파일 시스템을 사용하는 Amazon ECS Docker 컨테이너에서 호스팅되는 엔터프라이즈 웹 애플리케이션이 있습니다. 웜 대기 환경은 재해 복구를 위해 다른 AWS 리전에서 실행 중입니다. 기본 애플리케이션 스택이 중단되는 경우에만 재해 복구(DR) 환경으로 라이브 트래픽을 자동으로 라우팅하는 시스템을 설계하도록 솔루션 아키텍트가 배정되었습니다.Architect는 이 요구 사항을 충족하기 위해 무엇을 해야 합니까?,기본 Route 53 DNS 엔드포인트를 모니터링하고 사용자 지정 Lambda 함수를 생성하도록 CloudWatch 경보를 설정합니다. ChangeResourceRecordSets보조 DNS 레코드에 대한 장애 조치를 시작하는 함수를 사용하여 API 호출을 실행합니다 .,기본 서비스 엔드포인트에서 상태 확인을 추가하여 Route 53에서 장애 조치 라우팅 정책 구성을 설정합니다. 기본 리소스가 비정상일 때 DNS 쿼리를 보조 레코드로 보내도록 Route 53을 구성합니다. Route 53이 상태 확인에 지정된 엔드포인트로 요청을 보낼 수 있도록 네트워크 액세스 제어 목록과 라우팅 테이블을 구성합니다. Evaluate Target Health로 설정하여 옵션을 활성화합니다 Yes.,기본 스택과 DR 환경 모두에 상태 확인을 추가하여 Route 53에서 가중치 기반 라우팅 정책 구성을 설정합니다. Route 53이 상태 확인에 지정된 엔드포인트로 요청을 보낼 수 있도록 네트워크 액세스 제어 목록과 라우팅 테이블을 구성합니다. Evaluate Target Health로 설정하여 옵션을 활성화합니다 Yes.,기본 Route 53 DNS 엔드포인트를 모니터링하고 사용자 지정 Lambda 함수를 생성하도록 CloudWatch 이벤트 규칙을 설정합니다. ChangeResourceRecordSets보조 DNS 레코드에 대한 장애 조치를 시작하는 함수를 사용하여 API 호출을 실행합니다 .,,,0,,
udemy,CLF-01,82,"Both historical records and frequently accessed data are stored on an on-premises storage system. The amount of current data is growing at an exponential rate. As the storage’s capacity is nearing its limit, the company’s Solutions Architect has decided to move the historical records to AWS to free up space for the active data.Which of the following architectures deliver the best solution in terms of cost and operational management?",A,A,Use AWS DataSync to move the historical records from on-premises to AWS. Choose Amazon S3 Glacier Deep Archive to be the destination for the data.,Use AWS Storage Gateway to move the historical records from on-premises to AWS. Choose Amazon S3 Glacier Deep Archive to be the destination for the data.,Use AWS DataSync to move the historical records from on-premises to AWS. Choose Amazon S3 Standard to be the destination for the data. Modify the S3 lifecycle configuration to move the data from the Standard tier to Amazon S3 Glacier Deep Archive after 30 days.,Use AWS Storage Gateway to move the historical records from on-premises to AWS. Choose Amazon S3 Glacier to be the destination for the data. Modify the S3 lifecycle configuration to move the data from the Standard tier to Amazon S3 Glacier Deep Archive after 30 days.,,,,과거 기록과 자주 액세스하는 데이터는 온프레미스 스토리지 시스템에 저장됩니다. 현재 데이터의 양은 기하급수적으로 증가하고 있습니다. 스토리지의 용량이 한계에 가까워짐에 따라 회사의 Solutions Architect는 기록 레코드를 AWS로 이동하여 활성 데이터를 위한 공간을 확보하기로 결정했습니다.다음 중 비용 및 운영 관리 측면에서 최상의 솔루션을 제공하는 아키텍처는 무엇입니까?,AWS DataSync를 사용하여 온프레미스에서 AWS로 기록 레코드를 이동합니다. Amazon S3 Glacier Deep Archive를 데이터 대상으로 선택합니다.,AWS Storage Gateway를 사용하여 온프레미스에서 AWS로 기록 레코드를 이동합니다. Amazon S3 Glacier Deep Archive를 데이터 대상으로 선택합니다.,AWS DataSync를 사용하여 온프레미스에서 AWS로 기록 레코드를 이동합니다. 데이터의 대상으로 Amazon S3 Standard를 선택합니다. 30일 후에 표준 계층에서 Amazon S3 Glacier Deep Archive로 데이터를 이동하도록 S3 수명 주기 구성을 수정합니다.,AWS Storage Gateway를 사용하여 온프레미스에서 AWS로 기록 레코드를 이동합니다. Amazon S3 Glacier를 데이터의 대상으로 선택합니다. 30일 후에 표준 계층에서 Amazon S3 Glacier Deep Archive로 데이터를 이동하도록 S3 수명 주기 구성을 수정합니다.,,,0,,
udemy,CLF-01,83,A company developed a meal planning application that provides meal recommendations for the week as well as the food consumption of the users. The application resides on an EC2 instance which requires access to various AWS services for its day-to-day operations.Which of the following is the best way to allow the EC2 instance to access the S3 bucket and other AWS services?,D,D,Store the API credentials in the EC2 instance.,Add the API Credentials in the Security Group and assign it to the EC2 instance.,Store the API credentials in a bastion host.,Create a role in IAM and assign it to the EC2 instance.,,,,한 회사에서 일주일 동안의 식사 추천과 사용자의 음식 소비를 제공하는 식사 계획 애플리케이션을 개발했습니다. 애플리케이션은 일상적인 작업을 위해 다양한 AWS 서비스에 액세스해야 하는 EC2 인스턴스에 상주합니다.다음 중 EC2 인스턴스가 S3 버킷 및 기타 AWS 서비스에 액세스하도록 허용하는 가장 좋은 방법은 무엇입니까?,API 자격 증명을 EC2 인스턴스에 저장합니다.,보안 그룹에 API 자격 증명을 추가하고 EC2 인스턴스에 할당합니다.,배스천 호스트에 API 자격 증명을 저장합니다.,IAM에서 역할을 생성하고 EC2 인스턴스에 할당합니다.,,,0,,
udemy,CLF-01,84,A media company has two VPCs: VPC-1 and VPC-2 with peering connection between each other. VPC-1 only contains private subnets while VPC-2 only contains public subnets. The company uses a single AWS Direct Connect connection and a virtual interface to connect their on-premises network with VPC-1. Which of the following options increase the fault tolerance of the connection to VPC-1? (Select TWO.),CE,CE,Establish a hardware VPN over the Internet between VPC-2 and the on-premises network.,Establish a new AWS Direct Connect connection and private virtual interface in the same region as VPC-2.,Establish another AWS Direct Connect connection and private virtual interface in the same AWS region as VPC-1.,Use the AWS VPN CloudHub to create a new AWS Direct Connect connection and private virtual interface in the same region as VPC-2.,Establish a hardware VPN over the Internet between VPC-1 and the on-premises network.,,,미디어 회사에는 서로 피어링 연결이 있는 VPC-1과 VPC-2라는 두 개의 VPC가 있습니다. VPC-1에는 프라이빗 서브넷만 포함되고 VPC-2에는 퍼블릭 서브넷만 포함됩니다. 이 회사는 단일 AWS Direct Connect 연결과 가상 인터페이스를 사용하여 온프레미스 네트워크를 VPC-1과 연결합니다. 다음 중 VPC-1 연결의 내결함성을 높이는 옵션은 무엇입니까? (2개를 선택하세요.),VPC-2와 온프레미스 네트워크 간에 인터넷을 통해 하드웨어 VPN을 설정합니다.,VPC-2와 동일한 리전에서 새 AWS Direct Connect 연결 및 프라이빗 가상 인터페이스를 설정합니다.,VPC-1과 동일한 AWS 리전에서 다른 AWS Direct Connect 연결 및 프라이빗 가상 인터페이스를 설정합니다.,AWS VPN CloudHub를 사용하여 VPC-2와 동일한 리전에서 새 AWS Direct Connect 연결 및 프라이빗 가상 인터페이스를 생성합니다.,VPC-1과 온프레미스 네트워크 간에 인터넷을 통해 하드웨어 VPN을 설정합니다.,,0,,
udemy,CLF-01,85,"A music publishing company is building a multitier web application that requires a key-value store which will save the document models. Each model is composed of band ID, album ID, song ID, composer ID, lyrics, and other data. The web tier will be hosted in an Amazon ECS cluster with AWS Fargate launch type. Which of the following is the MOST suitable setup for the database-tier?",A,A,Launch a DynamoDB table.,Launch an Amazon RDS database with Read Replicas.,Launch an Amazon Aurora Serverless database.,Use Amazon WorkDocs to store the document models.,,,,"음악 출판 회사는 문서 모델을 저장할 키-값 저장소가 필요한 다중 계층 웹 애플리케이션을 구축하고 있습니다. 각 모델은 밴드 ID, 앨범 ID, 곡 ID, 작곡가 ID, 가사 및 기타 데이터로 구성됩니다. 웹 계층은 AWS Fargate 시작 유형의 Amazon ECS 클러스터에서 호스팅됩니다.다음 중 데이터베이스 계층에 가장 적합한 설정은 무엇입니까?",DynamoDB 테이블을 시작합니다.,읽기 전용 복제본으로 Amazon RDS 데이터베이스를 시작합니다.,Amazon Aurora Serverless 데이터베이스를 시작합니다.,Amazon WorkDocs를 사용하여 문서 모델을 저장합니다.,,,0,,
udemy,CLF-01,86,"A solutions architect is designing a three-tier website that will be hosted on an Amazon EC2 Auto Scaling group fronted by an Internet-facing Application Load Balancer (ALB). The website will persist data to an Amazon Aurora Serverless DB cluster, which will also be used for generating monthly reports.The company requires a network topology that follows a layered approach to reduce the impact of misconfigured security groups or network access lists. Web filtering must also be enabled to automatically stop traffic to known malicious URLs and to immediately drop requests coming from blacklisted fully qualified domain names (FQDNs).Which network topology provides the minimum resources needed for the website to work?",A,A,"Set up an Application Load Balancer deployed in a public subnet, then host the Auto Scaling Group of Amazon EC2 instances and the Aurora Serverless DB cluster in private subnets. Launch an AWS Network Firewall with the appropriate firewall policy to automatically stop traffic to known malicious URLs and drop requests coming from blacklisted FQDNs. Reroute your Amazon VPC network traffic through the firewall endpoints.",Set up an Application Load Balancer in front of an Auto Scaling group of Amazon EC2 instances with an Aurora Serverless DB cluster to persist data. Launch a NAT Gateway in a public subnet to restrict external services from initiating a connection to the EC2 instances and immediately drop requests from unauthorized FQDNs. Deploy all other resources in private subnets.,Set up an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer with an Aurora Serverless DB cluster to store application data. Deploy all resources in a public subnet. Configure host-based routing to the Application Load Balancer to stop traffic to known malicious URLs and drop requests from blacklisted FQDNs.,Set up an Application Load Balancer and a NAT Gateway deployed in public subnets. Launch the Auto Scaling Group of Amazon EC2 instances and Aurora Serverless DB cluster in private subnets. Directly integrate the AWS Network Firewall with the Application Load Balancer to automatically stop traffic to known malicious URLs and drop requests coming from blacklisted FQDNs.,,,,솔루션 설계자는 인터넷에 연결된 ALB(Application Load Balancer)가 전면에 있는 Amazon EC2 Auto Scaling 그룹에서 호스팅될 3계층 웹 사이트를 설계하고 있습니다. 웹 사이트는 데이터를 Amazon Aurora 서버리스 DB 클러스터에 유지하며 월별 보고서 생성에도 사용됩니다.회사에는 잘못 구성된 보안 그룹 또는 네트워크 액세스 목록의 영향을 줄이기 위해 계층화된 접근 방식을 따르는 네트워크 토폴로지가 필요합니다. 또한 알려진 악성 URL에 대한 트래픽을 자동으로 중지하고 블랙리스트에 포함된 FQDN(정규화된 도메인 이름)에서 오는 요청을 즉시 삭제하려면 웹 필터링을 활성화해야 합니다.웹 사이트가 작동하는 데 필요한 최소 리소스를 제공하는 네트워크 토폴로지는 무엇입니까?,퍼블릭 서브넷에 배포된 Application Load Balancer를 설정한 다음 프라이빗 서브넷에서 Amazon EC2 인스턴스 및 Aurora Serverless DB 클러스터의 Auto Scaling 그룹을 호스팅합니다. 적절한 방화벽 정책으로 AWS 네트워크 방화벽을 시작하여 알려진 악성 URL에 대한 트래픽을 자동으로 중지하고 블랙리스트에 있는 FQDN에서 오는 요청을 삭제합니다. 방화벽 엔드포인트를 통해 Amazon VPC 네트워크 트래픽을 다시 라우팅합니다.,Aurora 서버리스 DB 클러스터가 있는 Amazon EC2 인스턴스의 Auto Scaling 그룹 앞에 Application Load Balancer를 설정하여 데이터를 유지합니다. 퍼블릭 서브넷에서 NAT 게이트웨이를 시작하여 외부 서비스가 EC2 인스턴스에 대한 연결을 시작하지 못하도록 제한하고 승인되지 않은 FQDN의 요청을 즉시 삭제합니다. 프라이빗 서브넷에 다른 모든 리소스를 배포합니다.,애플리케이션 데이터를 저장하기 위해 Aurora Serverless DB 클러스터가 있는 Application Load Balancer 뒤에 Amazon EC2 인스턴스의 Auto Scaling 그룹을 설정합니다. 퍼블릭 서브넷에 모든 리소스를 배포합니다. Application Load Balancer에 대한 호스트 기반 라우팅을 구성하여 알려진 악성 URL에 대한 트래픽을 중지하고 블랙리스트에 포함된 FQDN의 요청을 삭제합니다.,퍼블릭 서브넷에 배포된 Application Load Balancer 및 NAT 게이트웨이를 설정합니다. 프라이빗 서브넷에서 Amazon EC2 인스턴스 및 Aurora 서버리스 DB 클러스터의 Auto Scaling 그룹을 시작합니다. AWS Network Firewall을 Application Load Balancer와 직접 통합하여 알려진 악성 URL에 대한 트래픽을 자동으로 중지하고 블랙리스트에 있는 FQDN에서 오는 요청을 삭제합니다.,,,0,,
udemy,CLF-01,87,An accounting application uses an RDS database configured with Multi-AZ deployments to improve availability. What would happen to RDS if the primary database instance fails?,C,C,The IP address of the primary DB instance is switched to the standby DB instance.,The primary database instance will reboot.,The canonical name record (CNAME) is switched from the primary to standby instance.,A new database instance is created in the standby Availability Zone.,,,,회계 애플리케이션은 가용성을 향상시키기 위해 다중 AZ 배포로 구성된 RDS 데이터베이스를 사용합니다. 기본 데이터베이스 인스턴스가 실패하면 RDS는 어떻게 됩니까?,기본 DB 인스턴스의 IP 주소는 대기 DB 인스턴스로 전환됩니다.,기본 데이터베이스 인스턴스가 재부팅됩니다.,정식 이름 레코드(CNAME)가 기본 인스턴스에서 대기 인스턴스로 전환됩니다.,대기 가용 영역에 새 데이터베이스 인스턴스가 생성됩니다.,,,0,,
udemy,CLF-01,88,A company has developed public APIs hosted in Amazon EC2 instances behind an Elastic Load Balancer. The APIs will be used by various clients from their respective on-premises data centers. A Solutions Architect received a report that the web service clients can only access trusted IP addresses whitelisted on their firewalls.What should you do to accomplish the above requirement?,C,C,Create a CloudFront distribution whose origin points to the private IP addresses of your web servers.,Associate an Elastic IP address to an Application Load Balancer.,Associate an Elastic IP address to a Network Load Balancer.,Create an Alias Record in Route 53 which maps to the DNS name of the load balancer.,,,,한 회사에서 Elastic Load Balancer 뒤의 Amazon EC2 인스턴스에서 호스팅되는 퍼블릭 API를 개발했습니다. API는 각 온프레미스 데이터 센터의 다양한 클라이언트에서 사용됩니다. Solutions Architect는 웹 서비스 클라이언트가 방화벽에 허용된 신뢰할 수 있는 IP 주소에만 액세스할 수 있다는 보고를 받았습니다.위의 요구 사항을 충족하려면 어떻게 해야 합니까?,오리진이 웹 서버의 프라이빗 IP 주소를 가리키는 CloudFront 배포를 만듭니다.,Elastic IP 주소를 Application Load Balancer에 연결합니다.,탄력적 IP 주소를 Network Load Balancer에 연결합니다.,로드 밸런서의 DNS 이름에 매핑되는 Route 53에서 별칭 레코드를 생성합니다.,,,0,,
udemy,CLF-01,89,"A company has a dynamic web app written in MEAN stack that is going to be launched in the next month. There is a probability that the traffic will be quite high in the first couple of weeks. In the event of a load failure, how can you set up DNS failover to a static website?",C,C,Add more servers in case the application fails.,Duplicate the exact application architecture in another region and configure DNS weight-based routing.,Use Route 53 with the failover option to a static S3 website bucket or CloudFront distribution.,Enable failover to an application hosted in an on-premises data center.,,,,한 회사에서 다음 달에 출시할 MEAN 스택으로 작성된 동적 웹 앱을 보유하고 있습니다. 처음 몇 주 동안 트래픽이 상당히 높을 가능성이 있습니다. 로드 실패 시 정적 웹 사이트에 대한 DNS 장애 조치를 어떻게 설정할 수 있습니까?,응용 프로그램이 실패할 경우를 대비하여 더 많은 서버를 추가하십시오.,다른 지역에 정확한 애플리케이션 아키텍처를 복제하고 DNS 가중치 기반 라우팅을 구성합니다.,정적 S3 웹 사이트 버킷 또는 CloudFront 배포에 대한 장애 조치 옵션과 함께 Route 53을 사용하십시오.,온프레미스 데이터 센터에서 호스팅되는 애플리케이션에 대한 장애 조치를 활성화합니다.,,,0,,
udemy,CLF-01,90,A company is receiving semi-structured and structured data from different sources every day. The Solutions Architect plans to use big data processing frameworks to analyze vast amounts of data and access it using various business intelligence tools and standard SQL queries.Which of the following provides the MOST high-performing solution that fulfills this requirement?,B,B,Use Amazon Kinesis Data Analytics and store the processed data in Amazon DynamoDB.,Create an Amazon EMR cluster and store the processed data in Amazon Redshift.,Use AWS Glue and store the processed data in Amazon S3.,Create an Amazon EC2 instance and store the processed data in Amazon EBS.,,,,회사는 매일 다양한 소스로부터 반구조화 및 구조화 데이터를 수신하고 있습니다. Solutions Architect는 빅 데이터 처리 프레임워크를 사용하여 방대한 양의 데이터를 분석하고 다양한 비즈니스 인텔리전스 도구와 표준 SQL 쿼리를 사용하여 액세스할 계획입니다.다음 중 이 요구 사항을 충족하는 가장 고성능 솔루션을 제공하는 것은 무엇입니까?,Amazon Kinesis Data Analytics를 사용하고 처리된 데이터를 Amazon DynamoDB에 저장합니다.,Amazon EMR 클러스터를 생성하고 처리된 데이터를 Amazon Redshift에 저장합니다.,AWS Glue를 사용하고 처리된 데이터를 Amazon S3에 저장합니다.,Amazon EC2 인스턴스를 생성하고 처리된 데이터를 Amazon EBS에 저장합니다.,,,0,,
udemy,CLF-01,91,A company plans to conduct a network security audit. The web application is hosted on an Auto Scaling group of EC2 Instances with an Application Load Balancer in front to evenly distribute the incoming traffic. A Solutions Architect has been tasked to enhance the security posture of the company’s cloud infrastructure and minimize the impact of DDoS attacks on its resources.Which of the following is the most effective solution that should be implemented?,A,A,Configure Amazon CloudFront distribution and set Application Load Balancer as the origin. Create a rate-based web ACL rule using AWS WAF and associate it with Amazon CloudFront.,Configure Amazon CloudFront distribution and set a Network Load Balancer as the origin. Use VPC Flow Logs to monitor abnormal traffic patterns. Set up a custom AWS Lambda function that processes the flow logs and invokes Amazon SNS for notification.,Configure Amazon CloudFront distribution and set an Application Load Balancer as the origin. Create a security group rule and deny all the suspicious addresses. Use Amazon SNS for notification.,Configure Amazon CloudFront distribution and set a Network Load Balancer as the origin. Use Amazon GuardDuty to block suspicious hosts based on its security findings. Set up a custom AWS Lambda function that processes the security logs and invokes Amazon SNS for notification.,,,,회사에서 네트워크 보안 감사를 수행할 계획입니다. 웹 애플리케이션은 들어오는 트래픽을 고르게 분산하기 위해 Application Load Balancer가 앞에 있는 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅됩니다. Solutions Architect는 회사 클라우드 인프라의 보안 태세를 강화하고 리소스에 대한 DDoS 공격의 영향을 최소화하는 임무를 받았습니다.다음 중 구현해야 하는 가장 효과적인 솔루션은 무엇입니까?,Amazon CloudFront 배포를 구성하고 Application Load Balancer를 오리진으로 설정합니다. AWS WAF를 사용하여 속도 기반 웹 ACL 규칙을 생성하고 이를 Amazon CloudFront와 연결합니다.,Amazon CloudFront 배포를 구성하고 Network Load Balancer를 오리진으로 설정합니다. VPC 흐름 로그를 사용하여 비정상적인 트래픽 패턴을 모니터링합니다. 흐름 로그를 처리하고 알림을 위해 Amazon SNS를 호출하는 사용자 지정 AWS Lambda 함수를 설정합니다.,Amazon CloudFront 배포를 구성하고 Application Load Balancer를 오리진으로 설정합니다. 보안 그룹 규칙을 만들고 모든 의심스러운 주소를 거부합니다. 알림을 위해 Amazon SNS를 사용하십시오.,Amazon CloudFront 배포를 구성하고 Network Load Balancer를 오리진으로 설정합니다. Amazon GuardDuty를 사용하여 보안 결과를 기반으로 의심스러운 호스트를 차단합니다. 보안 로그를 처리하고 알림을 위해 Amazon SNS를 호출하는 사용자 지정 AWS Lambda 함수를 설정합니다.,,,0,,
udemy,CLF-01,92,"A start-up company that offers an intuitive financial data analytics service has consulted you about their AWS architecture. They have a fleet of Amazon EC2 worker instances that process financial data and then outputs reports which are used by their clients. You must store the generated report files in a durable storage. The number of files to be stored can grow over time as the start-up company is expanding rapidly overseas and hence, they also need a way to distribute the reports faster to clients located across the globe.  Which of the following is a cost-efficient and scalable storage option that you should use for this scenario?",C,C,Use multiple EC2 instance stores for data storage and ElastiCache as the CDN.,Use Amazon Glacier as the data storage and ElastiCache as the CDN.,Use Amazon S3 as the data storage and CloudFront as the CDN.,Use Amazon Redshift as the data storage and CloudFront as the CDN.,,,,직관적인 금융 데이터 분석 서비스를 제공하는 스타트업 회사에서 AWS 아키텍처에 대해 상담했습니다. 재무 데이터를 처리한 다음 고객이 사용하는 보고서를 출력하는 Amazon EC2 작업자 인스턴스를 보유하고 있습니다. 생성된 보고서 파일은 내구성 있는 저장소에 저장해야 합니다. 저장해야 할 파일의 수는 스타트업 회사가 빠르게 해외로 확장함에 따라 시간이 지남에 따라 증가할 수 있으므로 전 세계에 있는 고객에게 보고서를 더 빨리 배포할 방법도 필요합니다.  다음 중 이 시나리오에 사용해야 하는 비용 효율적이고 확장 가능한 스토리지 옵션은 무엇입니까?,데이터 스토리지에 여러 EC2 인스턴스 스토어를 사용하고 ElastiCache를 CDN으로 사용합니다.,Amazon Glacier를 데이터 스토리지로 사용하고 ElastiCache를 CDN으로 사용합니다.,Amazon S3를 데이터 스토리지로 사용하고 CloudFront를 CDN으로 사용합니다.,Amazon Redshift를 데이터 스토리지로 사용하고 CloudFront를 CDN으로 사용합니다.,,,0,,
udemy,CLF-01,93,"A media company wants to ensure that the images it delivers through Amazon CloudFront are compatible across various user devices. The company plans to serve images in WebP format to user agents that support it and return to JPEG format for those that don't. Additionally, they want to add a custom header to the response for tracking purposes.As a solution architect, what approach would you recommend to meet these requirements while minimizing operational overhead?",B,B,Implement an image conversion service on EC2 instances and integrate it with CloudFront. Use Lambda functions to modify the response headers and serve the appropriate format based on the User-Agent header.,Configure CloudFront behaviors to handle different image formats based on the User-Agent header. Use Lambda@Edge functions to modify the response headers and serve the appropriate format.,Generate a CloudFront response headers policy. Utilize the policy to deliver the suitable image format according to the User-Agent HTTP header in the incoming request.,"Create multiple CloudFront distributions, each serving a specific image format (WebP or JPEG). Route incoming requests based on the User-Agent header to the respective distribution using Amazon Route 53.",,,,미디어 회사는 Amazon CloudFront를 통해 제공하는 이미지가 다양한 사용자 장치에서 호환되는지 확인하려고 합니다. 회사는 WebP 형식의 이미지를 지원하는 사용자 에이전트에 제공하고 지원하지 않는 사용자 에이전트에 대해서는 JPEG 형식으로 돌아갈 계획입니다. 또한 추적 목적으로 응답에 사용자 지정 헤더를 추가하려고 합니다.솔루션 설계자로서 운영 오버헤드를 최소화하면서 이러한 요구 사항을 충족하기 위해 어떤 접근 방식을 권장하시겠습니까?,EC2 인스턴스에서 이미지 변환 서비스를 구현하고 CloudFront와 통합합니다. Lambda 함수를 사용하여 응답 헤더를 수정하고 User-Agent 헤더를 기반으로 적절한 형식을 제공합니다.,User-Agent 헤더를 기반으로 다양한 이미지 형식을 처리하도록 CloudFront 동작을 구성합니다. Lambda@Edge 함수를 사용하여 응답 헤더를 수정하고 적절한 형식을 제공합니다.,CloudFront 응답 헤더 정책을 생성합니다. 들어오는 요청의 User-Agent HTTP 헤더에 따라 적합한 이미지 형식을 전달하는 정책을 활용합니다.,각각 특정 이미지 형식(WebP 또는 JPEG)을 제공하는 여러 CloudFront 배포를 만듭니다. User-Agent 헤더를 기반으로 수신 요청을 Amazon Route 53을 사용하여 각 배포로 라우팅합니다.,,,0,,
udemy,CLF-01,94,A software company has resources hosted in AWS and on-premises servers. You have been requested to create a decoupled architecture for applications which make use of both resources. Which of the following options are valid? (Select TWO.),BD,BD,Use RDS to utilize both on-premises servers and EC2 instances for your decoupled application,Use SQS to utilize both on-premises servers and EC2 instances for your decoupled application,Use DynamoDB to utilize both on-premises servers and EC2 instances for your decoupled application,Use SWF to utilize both on-premises servers and EC2 instances for your decoupled application,Use VPC peering to connect both on-premises servers and EC2 instances for your decoupled application,,,소프트웨어 회사는 AWS 및 온프레미스 서버에서 호스팅되는 리소스를 보유하고 있습니다. 두 리소스를 모두 사용하는 애플리케이션을 위한 분리된 아키텍처를 생성하라는 요청을 받았습니다. 다음 옵션 중 유효한 것은 무엇입니까? (2개를 선택하세요.),분리된 애플리케이션을 위해 RDS를 사용하여 온프레미스 서버와 EC2 인스턴스를 모두 활용,분리된 애플리케이션에 대해 SQS를 사용하여 온프레미스 서버와 EC2 인스턴스를 모두 활용,DynamoDB를 사용하여 분리된 애플리케이션에 온프레미스 서버와 EC2 인스턴스를 모두 활용,분리된 애플리케이션에 대해 SWF를 사용하여 온프레미스 서버와 EC2 인스턴스를 모두 활용,분리된 애플리케이션을 위해 VPC 피어링을 사용하여 온프레미스 서버와 EC2 인스턴스를 모두 연결합니다.,,0,,
udemy,CLF-01,95,"A solutions architect is instructed to host a website that consists of HTML, CSS, and some Javascript files. The web pages will display several high-resolution images. The website should have optimal loading times and be able to respond to high request rates.Which of the following architectures can provide the most cost-effective and fastest loading experience?",A,A,"Upload the HTML, CSS, Javascript, and the images in a single bucket. Then enable website hosting. Create a CloudFront distribution and point the domain on the S3 website endpoint.",Host the website in an AWS Elastic Beanstalk environment. Upload the images in an S3 bucket. Use CloudFront as a CDN to deliver the images closer to your end-users.,Host the website using an Nginx server in an EC2 instance. Upload the images in an S3 bucket. Use CloudFront as a CDN to deliver the images closer to end-users.,"Launch an Auto Scaling Group using an AMI that has a pre-configured Apache web server, then configure the scaling policy accordingly. Store the images in an Elastic Block Store. Then, point your instance’s endpoint to AWS Global Accelerator.",,,,"솔루션 설계자는 HTML, CSS 및 일부 Javascript 파일로 구성된 웹 사이트를 호스팅하도록 지시받습니다. 웹 페이지에는 여러 고해상도 이미지가 표시됩니다. 웹사이트는 최적의 로딩 시간을 갖고 높은 요청률에 대응할 수 있어야 합니다.다음 중 가장 비용 효율적이고 빠른 로드 경험을 제공할 수 있는 아키텍처는 무엇입니까?","단일 버킷에 HTML, CSS, Javascript 및 이미지를 업로드합니다. 그런 다음 웹사이트 호스팅을 활성화합니다. CloudFront 배포를 생성하고 S3 웹사이트 엔드포인트에서 도메인을 가리킵니다.",AWS Elastic Beanstalk 환경에서 웹 사이트를 호스팅합니다. S3 버킷에 이미지를 업로드합니다. CloudFront를 CDN으로 사용하여 이미지를 최종 사용자에게 더 가깝게 전달합니다.,EC2 인스턴스에서 Nginx 서버를 사용하여 웹사이트를 호스팅합니다. S3 버킷에 이미지를 업로드합니다. CloudFront를 CDN으로 사용하여 이미지를 최종 사용자에게 더 가깝게 전달합니다.,미리 구성된 Apache 웹 서버가 있는 AMI를 사용하여 Auto Scaling Group을 시작한 다음 그에 따라 조정 정책을 구성합니다. Elastic Block Store에 이미지를 저장합니다. 그런 다음 인스턴스의 엔드포인트를 AWS Global Accelerator로 지정합니다.,,,0,,
udemy,CLF-01,96,"An advertising company is currently working on a proof of concept project that automatically provides SEO analytics for its clients. Your company has a VPC in AWS that operates in a dual-stack mode in which IPv4 and IPv6 communication is allowed. You deployed the application to an Auto Scaling group of EC2 instances with an Application Load Balancer in front that evenly distributes the incoming traffic. You are ready to go live but you need to point your domain name (tutorialsdojo.com) to the Application Load Balancer.In Route 53, which record types will you use to point the DNS name of the Application Load Balancer? (Select TWO.)",DE,DE,"Alias with a type ""CNAME"" record set","Non-Alias with a type ""A"" record set",Alias with a type of “MX” record set,"Alias with a type ""AAAA"" record set","Alias with a type ""A"" record set",,,광고 회사는 현재 고객에게 SEO 분석을 자동으로 제공하는 개념 증명 프로젝트를 진행하고 있습니다. 회사에는 IPv4 및 IPv6 통신이 허용되는 이중 스택 모드에서 작동하는 AWS의 VPC가 있습니다. 들어오는 트래픽을 균등하게 분산하는 Application Load Balancer가 앞에 있는 EC2 인스턴스의 Auto Scaling 그룹에 애플리케이션을 배포했습니다. 라이브로 시작할 준비가 되었지만 도메인 이름(tutorialsdojo.com)이 Application Load Balancer를 가리키도록 해야 합니다.Route 53에서 Application Load Balancer의 DNS 이름을 가리키는 데 사용할 레코드 유형은 무엇입니까? (2개를 선택하세요.),"유형이 ""CNAME"" 레코드 집합인 별칭","유형 ""A"" 레코드 집합이 있는 비별칭","""MX"" 레코드 집합 유형의 별칭","유형이 ""AAAA"" 레코드 집합인 별칭","유형 ""A"" 레코드 집합이 있는 별칭",,0,,
udemy,CLF-01,97,A company has a static corporate website hosted in a standard S3 bucket and a new web domain name that was registered using Route 53. You are instructed by your manager to integrate these two services in order to successfully launch their corporate website.What are the prerequisites when routing traffic using Amazon Route 53 to a website that is hosted in an Amazon S3 Bucket? (Select TWO.),AC,AC,The S3 bucket name must be the same as the domain name,The S3 bucket must be in the same region as the hosted zone,A registered domain name,"The record set must be of type ""MX""",The Cross-Origin Resource Sharing (CORS) option should be enabled in the S3 bucket,,,회사에는 표준 S3 버킷에서 호스팅되는 정적 회사 웹 사이트와 Route 53을 사용하여 등록된 새 웹 도메인 이름이 있습니다. 관리자는 회사 웹 사이트를 성공적으로 시작하기 위해 이 두 서비스를 통합하라는 지시를 받았습니다.Amazon Route 53을 사용하여 Amazon S3 버킷에서 호스팅되는 웹 사이트로 트래픽을 라우팅할 때 전제 조건은 무엇입니까? (2개를 선택하세요.),S3 버킷 이름은 도메인 이름과 동일해야 합니다.,S3 버킷은 호스팅 영역과 동일한 리전에 있어야 합니다.,등록된 도메인 이름,"레코드 세트는 ""MX"" 유형이어야 합니다.",Cross-Origin Resource Sharing(CORS) 옵션은 S3 버킷에서 활성화되어야 합니다.,,0,,
udemy,CLF-01,98,A company needs to assess and audit all the configurations in their AWS account. It must enforce strict compliance by tracking all configuration changes made to any of its Amazon S3 buckets. Publicly accessible S3 buckets should also be identified automatically to avoid data breaches.Which of the following options will meet this requirement?,B,B,Use AWS IAM to generate a credential report.,Use AWS Config to set up a rule in your AWS account.,Use AWS CloudTrail and review the event history of your AWS account.,Use AWS Trusted Advisor to analyze your AWS environment.,,,,회사는 AWS 계정의 모든 구성을 평가하고 감사해야 합니다. Amazon S3 버킷에 대한 모든 구성 변경 사항을 추적하여 엄격한 규정 준수를 시행해야 합니다. 공개적으로 액세스할 수 있는 S3 버킷도 데이터 위반을 방지하기 위해 자동으로 식별되어야 합니다.다음 중 이 요구 사항을 충족하는 옵션은 무엇입니까?,AWS IAM을 사용하여 자격 증명 보고서를 생성합니다.,AWS Config를 사용하여 AWS 계정에서 규칙을 설정합니다.,AWS CloudTrail을 사용하고 AWS 계정의 이벤트 기록을 검토하십시오.,AWS Trusted Advisor를 사용하여 AWS 환경을 분석하십시오.,,,0,,
udemy,CLF-01,99,An Intelligence Agency developed a missile tracking application that is hosted on both development and production AWS accounts. The Intelligence agency’s junior developer only has access to the development account. She has received security clearance to access the agency’s production account but the access is only temporary and only write access to EC2 and S3 is allowed.Which of the following allows you to issue short-lived access tokens that act as temporary security credentials to allow access to your AWS resources?,A,A,Use AWS STS,All of the given options are correct.,Use AWS Cognito to issue JSON Web Tokens (JWT),Use AWS SSO,,,,정보국은 개발 및 프로덕션 AWS 계정 모두에서 호스팅되는 미사일 추적 애플리케이션을 개발했습니다. 정보국의 주니어 개발자는 개발 계정에만 액세스할 수 있습니다. 그녀는 에이전시의 프로덕션 계정에 액세스할 수 있는 보안 승인을 받았지만 액세스는 일시적이며 EC2 및 S3에 대한 쓰기 액세스만 허용됩니다.다음 중 AWS 리소스에 대한 액세스를 허용하는 임시 보안 자격 증명 역할을 하는 단기 액세스 토큰을 발급할 수 있는 것은 무엇입니까?,AWS STS 사용,주어진 모든 옵션이 정확합니다.,AWS Cognito를 사용하여 JSON 웹 토큰(JWT) 발행,AWS SSO 사용,,,0,,
udemy,CLF-01,100,A Solutions Architect is working for an online hotel booking firm with terabytes of customer data coming from the websites and applications. There is an annual corporate meeting where the Architect needs to present the booking behavior and acquire new insights from the customers’ data. The Architect is looking for a service to perform super-fast analytics on massive data sets in near real-time.Which of the following services gives the Architect the ability to store huge amounts of data and perform quick and flexible queries on it?,D,D,Amazon RDS,Amazon DynamoDB,Amazon ElastiCache,Amazon Redshift,,,,Solutions Architect는 웹 사이트 및 애플리케이션에서 들어오는 테라바이트 규모의 고객 데이터를 사용하는 온라인 호텔 예약 회사에서 일하고 있습니다. Architect가 예약 행동을 제시하고 고객 데이터에서 새로운 통찰력을 얻어야 하는 연례 기업 회의가 있습니다. Architect는 방대한 데이터 세트에 대해 거의 실시간으로 초고속 분석을 수행할 수 있는 서비스를 찾고 있습니다.다음 중 Architect가 막대한 양의 데이터를 저장하고 빠르고 유연한 쿼리를 수행할 수 있는 기능을 제공하는 서비스는 무엇입니까?,아마존 RDS,아마존 다이나모DB,아마존 엘라스티캐시,아마존 레드시프트,,,0,,
udemy,CLF-01,101,"A Solutions Architect created a new Standard-class S3 bucket to store financial reports that are not frequently accessed but should immediately be available when an auditor requests them. To save costs, the Architect changed the storage class of the S3 bucket from Standard to Infrequent Access storage class.In Amazon S3 Standard - Infrequent Access storage class, which of the following statements are true? (Select TWO.)",AE,AE,It is designed for data that is accessed less frequently.,It automatically moves data to the most cost-effective access tier without any operational overhead.,Ideal to use for data archiving.,It provides high latency and low throughput performance,It is designed for data that requires rapid access when needed.,,,Solutions Architect는 자주 액세스하지 않지만 감사자가 요청할 때 즉시 사용할 수 있어야 하는 재무 보고서를 저장하기 위해 새로운 표준 등급 S3 버킷을 만들었습니다. 비용을 절감하기 위해 Architect는 S3 버킷의 스토리지 클래스를 Standard에서 Infrequent Access 스토리지 클래스로 변경했습니다.Amazon S3 Standard - Infrequent Access 스토리지 클래스에서 다음 중 참인 설명은 무엇입니까? (2개를 선택하세요.),자주 액세스하지 않는 데이터용으로 설계되었습니다.,운영 오버헤드 없이 가장 비용 효율적인 액세스 계층으로 데이터를 자동으로 이동합니다.,데이터 보관에 사용하기에 이상적입니다.,높은 대기 시간과 낮은 처리량 성능을 제공합니다.,필요할 때 빠른 액세스가 필요한 데이터를 위해 설계되었습니다.,,0,,
udemy,CLF-01,102,"An organization stores and manages financial records of various companies in its on-premises data center, which is almost out of space. The management decided to move all of their existing records to a cloud storage service. All future financial records will also be stored in the cloud. For additional security, all records must be prevented from being deleted or overwritten.Which of the following should you do to meet the above requirement?",B,B,Use AWS Storage Gateway to establish hybrid cloud storage. Store all of your data in Amazon S3 and enable object lock.,Use AWS DataSync to move the data. Store all of your data in Amazon S3 and enable object lock.,Use AWS DataSync to move the data. Store all of your data in Amazon EFS and enable object lock.,Use AWS Storage Gateway to establish hybrid cloud storage. Store all of your data in Amazon EBS and enable object lock.,,,,조직은 공간이 거의 없는 온프레미스 데이터 센터에서 다양한 회사의 재무 기록을 저장하고 관리합니다. 경영진은 모든 기존 기록을 클라우드 스토리지 서비스로 옮기기로 결정했습니다. 향후 모든 재무 기록도 클라우드에 저장됩니다. 추가 보안을 위해 모든 기록이 삭제되거나 덮어쓰여지지 않도록 해야 합니다.위의 요구 사항을 충족하려면 다음 중 무엇을 해야 합니까?,AWS Storage Gateway를 사용하여 하이브리드 클라우드 스토리지를 설정합니다. 모든 데이터를 Amazon S3에 저장하고 객체 잠금을 활성화하십시오.,AWS DataSync를 사용하여 데이터를 이동합니다. 모든 데이터를 Amazon S3에 저장하고 객체 잠금을 활성화하십시오.,AWS DataSync를 사용하여 데이터를 이동합니다. 모든 데이터를 Amazon EFS에 저장하고 객체 잠금을 활성화합니다.,AWS Storage Gateway를 사용하여 하이브리드 클라우드 스토리지를 설정합니다. 모든 데이터를 Amazon EBS에 저장하고 객체 잠금을 활성화하십시오.,,,0,,
udemy,CLF-01,103,"A solutions architect is designing a cost-efficient, highly available storage solution for company data. One of the requirements is to ensure that the previous state of a file is preserved and retrievable if a modified version of it is uploaded. Also, to meet regulatory compliance, data over 3 years must be retained in an archive and will only be accessible once a year.How should the solutions architect build the solution?",A,A,Create an S3 Standard bucket with object-level versioning enabled and configure a lifecycle rule that transfers files to Amazon S3 Glacier Deep Archive after 3 years.,Create a One-Zone-IA bucket with object-level versioning enabled and configure a lifecycle rule that transfers files to Amazon S3 Glacier Deep Archive after 3 years.,Create an S3 Standard bucket with S3 Object Lock in compliance mode enabled then configure a lifecycle rule that transfers files to Amazon S3 Glacier Deep Archive after 3 years.,Create an S3 Standard bucket and enable S3 Object Lock in governance mode.,,,,솔루션 설계자는 회사 데이터를 위한 비용 효율적이고 가용성이 높은 스토리지 솔루션을 설계하고 있습니다. 요구 사항 중 하나는 파일의 수정된 버전이 업로드되는 경우 파일의 이전 상태가 보존되고 검색 가능하도록 하는 것입니다. 또한 규정 준수를 충족하려면 3년 이상의 데이터를 아카이브에 보관해야 하며 1년에 한 번만 액세스할 수 있습니다.솔루션 설계자는 솔루션을 어떻게 구축해야 합니까?,객체 수준 버전 관리가 활성화된 S3 Standard 버킷을 생성하고 3년 후 파일을 Amazon S3 Glacier Deep Archive로 전송하는 수명 주기 규칙을 구성합니다.,객체 수준 버전 관리가 활성화된 One-Zone-IA 버킷을 생성하고 3년 후 파일을 Amazon S3 Glacier Deep Archive로 전송하는 수명 주기 규칙을 구성합니다.,규정 준수 모드에서 S3 객체 잠금이 활성화된 S3 Standard 버킷을 생성한 다음 3년 후 파일을 Amazon S3 Glacier Deep Archive로 전송하는 수명 주기 규칙을 구성합니다.,S3 Standard 버킷을 생성하고 거버넌스 모드에서 S3 객체 잠금을 활성화합니다.,,,0,,
udemy,CLF-01,104,A company runs a messaging application in the ap-northeast-1 and ap-southeast-2 region. A Solutions Architect needs to create a routing policy wherein a larger portion of traffic from the Philippines and North India will be routed to the resource in the ap-northeast-1 region.Which Route 53 routing policy should the Solutions Architect use?,B,B,Weighted Routing,Geoproximity Routing,Latency Routing,Geolocation Routing,,,,회사는 ap-northeast-1및 ap-southeast-2지역에서 메시징 응용 프로그램을 실행합니다. Solutions Architect는 필리핀과 인도 북부에서 오는 트래픽의 대부분이 해당 ap-northeast-1지역의 리소스로 라우팅되는 라우팅 정책을 만들어야 합니다.Solutions Architect는 어떤 Route 53 라우팅 정책을 사용해야 합니까?,가중 라우팅,지리 근접 라우팅,대기 시간 라우팅,지리적 위치 라우팅,,,0,,
udemy,CLF-01,105,A company launched a website that accepts high-quality photos and turns them into a downloadable video montage. The website offers a free and a premium account that guarantees faster processing. All requests by both free and premium members go through a single SQS queue and then processed by a group of EC2 instances that generate the videos. The company needs to ensure that the premium users who paid for the service have higher priority than the free members.How should the company re-design its architecture to address this requirement?,B,B,Use Amazon Kinesis to process the photos and generate the video montage in real-time.,"Create an SQS queue for free members and another one for premium members. Configure your EC2 instances to consume messages from the premium queue first and if it is empty, poll from the free members' SQS queue.","For the requests made by premium members, set a higher priority in the SQS queue so it will be processed first compared to the requests made by free members.",Use Amazon S3 to store and process the photos and then generate the video montage afterward.,,,,한 회사에서 고품질 사진을 받아 다운로드 가능한 비디오 몽타주로 변환하는 웹사이트를 개설했습니다. 웹사이트는 더 빠른 처리를 보장하는 무료 및 프리미엄 계정을 제공합니다. 무료 및 프리미엄 회원의 모든 요청은 단일 SQS 대기열을 통과한 다음 비디오를 생성하는 EC2 인스턴스 그룹에서 처리됩니다. 회사는 유료 회원이 무료 회원보다 서비스 이용료를 지불한 프리미엄 회원이 우선순위를 갖도록 해야 합니다.회사는 이 요구 사항을 해결하기 위해 아키텍처를 어떻게 재설계해야 합니까?,Amazon Kinesis를 사용하여 실시간으로 사진을 처리하고 비디오 몽타주를 생성합니다.,무료 회원용 SQS 대기열과 프리미엄 회원용 SQS 대기열을 만듭니다. 먼저 프리미엄 대기열의 메시지를 사용하고 비어 있는 경우 무료 멤버의 SQS 대기열에서 폴링하도록 EC2 인스턴스를 구성합니다.,프리미엄 회원이 요청한 경우 SQS 대기열에서 더 높은 우선순위를 설정하여 무료 회원이 요청한 것보다 먼저 처리되도록 합니다.,Amazon S3를 사용하여 사진을 저장 및 처리한 다음 나중에 비디오 몽타주를 생성합니다.,,,0,,
udemy,CLF-01,106,"An insurance company utilizes SAP HANA for its day-to-day ERP operations. Since they can’t migrate this database due to customer preferences, they need to integrate it with the current AWS workload in the VPC in which they are required to establish a site-to-site VPN connection.What needs to be configured outside of the VPC for them to have a successful site-to-site VPN connection?",C,C,The main route table in your VPC to route traffic through a NAT instance,A dedicated NAT instance in a public subnet,An Internet-routable IP address (static) of the customer gateway's external interface for the on-premises network,An EIP to the Virtual Private Gateway,,,,보험 회사는 일상적인 ERP 운영에 SAP HANA를 활용합니다. 고객 선호도 때문에 이 데이터베이스를 마이그레이션할 수 없기 때문에 사이트 간 VPN 연결을 설정해야 하는 VPC의 현재 AWS 워크로드와 통합해야 합니다.성공적인 사이트 간 VPN 연결을 위해 VPC 외부에서 구성해야 하는 것은 무엇입니까?,NAT 인스턴스를 통해 트래픽을 라우팅하기 위한 VPC의 기본 라우팅 테이블,퍼블릭 서브넷의 전용 NAT 인스턴스,온프레미스 네트워크에 대한 고객 게이트웨이 외부 인터페이스의 인터넷 라우팅 가능 IP 주소(정적),가상 프라이빗 게이트웨이에 대한 EIP,,,0,,
udemy,CLF-01,107,A company wants to streamline the process of creating multiple AWS accounts within an AWS Organization. Each organization unit (OU) must be able to launch new accounts with preapproved configurations from the security team which will standardize the baselines and network configurations for all accounts in the organization.Which solution entails the least amount of effort to implement?,D,D,Configure AWS Resource Access Manager (AWS RAM) to launch new AWS accounts as well as standardize the baselines and network configurations for each organization unit,"Set up an AWS Config aggregator on the root account of the organization to enable multi-account, multi-region data aggregation. Deploy conformance packs to standardize the baselines and network configurations for all accounts.",Centralized the creation of AWS accounts using AWS Systems Manager OpsCenter. Enforce policies and detect violations to all AWS accounts using AWS Security Hub.,Set up an AWS Control Tower Landing Zone. Enable pre-packaged guardrails to enforce policies or detect violations.,,,,한 회사에서 AWS Organization 내에서 여러 AWS 계정을 생성하는 프로세스를 간소화하려고 합니다. 각 조직 단위(OU)는 조직의 모든 계정에 대한 기준선 및 네트워크 구성을 표준화할 보안 팀의 사전 승인된 구성으로 새 계정을 시작할 수 있어야 합니다.구현하는 데 가장 적은 노력이 필요한 솔루션은 무엇입니까?,AWS Resource Access Manager(AWS RAM)를 구성하여 새 AWS 계정을 시작하고 각 조직 단위의 기준선 및 네트워크 구성을 표준화합니다.,"다중 계정, 다중 리전 데이터 집계를 활성화하려면 조직의 루트 계정에 AWS Config 집계자를 설정합니다. 적합성 팩을 배포하여 모든 계정에 대한 기준선 및 네트워크 구성을 표준화합니다.",AWS Systems Manager OpsCenter를 사용하여 AWS 계정 생성을 중앙 집중화했습니다. AWS Security Hub를 사용하여 모든 AWS 계정에 대한 정책을 시행하고 위반을 감지합니다.,AWS Control Tower Landing Zone을 설정합니다. 사전 패키징된 가드레일을 활성화하여 정책을 시행하거나 위반을 감지합니다.,,,0,,
udemy,CLF-01,108,"An application is hosted in AWS Fargate and uses RDS database in Multi-AZ Deployments configuration with several Read Replicas. A Solutions Architect was instructed to ensure that all of their database credentials, API keys, and other secrets are encrypted and rotated on a regular basis to improve data security. The application should also use the latest version of the encrypted credentials when connecting to the RDS database. Which of the following is the MOST appropriate solution to secure the credentials?",D,D,"Store the database credentials, API keys, and other secrets to AWS ACM.","Store the database credentials, API keys, and other secrets in AWS KMS.","Store the database credentials, API keys, and other secrets to Systems Manager Parameter Store each with a SecureString data type. The credentials are automatically rotated by default.","Use AWS Secrets Manager to store and encrypt the database credentials, API keys, and other secrets. Enable automatic rotation for all of the credentials.",,,,"애플리케이션은 AWS Fargate에서 호스팅되며 여러 읽기 전용 복제본이 있는 다중 AZ 배포 구성에서 RDS 데이터베이스를 사용합니다. Solutions Architect는 데이터 보안을 개선하기 위해 모든 데이터베이스 자격 증명, API 키 및 기타 암호를 정기적으로 암호화하고 교체하도록 지시받았습니다. 애플리케이션은 또한 RDS 데이터베이스에 연결할 때 최신 버전의 암호화된 자격 증명을 사용해야 합니다.자격 증명을 보호하는 데 가장 적합한 솔루션은 다음 중 무엇입니까?","데이터베이스 자격 증명, API 키 및 기타 암호를 AWS ACM에 저장합니다.","데이터베이스 자격 증명, API 키 및 기타 암호를 AWS KMS에 저장합니다.","데이터베이스 자격 증명, API 키 및 기타 암호를 각각 SecureString데이터 유형과 함께 Systems Manager Parameter Store에 저장합니다. 자격 증명은 기본적으로 자동으로 교체됩니다.","AWS Secrets Manager를 사용하여 데이터베이스 자격 증명, API 키 및 기타 암호를 저장하고 암호화합니다. 모든 자격 증명에 대해 자동 교체를 활성화합니다.",,,0,,
udemy,CLF-01,109,"A company owns a photo-sharing app that stores user uploads on Amazon S3. There has been an increase in the number of explicit and offensive images being reported. The company currently relies on human efforts to moderate content, and they want to streamline this process by using Artificial Intelligence to only flag images for review. For added security, any communication with your resources on your Amazon VPC must not traverse the public Internet.How can this task be accomplished with the LEAST amount of effort?",A,A,Use Amazon Rekognition to detect images with graphic nudity or violence in Amazon S3. Create an Interface VPC endpoint for Amazon Rekognition with the necessary policies to prevent any traffic from traversing the public Internet.,Use Amazon Detective to detect images with graphic nudity or violence in Amazon S3. Ensure that all communications made by your AWS resources do not traverse the public Internet via the AWS Audit Manager service.,Use Amazon Monitron to monitor each user upload in S3. Use the AWS Transit Gateway Network Manager to block any outbound requests to the public Internet.,Use an image classification model in Amazon SageMaker. Set up Amazon GuardDuty and connect it with Amazon SageMaker to ensure that all communications do not traverse the public Internet.,,,,회사는 Amazon S3에 사용자 업로드를 저장하는 사진 공유 앱을 소유하고 있습니다. 노골적이고 공격적인 이미지가 보고되는 횟수가 증가했습니다. 회사는 현재 콘텐츠를 조정하기 위해 인간의 노력에 의존하고 있으며 인공 지능을 사용하여 검토용 이미지에만 플래그를 지정하여 이 프로세스를 간소화하려고 합니다. 보안 강화를 위해 Amazon VPC의 리소스와의 모든 통신은 퍼블릭 인터넷을 통과하지 않아야 합니다.최소한의 노력으로 이 작업을 수행할 수 있는 방법은 무엇입니까?,Amazon Rekognition을 사용하여 Amazon S3에서 노골적인 과도한 노출 또는 폭력이 포함된 이미지를 감지합니다. 트래픽이 퍼블릭 인터넷을 통과하지 못하도록 방지하는 데 필요한 정책을 사용하여 Amazon Rekognition용 인터페이스 VPC 엔드포인트를 생성합니다.,Amazon Detective를 사용하여 Amazon S3에서 노골적인 과도한 노출 또는 폭력이 포함된 이미지를 감지합니다. AWS 리소스가 수행하는 모든 통신이 AWS Audit Manager 서비스를 통해 퍼블릭 인터넷을 통과하지 않는지 확인하십시오.,Amazon Monitron을 사용하여 S3에서 각 사용자 업로드를 모니터링합니다. AWS Transit Gateway Network Manager를 사용하여 퍼블릭 인터넷에 대한 모든 아웃바운드 요청을 차단합니다.,Amazon SageMaker에서 이미지 분류 모델을 사용합니다. Amazon GuardDuty를 설정하고 Amazon SageMaker와 연결하여 모든 통신이 공용 인터넷을 통과하지 않도록 합니다.,,,0,,
udemy,CLF-01,110,All objects uploaded to an Amazon S3 bucket must be encrypted for security compliance. The bucket will use server-side encryption with Amazon S3-Managed encryption keys (SSE-S3) to encrypt data using 256-bit Advanced Encryption Standard (AES-256) block cipher.Which of the following request headers must be used?,B,B,x-amz-server-side-encryption-customer-key,x-amz-server-side-encryption,x-amz-server-side-encryption-customer-algorithm,x-amz-server-side-encryption-customer-key-MD5,,,,Amazon S3 버킷에 업로드된 모든 객체는 보안 규정 준수를 위해 암호화되어야 합니다. 버킷은 Amazon S3 관리형 암호화 키(SSE-S3)로 서버 측 암호화를 사용하여 256비트 고급 암호화 표준(AES-256) 블록 암호를 사용하여 데이터를 암호화합니다.다음 요청 헤더 중 사용해야 하는 것은 무엇입니까?,x-amz-server-side-encryption-customer-key,x-amz-server-side-encryption,x-amz-server-side-encryption-customer-algorithm,x-amz-server-side-encryption-customer-key-MD5,,,0,,
udemy,CLF-01,111,An organization is currently using a tape backup solution to store its application data on-premises. They plan to use a cloud storage service to preserve the backup data for up to 10 years that may be accessed about once or twice a year.Which of the following is the most cost-effective option to implement this solution?,C,C,Use AWS Storage Gateway to backup the data directly to Amazon S3 Glacier.,Order an AWS Snowball Edge appliance to import the backup directly to Amazon S3 Glacier.,Use AWS Storage Gateway to backup the data directly to Amazon S3 Glacier Deep Archive.,Use Amazon S3 to store the backup data and add a lifecycle rule to transition the current version to Amazon S3 Glacier.,,,,조직은 현재 애플리케이션 데이터를 온프레미스에 저장하기 위해 테이프 백업 솔루션을 사용하고 있습니다. 그들은 클라우드 스토리지 서비스를 사용하여 1년에 한두 번 정도 액세스할 수 있는 백업 데이터를 최대 10년 동안 보존할 계획입니다.다음 중 이 솔루션을 구현하는 가장 비용 효율적인 옵션은 무엇입니까?,AWS Storage Gateway를 사용하여 데이터를 Amazon S3 Glacier에 직접 백업하십시오.,백업을 Amazon S3 Glacier로 직접 가져오려면 AWS Snowball Edge 어플라이언스를 주문하십시오.,AWS Storage Gateway를 사용하여 데이터를 Amazon S3 Glacier Deep Archive에 직접 백업하십시오.,Amazon S3를 사용하여 백업 데이터를 저장하고 수명 주기 규칙을 추가하여 현재 버전을 Amazon S3 Glacier로 전환합니다.,,,0,,
udemy,CLF-01,112,A company has a requirement to move 80 TB data warehouse to the cloud. It would take 2 months to transfer the data given their current bandwidth allocation. Which is the most cost-effective service that would allow you to quickly upload their data into AWS?,A,A,AWS Snowball Edge,AWS Snowmobile,AWS Direct Connect,Amazon S3 Multipart Upload,,,,회사에서 80TB 데이터 웨어하우스를 클라우드로 이전해야 하는 요구 사항이 있습니다. 현재 대역폭 할당으로 데이터를 전송하는 데 2개월이 걸립니다.데이터를 AWS에 빠르게 업로드할 수 있는 가장 비용 효율적인 서비스는 무엇입니까?,AWS 스노우볼 에지,AWS 스노우모빌,AWS 다이렉트 커넥트,Amazon S3 멀티파트 업로드,,,0,,
udemy,CLF-01,113,A company has two On-Demand EC2 instances inside the Virtual Private Cloud in the same Availability Zone but are deployed to different subnets. One EC2 instance is running a database and the other EC2 instance a web application that connects with the database. You need to ensure that these two instances can communicate with each other for the system to work properly.What are the things you have to check so that these EC2 instances can communicate inside the VPC? (Select TWO.),AE,AE,Check the Network ACL if it allows communication between the two subnets.,Check if both instances are the same instance class.,Check if the default route is set to a NAT instance or Internet Gateway (IGW) for them to communicate.,Ensure that the EC2 instances are in the same Placement Group.,Check if all security groups are set to allow the application host to communicate to the database on the right port and protocol.,,,회사는 동일한 가용 영역의 Virtual Private Cloud 내에 두 개의 온디맨드 EC2 인스턴스를 가지고 있지만 서로 다른 서브넷에 배포되어 있습니다. 하나의 EC2 인스턴스는 데이터베이스를 실행하고 다른 EC2 인스턴스는 데이터베이스와 연결되는 웹 애플리케이션을 실행합니다. 시스템이 제대로 작동하려면 이 두 인스턴스가 서로 통신할 수 있는지 확인해야 합니다.이러한 EC2 인스턴스가 VPC 내부에서 통신할 수 있도록 확인해야 하는 사항은 무엇입니까? (2개를 선택하세요.),두 서브넷 간의 통신을 허용하는 경우 네트워크 ACL을 확인하십시오.,두 인스턴스가 동일한 인스턴스 클래스인지 확인하십시오.,통신을 위해 기본 경로가 NAT 인스턴스 또는 IGW(Internet Gateway)로 설정되어 있는지 확인하십시오.,EC2 인스턴스가 동일한 배치 그룹에 있는지 확인하십시오.,애플리케이션 호스트가 올바른 포트 및 프로토콜에서 데이터베이스와 통신할 수 있도록 모든 보안 그룹이 설정되어 있는지 확인하십시오.,,0,,
udemy,CLF-01,114,"A commercial bank has a forex trading application. They created an Auto Scaling group of EC2 instances that allow the bank to cope with the current traffic and achieve cost-efficiency. They want the Auto Scaling group to behave in such a way that it will follow a predefined set of parameters before it scales down the number of EC2 instances, which protects the system from unintended slowdown or unavailability.Which of the following statements are true regarding the cooldown period? (Select TWO.)",CD,CD,"It ensures that before the Auto Scaling group scales out, the EC2 instances have ample time to cooldown.",Its default value is 600 seconds.,Its default value is 300 seconds.,It ensures that the Auto Scaling group does not launch or terminate additional EC2 instances before the previous scaling activity takes effect.,It ensures that the Auto Scaling group launches or terminates additional EC2 instances without any downtime.,,,상업 은행에는 외환 거래 애플리케이션이 있습니다. 그들은 은행이 현재 트래픽에 대처하고 비용 효율성을 달성할 수 있도록 EC2 인스턴스의 Auto Scaling 그룹을 만들었습니다. 그들은 Auto Scaling 그룹이 EC2 인스턴스 수를 축소하기 전에 사전 정의된 매개변수 세트를 따르는 방식으로 작동하여 의도하지 않은 속도 저하 또는 사용 불가로부터 시스템을 보호하기를 원합니다.휴지 기간에 대한 다음 설명 중 올바른 것은 무엇입니까? (2개를 선택하세요.),Auto Scaling 그룹이 확장되기 전에 EC2 인스턴스에 충분한 냉각 시간이 있는지 확인합니다.,기본값은 600초입니다.,기본값은 300초입니다.,이전 조정 활동이 적용되기 전에 Auto Scaling 그룹이 추가 EC2 인스턴스를 시작하거나 종료하지 않도록 합니다.,Auto Scaling 그룹이 중단 시간 없이 추가 EC2 인스턴스를 시작하거나 종료하도록 합니다.,,0,,
udemy,CLF-01,115,A company plans to migrate its suite of containerized applications running on-premises to a container service in AWS. The solution must be cloud-agnostic and use an open-source platform that can automatically manage containerized workloads and services. It should also use the same configuration and tools across various production environments.What should the Solution Architect do to properly migrate and satisfy the given requirement?,C,C,Migrate the application to Amazon Container Registry (ECR) with Amazon EC2 instance worker nodes.,Migrate the application to Amazon Elastic Container Service with ECS tasks that use the Amazon EC2 launch type.,Migrate the application to Amazon Elastic Kubernetes Service with EKS worker nodes.,Migrate the application to Amazon Elastic Container Service with ECS tasks that use the AWS Fargate launch type.,,,,회사는 온프레미스에서 실행되는 컨테이너화된 애플리케이션 제품군을 AWS의 컨테이너 서비스로 마이그레이션할 계획입니다. 솔루션은 클라우드에 구애받지 않고 컨테이너화된 워크로드 및 서비스를 자동으로 관리할 수 있는 오픈 소스 플랫폼을 사용해야 합니다. 또한 다양한 프로덕션 환경에서 동일한 구성과 도구를 사용해야 합니다.Solution Architect는 제대로 마이그레이션하고 주어진 요구 사항을 충족하기 위해 무엇을 해야 합니까?,Amazon EC2 인스턴스 작업자 노드를 사용하여 애플리케이션을 Amazon Container Registry(ECR)로 마이그레이션합니다.,Amazon EC2 시작 유형을 사용하는 ECS 작업을 통해 애플리케이션을 Amazon Elastic Container Service로 마이그레이션합니다.,EKS 작업자 노드를 사용하여 애플리케이션을 Amazon Elastic Kubernetes Service로 마이그레이션합니다.,AWS Fargate 시작 유형을 사용하는 ECS 작업을 통해 애플리케이션을 Amazon Elastic Container Service로 마이그레이션합니다.,,,0,,
udemy,CLF-01,116,A company has an application that continually sends encrypted documents to Amazon S3. The company requires that the configuration for data access is in line with their strict compliance standards. They should also be alerted if there is any risk of unauthorized access or suspicious access patterns.Which step is needed to meet the requirements?,A,A,Use Amazon GuardDuty to monitor malicious activity on S3.,Use Amazon Macie to monitor and detect access patterns on S3.,Use Amazon Rekognition to monitor and recognize patterns on S3.,Use Amazon Inspector to alert whenever a security violation is detected on S3.,,,,회사에는 암호화된 문서를 Amazon S3로 지속적으로 전송하는 애플리케이션이 있습니다. 회사는 데이터 액세스에 대한 구성이 엄격한 규정 준수 표준에 부합할 것을 요구합니다. 또한 무단 액세스 또는 의심스러운 액세스 패턴의 위험이 있는 경우 경고를 받아야 합니다.요구 사항을 충족하려면 어떤 단계가 필요합니까?,Amazon GuardDuty를 사용하여 S3에서 악의적인 활동을 모니터링합니다.,Amazon Macie를 사용하여 S3에서 액세스 패턴을 모니터링하고 감지합니다.,Amazon Rekognition을 사용하여 S3에서 패턴을 모니터링하고 인식합니다.,Amazon Inspector를 사용하여 S3에서 보안 위반이 감지될 때마다 경고하십시오.,,,0,,
udemy,CLF-01,117,"A start-up company has an EC2 instance that is hosting a web application. The volume of users is expected to grow in the coming months, and hence, you need to add more elasticity and scalability in your AWS architecture to cope with the demand.Which of the following options can satisfy the above requirement for the given scenario? (Select TWO.)",AD,AD,Set up two EC2 instances and then put them behind an Elastic Load balancer (ELB).,Set up an S3 Cache in front of the EC2 instance.,Set up an AWS WAF behind your EC2 Instance.,Set up two EC2 instances and use Route 53 to route traffic based on a Weighted Routing Policy.,Set up two EC2 instances deployed using Launch Templates and integrated with AWS Glue.,,,신생 회사에 웹 애플리케이션을 호스팅하는 EC2 인스턴스가 있습니다. 앞으로 몇 달 동안 사용자 수가 증가할 것으로 예상되므로 수요에 대처하기 위해 AWS 아키텍처에 더 많은 탄력성과 확장성을 추가해야 합니다.다음 중 주어진 시나리오에 대한 위의 요구 사항을 충족할 수 있는 옵션은 무엇입니까? (2개를 선택하세요.),2개의 EC2 인스턴스를 설정한 다음 Elastic Load Balancer(ELB) 뒤에 배치합니다.,EC2 인스턴스 앞에 S3 Cache를 설정합니다.,EC2 인스턴스 뒤에 AWS WAF를 설정합니다.,2개의 EC2 인스턴스를 설정하고 Route 53을 사용하여 가중 라우팅 정책에 따라 트래픽을 라우팅합니다.,시작 템플릿을 사용하여 배포되고 AWS Glue와 통합된 두 개의 EC2 인스턴스를 설정합니다.,,0,,
udemy,CLF-01,118,A company has multiple VPCs with IPv6 enabled for its suite of web applications. The Solutions Architect tried to deploy a new Amazon EC2 instance but she received an error saying that there is no IP address available on the subnet.How should the Solutions Architect resolve this problem?,A,A,Set up a new IPv4 subnet with a larger CIDR range. Associate the new subnet with the VPC and then launch the instance.,Set up a new IPv6-only subnet with a large CIDR range. Associate the new subnet with the VPC then launch the instance.,Ensure that the VPC has IPv6 CIDRs only. Remove any IPv4 CIDRs associated with the VPC.,Disable the IPv4 support in the VPC and use the available IPv6 addresses.,,,,회사에는 웹 애플리케이션 제품군에 대해 IPv6가 활성화된 여러 VPC가 있습니다. Solutions Architect가 새 Amazon EC2 인스턴스를 배포하려고 했지만 서브넷에서 사용할 수 있는 IP 주소가 없다는 오류를 받았습니다.Solutions Architect는 이 문제를 어떻게 해결해야 합니까?,더 큰 CIDR 범위로 새 IPv4 서브넷을 설정합니다. 새 서브넷을 VPC와 연결한 다음 인스턴스를 시작합니다.,CIDR 범위가 큰 새 IPv6 전용 서브넷을 설정합니다. 새 서브넷을 VPC와 연결한 다음 인스턴스를 시작합니다.,VPC에 IPv6 CIDR만 있는지 확인합니다. VPC와 연결된 모든 IPv4 CIDR을 제거합니다.,VPC에서 IPv4 지원을 비활성화하고 사용 가능한 IPv6 주소를 사용합니다.,,,0,,
udemy,CLF-01,119,An online events registration system is hosted in AWS and uses ECS to host its front-end tier and an RDS configured with Multi-AZ for its database tier. What are the events that will make Amazon RDS automatically perform a failover to the standby replica? (Select TWO.),CD,CD,Compute unit failure on secondary DB instance,Storage failure on secondary DB instance,Storage failure on primary,Loss of availability in primary Availability Zone,In the event of Read Replica failure,,,온라인 이벤트 등록 시스템은 AWS에서 호스팅되며 ECS를 사용하여 프런트 엔드 계층과 데이터베이스 계층에 대한 다중 AZ로 구성된 RDS를 호스팅합니다. Amazon RDS가 자동으로 대기 복제본으로 장애 조치를 수행하게 하는 이벤트는 무엇입니까? (2개를 선택하세요.),보조 DB 인스턴스의 컴퓨팅 유닛 오류,보조 DB 인스턴스의 스토리지 장애,기본 스토리지 장애,기본 가용 영역의 가용성 손실,읽기 전용 복제본 실패 시,,0,,
udemy,CLF-01,120,"A DevOps Engineer is required to design a cloud architecture in AWS. The Engineer is planning to develop a highly available and fault-tolerant architecture consisting of an Elastic Load Balancer and an Auto Scaling group of EC2 instances deployed across multiple Availability Zones. This will be used by an online accounting application that requires path-based routing, host-based routing, and bi-directional streaming using Remote Procedure Call (gRPC).Which configuration will satisfy the given requirement?",C,C,Configure a Network Load Balancer in front of the auto-scaling group. Create an AWS Global Accelerator accelerator and set the load balancer as an endpoint.,Configure a Gateway Load Balancer in front of the auto-scaling group. Ensure that the IP Listener Routing uses the GENEVE protocol on port 6081 to allow gRPC response traffic.,Configure an Application Load Balancer in front of the auto-scaling group. Select gRPC as the protocol version.,Configure a Network Load Balancer in front of the auto-scaling group. Use a UDP listener for routing.,,,,"AWS에서 클라우드 아키텍처를 설계하려면 DevOps 엔지니어가 필요합니다. 엔지니어는 여러 가용 영역에 배포된 EC2 인스턴스의 Auto Scaling 그룹과 Elastic Load Balancer로 구성된 고가용성 및 내결함성 아키텍처를 개발할 계획입니다. 이것은 원격 프로시저 호출(gRPC)을 사용하는 경로 기반 라우팅, 호스트 기반 라우팅 및 양방향 스트리밍이 필요한 온라인 회계 애플리케이션에서 사용됩니다.주어진 요구 사항을 충족하는 구성은 무엇입니까?",Auto-Scaling 그룹 앞에 Network Load Balancer를 구성합니다. AWS Global Accelerator 액셀러레이터를 생성하고 로드 밸런서를 엔드포인트로 설정합니다.,Auto Scaling 그룹 앞에 Gateway Load Balancer를 구성합니다. IP 수신기 라우팅이 포트 6081에서 GENEVE 프로토콜을 사용하여 gRPC 응답 트래픽을 허용하는지 확인합니다.,Auto-Scaling 그룹 앞에 Application Load Balancer를 구성합니다. 프로토콜 버전으로 gRPC를 선택합니다.,Auto-Scaling 그룹 앞에 Network Load Balancer를 구성합니다. 라우팅에 UDP 수신기를 사용합니다.,,,0,,
udemy,CLF-01,121,"A company has a top priority requirement to monitor a few database metrics and then afterward, send email notifications to the Operations team in case there is an issue. Which AWS services can accomplish this requirement? (Select TWO.)",AC,AC,Amazon CloudWatch,Amazon EC2 Instance with a running Berkeley Internet Name Domain (BIND) Server.,Amazon Simple Notification Service (SNS),Amazon Simple Queue Service (SQS),Amazon Simple Email Service,,,회사는 몇 가지 데이터베이스 메트릭을 모니터링한 다음 문제가 있는 경우 운영 팀에 이메일 알림을 보내야 하는 최우선 요구 사항이 있습니다. 이 요구 사항을 충족할 수 있는 AWS 서비스는 무엇입니까? (2개를 선택하세요.),아마존 클라우드워치,실행 중인 BIND(Berkeley Internet Name Domain) 서버가 있는 Amazon EC2 인스턴스.,Amazon 단순 알림 서비스(SNS),Amazon 단순 대기열 서비스(SQS),아마존 심플 이메일 서비스,,0,,
udemy,CLF-01,122,A digital media company shares static content to its premium users around the world and also to their partners who syndicate their media files. The company is looking for ways to reduce its server costs and securely deliver their data to their customers globally with low latency. Which combination of services should be used to provide the MOST suitable and cost-effective architecture? (Select TWO.),CE,CE,AWS Fargate,AWS Global Accelerator,Amazon S3,AWS Lambda,Amazon CloudFront,,,디지털 미디어 회사는 전 세계 프리미엄 사용자와 미디어 파일을 신디케이트하는 파트너에게 정적 콘텐츠를 공유합니다. 이 회사는 서버 비용을 줄이고 대기 시간이 짧은 전 세계 고객에게 데이터를 안전하게 제공할 수 있는 방법을 찾고 있습니다.가장 적합하고 비용 효율적인 아키텍처를 제공하려면 어떤 서비스 조합을 사용해야 합니까? (2개를 선택하세요.),AWS 파게이트,AWS 글로벌 액셀러레이터,아마존 S3,AWS 람다,아마존 클라우드프론트,,0,,
udemy,CLF-01,123,"A Solutions Architect of a multinational gaming company develops video games for PS4, Xbox One, and Nintendo Switch consoles, plus a number of mobile games for Android and iOS. Due to the wide range of their products and services, the architect proposed that they use API Gateway.What are the key features of API Gateway that the architect can tell to the client? (Select TWO.)",DE,DE,It automatically provides a query language for your APIs similar to GraphQL.,Provides you with static anycast IP addresses that serve as a fixed entry point to your applications hosted in one or more AWS Regions.,Enables you to run applications requiring high levels of inter-node communications at scale on AWS through its custom-built operating system (OS) bypass hardware interface.,You pay only for the API calls you receive and the amount of data transferred out.,Enables you to build RESTful APIs and WebSocket APIs that are optimized for serverless workloads.,,,"다국적 게임 회사의 솔루션 설계자는 PS4, Xbox One 및 Nintendo Switch 콘솔용 비디오 게임과 Android 및 iOS용 여러 모바일 게임을 개발합니다. 광범위한 제품 및 서비스로 인해 설계자는 API 게이트웨이를 사용할 것을 제안했습니다.설계자가 클라이언트에게 알릴 수 있는 API Gateway의 주요 기능은 무엇입니까? (2개를 선택하세요.)",GraphQL과 유사한 API용 쿼리 언어를 자동으로 제공합니다.,하나 이상의 AWS 리전에서 호스팅되는 애플리케이션에 대한 고정 진입점 역할을 하는 정적 애니캐스트 IP 주소를 제공합니다.,맞춤형 운영 체제(OS) 바이패스 하드웨어 인터페이스를 통해 대규모 AWS에서 높은 수준의 노드 간 통신이 필요한 애플리케이션을 실행할 수 있습니다.,수신한 API 호출과 외부로 전송된 데이터 양에 대해서만 비용을 지불합니다.,서버리스 워크로드에 최적화된 RESTful API 및 WebSocket API를 구축할 수 있습니다.,,0,,
udemy,CLF-01,124,"A company developed a web application and deployed it on a fleet of EC2 instances that uses Amazon SQS. The requests are saved as messages in the SQS queue, which is configured with the maximum message retention period. However, after thirteen days of operation, the web application suddenly crashed and there are 10,000 unprocessed messages that are still waiting in the queue. Since they developed the application, they can easily resolve the issue but they need to send a communication to the users on the issue.What information should they provide and what will happen to the unprocessed messages?",B,B,"Tell the users that unfortunately, they have to resubmit all the requests again.",Tell the users that the application will be operational shortly and all received requests will be processed after the web application is restarted.,"Tell the users that the application will be operational shortly however, requests sent over three days ago will need to be resubmitted.","Tell the users that unfortunately, they have to resubmit all of the requests since the queue would not be able to process the 10,000 messages together.",,,,"한 회사에서 웹 애플리케이션을 개발하여 Amazon SQS를 사용하는 EC2 인스턴스 플릿에 배포했습니다. 요청은 최대 메시지 보존 기간으로 구성된 SQS 대기열에 메시지로 저장됩니다. 그러나 운영 13일 후 웹 애플리케이션이 갑자기 중단되었고 아직 대기열에서 대기 중인 처리되지 않은 메시지가 10,000개 있습니다. 그들은 애플리케이션을 개발했기 때문에 문제를 쉽게 해결할 수 있지만 문제에 대해 사용자에게 커뮤니케이션을 보내야 합니다.어떤 정보를 제공해야 하며 처리되지 않은 메시지는 어떻게 됩니까?",유감스럽게도 모든 요청을 다시 제출해야 한다고 사용자에게 알립니다.,사용자에게 애플리케이션이 곧 작동할 것이며 수신된 모든 요청은 웹 애플리케이션이 다시 시작된 후에 처리될 것이라고 알립니다.,애플리케이션이 곧 작동할 것이지만 3일 전에 보낸 요청은 다시 제출해야 한다고 사용자에게 알립니다.,"큐가 10,000개의 메시지를 함께 처리할 수 없기 때문에 불행하게도 모든 요청을 다시 제출해야 한다고 사용자에게 알립니다.",,,0,,
udemy,CLF-01,125,"As part of the Business Continuity Plan of your company, your IT Director instructed you to set up an automated backup of all of the EBS Volumes for your EC2 instances as soon as possible.  What is the fastest and most cost-effective solution to automatically back up all of your EBS Volumes?",B,B,"For an automated solution, create a scheduled job that calls the ""create-snapshot"" command via the AWS CLI to take a snapshot of production EBS volumes periodically.",Use Amazon Data Lifecycle Manager (Amazon DLM) to automate the creation of EBS snapshots.,Set your Amazon Storage Gateway with EBS volumes as the data source and store the backups in your on-premises servers through the storage gateway.,Use an EBS-cycle policy in Amazon S3 to automatically back up the EBS volumes.,,,,회사의 비즈니스 연속성 계획의 일환으로 IT 책임자는 가능한 한 빨리 EC2 인스턴스에 대한 모든 EBS 볼륨의 자동 백업을 설정하도록 지시했습니다.  모든 EBS 볼륨을 자동으로 백업하는 가장 빠르고 비용 효율적인 솔루션은 무엇입니까?,"자동화된 솔루션의 경우 AWS CLI를 통해 ""create-snapshot"" 명령을 호출하는 예약된 작업을 생성하여 프로덕션 EBS 볼륨의 스냅샷을 주기적으로 생성합니다.",Amazon Data Lifecycle Manager(Amazon DLM)를 사용하여 EBS 스냅샷 생성을 자동화합니다.,EBS 볼륨이 있는 Amazon Storage Gateway를 데이터 원본으로 설정하고 스토리지 게이트웨이를 통해 온프레미스 서버에 백업을 저장합니다.,Amazon S3에서 EBS 주기 정책을 사용하여 EBS 볼륨을 자동으로 백업합니다.,,,0,,
udemy,CLF-01,126,"A company is running a multi-tier web application farm in a virtual private cloud (VPC) that is not connected to their corporate network. They are connecting to the VPC over the Internet to manage the fleet of Amazon EC2 instances running in both the public and private subnets. The Solutions Architect has added a bastion host with Microsoft Remote Desktop Protocol (RDP) access to the application instance security groups, but the company wants to further limit administrative access to all of the instances in the VPC.Which of the following bastion host deployment options will meet this requirement?",A,A,Deploy a Windows Bastion host with an Elastic IP address in the public subnet and allow RDP access to bastion only from the corporate IP addresses.,Deploy a Windows Bastion host with an Elastic IP address in the public subnet and allow SSH access to the bastion from anywhere.,"Deploy a Windows Bastion host with an Elastic IP address in the private subnet, and restrict RDP access to the bastion from only the corporate public IP addresses.",Deploy a Windows Bastion host on the corporate network that has RDP access to all EC2 instances in the VPC.,,,,회사는 회사 네트워크에 연결되지 않은 가상 사설 클라우드(VPC)에서 다중 계층 웹 애플리케이션 팜을 실행하고 있습니다. 인터넷을 통해 VPC에 연결하여 퍼블릭 및 프라이빗 서브넷 모두에서 실행되는 Amazon EC2 인스턴스 플릿을 관리합니다. Solutions Architect는 애플리케이션 인스턴스 보안 그룹에 대한 Microsoft RDP(원격 데스크톱 프로토콜) 액세스 권한이 있는 배스천 호스트를 추가했지만 회사는 VPC의 모든 인스턴스에 대한 관리 액세스를 추가로 제한하려고 합니다.다음 배스천 호스트 배포 옵션 중 이 요구 사항을 충족하는 것은 무엇입니까?,탄력적 IP 주소가 있는 Windows 배스천 호스트를 퍼블릭 서브넷에 배포하고 회사 IP 주소에서만 배스천에 대한 RDP 액세스를 허용합니다.,탄력적 IP 주소가 있는 Windows 배스천 호스트를 퍼블릭 서브넷에 배포하고 어디서나 배스천에 대한 SSH 액세스를 허용합니다.,탄력적 IP 주소가 있는 Windows 배스천 호스트를 프라이빗 서브넷에 배포하고 회사 퍼블릭 IP 주소에서만 배스천에 대한 RDP 액세스를 제한합니다.,VPC의 모든 EC2 인스턴스에 대한 RDP 액세스 권한이 있는 회사 네트워크에 Windows Bastion 호스트를 배포합니다.,,,0,,
udemy,CLF-01,127,"A company hosts its web application on a set of Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The application has an embedded NoSQL database. As the application receives more traffic, the application becomes overloaded mainly due to database requests. The management wants to ensure that the database is eventually consistent and highly available.Which of the following options can meet the company requirements with the least operational overhead?",D,D,Configure the Auto Scaling group to spread the Amazon EC2 instances across three Availability Zones. Configure replication of the NoSQL database on the set of Amazon EC2 instances to spread the database load.,Change the ALB with a Network Load Balancer (NLB) to handle more traffic. Use the AWS Migration Service (DMS) to migrate the embedded NoSQL database to Amazon DynamoDB.,Change the ALB with a Network Load Balancer (NLB) to handle more traffic and integrate AWS Global Accelerator to ensure high availability. Configure replication of the NoSQL database on the set of Amazon EC2 instances to spread the database load.,Configure the Auto Scaling group to spread the Amazon EC2 instances across three Availability Zones. Use the AWS Database Migration Service (DMS) with a replication server and an ongoing replication task to migrate the embedded NoSQL database to Amazon DynamoDB,,,,회사는 ALB(Application Load Balancer) 뒤에 있는 Auto Scaling 그룹의 Amazon EC2 인스턴스 세트에서 웹 애플리케이션을 호스팅합니다. 애플리케이션에는 임베디드 NoSQL 데이터베이스가 있습니다. 애플리케이션이 더 많은 트래픽을 수신함에 따라 애플리케이션은 주로 데이터베이스 요청으로 인해 과부하 상태가 됩니다. 경영진은 데이터베이스가 궁극적으로 일관되고 가용성이 높기를 원합니다.다음 중 최소한의 운영 오버헤드로 회사 요구 사항을 충족할 수 있는 옵션은 무엇입니까?,3개의 가용 영역에 Amazon EC2 인스턴스를 분산하도록 Auto Scaling 그룹을 구성합니다. Amazon EC2 인스턴스 세트에서 NoSQL 데이터베이스의 복제를 구성하여 데이터베이스 로드를 분산시킵니다.,더 많은 트래픽을 처리하려면 NLB(Network Load Balancer)로 ALB를 변경하십시오. AWS Migration Service(DMS)를 사용하여 내장된 NoSQL 데이터베이스를 Amazon DynamoDB로 마이그레이션합니다.,NLB(Network Load Balancer)로 ALB를 변경하여 더 많은 트래픽을 처리하고 AWS Global Accelerator를 통합하여 고가용성을 보장하십시오. Amazon EC2 인스턴스 세트에서 NoSQL 데이터베이스의 복제를 구성하여 데이터베이스 로드를 분산시킵니다.,3개의 가용 영역에 Amazon EC2 인스턴스를 분산하도록 Auto Scaling 그룹을 구성합니다. 복제 서버 및 지속적인 복제 작업과 함께 AWS Database Migration Service(DMS)를 사용하여 내장된 NoSQL 데이터베이스를 Amazon DynamoDB로 마이그레이션,,,0,,
udemy,CLF-01,128,"A company has an on-premises MySQL database that needs to be replicated in Amazon S3 as CSV files. The database will eventually be launched to an Amazon Aurora Serverless cluster and be integrated with an RDS Proxy to allow the web applications to pool and share database connections. Once data has been fully copied, the ongoing changes to the on-premises database should be continually streamed into the S3 bucket. The company wants a solution that can be implemented with little management overhead yet still highly secure.Which ingestion pattern should a solutions architect take?",C,C,Use an AWS Snowball Edge cluster to migrate data to Amazon S3 and AWS DataSync to capture ongoing changes. Create your own custom AWS KMS envelope encryption key for the associated AWS Snowball Edge job.,Use AWS Schema Conversion Tool (AWS SCT) to convert MySQL data to CSV files. Set up the AWS Server Migration Service (AWS MGN) to capture ongoing changes from the on-premises MySQL database and send them to Amazon S3.,Create a full load and change data capture (CDC) replication task using AWS Database Migration Service (AWS DMS). Add a new Certificate Authority (CA) certificate and create an AWS DMS endpoint with SSL.,Set up a full load replication task using AWS Database Migration Service (AWS DMS). Launch an AWS DMS endpoint with SSL using the AWS Network Firewall service.,,,,회사에는 Amazon S3에서 CSV 파일로 복제해야 하는 온프레미스 MySQL 데이터베이스가 있습니다. 데이터베이스는 결국 Amazon Aurora Serverless 클러스터로 시작되고 웹 애플리케이션이 데이터베이스 연결을 풀링하고 공유할 수 있도록 RDS 프록시와 통합됩니다. 데이터가 완전히 복사되면 온프레미스 데이터베이스에 대한 진행 중인 변경 사항이 S3 버킷으로 계속 스트리밍되어야 합니다. 회사는 관리 오버헤드가 거의 없이 구현될 수 있으면서도 매우 안전한 솔루션을 원합니다.솔루션 설계자는 어떤 수집 패턴을 취해야 합니까?,AWS Snowball Edge 클러스터를 사용하여 데이터를 Amazon S3 및 AWS DataSync로 마이그레이션하여 진행 중인 변경 사항을 캡처합니다. 연결된 AWS Snowball Edge 작업에 대한 고유한 사용자 지정 AWS KMS 엔벨로프 암호화 키를 생성합니다.,AWS Schema Conversion Tool(AWS SCT)을 사용하여 MySQL 데이터를 CSV 파일로 변환합니다. 온프레미스 MySQL 데이터베이스에서 진행 중인 변경 사항을 캡처하여 Amazon S3로 보내도록 AWS MGN(AWS Server Migration Service)을 설정합니다.,AWS Database Migration Service(AWS DMS)를 사용하여 전체 로드 및 변경 데이터 캡처(CDC) 복제 작업을 생성합니다. 새 인증 기관(CA) 인증서를 추가하고 SSL을 사용하여 AWS DMS 엔드포인트를 생성합니다.,AWS Database Migration Service(AWS DMS)를 사용하여 전체 로드 복제 작업을 설정합니다. AWS Network Firewall 서비스를 사용하여 SSL로 AWS DMS 엔드포인트를 시작합니다.,,,0,,
udemy,CLF-01,129,A company wants to organize the way it tracks its spending on AWS resources. A report that summarizes the total billing accrued by each department must be generated at the end of the month.Which solution will meet the requirements?,D,D,Tag resources with the department name and configure a budget action in AWS Budget.,Use AWS Cost Explorer to view spending and filter usage data by Resource.,Create a Cost and Usage report for AWS services that each department is using.,Tag resources with the department name and enable cost allocation tags.,,,,회사에서 AWS 리소스에 대한 지출을 추적하는 방식을 구성하려고 합니다. 각 부서에서 발생한 총 청구액을 요약한 보고서는 월말에 생성되어야 합니다.어떤 솔루션이 요구 사항을 충족합니까?,부서 이름으로 리소스에 태그를 지정하고 AWS 예산에서 예산 작업을 구성합니다.,AWS Cost Explorer를 사용하여 지출을 보고 사용량 데이터를 Resource.,각 부서에서 사용 중인 AWS 서비스에 대한 비용 및 사용 보고서를 생성합니다.,부서 이름으로 리소스에 태그를 지정하고 비용 할당 태그를 활성화합니다.,,,0,,
udemy,CLF-01,130,A company is running a dashboard application on a Spot EC2 instance inside a private subnet. The dashboard is reachable via a domain name that maps to the private IPv4 address of the instance’s network interface. A solutions architect needs to increase network availability by allowing the traffic flow to resume in another instance if the primary instance is terminated.Which solution accomplishes these requirements?,B,B,Attach an elastic IP address to the instance’s primary network interface and point its IP address to the application’s domain name. Automatically move the EIP to a secondary instance if the primary instance becomes unavailable using the AWS Transit Gateway.,"Create a secondary elastic network interface and point its private IPv4 address to the application’s domain name. Attach the new network interface to the primary instance. If the instance goes down, move the secondary network interface to another instance.",Set up AWS Transfer for FTPS service in Implicit FTPS mode to automatically disable the source/destination checks on the instance’s primary elastic network interface and reassociate it to another instance.,Use the AWS Network Firewall to detach the instance’s primary elastic network interface and move it to a new instance upon failure.,,,,회사는 프라이빗 서브넷 내부의 스팟 EC2 인스턴스에서 대시보드 애플리케이션을 실행하고 있습니다. 대시보드는 인스턴스 네트워크 인터페이스의 프라이빗 IPv4 주소에 매핑되는 도메인 이름을 통해 연결할 수 있습니다. 솔루션 설계자는 기본 인스턴스가 종료된 경우 트래픽 흐름이 다른 인스턴스에서 재개되도록 허용하여 네트워크 가용성을 높여야 합니다.이러한 요구 사항을 충족하는 솔루션은 무엇입니까?,탄력적 IP 주소를 인스턴스의 기본 네트워크 인터페이스에 연결하고 해당 IP 주소가 애플리케이션의 도메인 이름을 가리키도록 합니다. AWS Transit Gateway를 사용하여 기본 인스턴스를 사용할 수 없게 되면 자동으로 EIP를 보조 인스턴스로 이동합니다.,보조 탄력적 네트워크 인터페이스를 생성하고 프라이빗 IPv4 주소가 애플리케이션의 도메인 이름을 가리키도록 합니다. 새 네트워크 인터페이스를 기본 인스턴스에 연결합니다. 인스턴스가 다운되면 보조 네트워크 인터페이스를 다른 인스턴스로 이동합니다.,암시적 FTPS 모드에서 AWS Transfer for FTPS 서비스를 설정하여 source/destination인스턴스의 기본 탄력적 네트워크 인터페이스에 대한 검사를 자동으로 비활성화하고 다른 인스턴스에 다시 연결합니다.,AWS Network Firewall을 사용하여 인스턴스의 기본 탄력적 네트워크 인터페이스를 분리하고 실패 시 새 인스턴스로 이동합니다.,,,0,,
udemy,CLF-01,131,A Solutions Architect working for a startup is designing a High Performance Computing (HPC) application which is publicly accessible for their customers. The startup founders want to mitigate distributed denial-of-service (DDoS) attacks on their application. Which of the following options are not suitable to be implemented in this scenario? (Select TWO.),CD,CD,Use an Application Load Balancer with Auto Scaling groups for your EC2 instances. Prevent direct Internet traffic to your Amazon RDS database by deploying it to a new private subnet.,Use AWS Shield and AWS WAF.,Use Dedicated EC2 instances to ensure that each instance has the maximum performance possible.,Add multiple Elastic Fabric Adapters (EFA) to each EC2 instance to increase the network bandwidth.,Use an Amazon CloudFront service for distributing both static and dynamic content.,,,스타트업에서 일하는 솔루션 설계자는 고객이 공개적으로 액세스할 수 있는 HPC(고성능 컴퓨팅) 애플리케이션을 설계하고 있습니다. 스타트업 설립자는 애플리케이션에 대한 DDoS(분산 서비스 거부) 공격을 완화하기를 원합니다.다음 중 이 시나리오에서 구현하기에 적합하지 않은 옵션은 무엇입니까? (2개를 선택하세요.),EC2 인스턴스에 대해 Auto Scaling 그룹과 함께 Application Load Balancer를 사용하십시오. 새로운 프라이빗 서브넷에 배포하여 Amazon RDS 데이터베이스에 대한 직접적인 인터넷 트래픽을 방지합니다.,AWS Shield 및 AWS WAF를 사용합니다.,전용 EC2 인스턴스를 사용하여 각 인스턴스가 가능한 최대 성능을 갖도록 합니다.,각 EC2 인스턴스에 여러 EFA(Elastic Fabric Adapter)를 추가하여 네트워크 대역폭을 늘립니다.,정적 콘텐츠와 동적 콘텐츠를 모두 배포하려면 Amazon CloudFront 서비스를 사용하십시오.,,0,,
udemy,CLF-01,132,"A company needs to collect gigabytes of data per second from websites and social media feeds to gain insights into its product offerings and continuously improve the user experience. To meet this design requirement, an application is deployed on an Auto Scaling group of Spot EC2 instances which processes the data and stores the results to DynamoDB and Redshift. The solution should have a built-in enhanced fan-out feature.Which fully-managed AWS service can you use to collect and process large streams of data records in real-time with the LEAST amount of administrative overhead?",C,C,AWS Data Exchange,Amazon Managed Streaming for Apache Kafka (Amazon MSK),Amazon Kinesis Data Streams,Amazon S3 Access Points,,,,회사는 웹 사이트 및 소셜 미디어 피드에서 초당 기가바이트의 데이터를 수집하여 제품 제공에 대한 통찰력을 얻고 사용자 경험을 지속적으로 개선해야 합니다. 이 설계 요구 사항을 충족하기 위해 데이터를 처리하고 결과를 DynamoDB 및 Redshift에 저장하는 스팟 EC2 인스턴스의 Auto Scaling 그룹에 애플리케이션이 배포됩니다. 솔루션에는 향상된 팬아웃 기능이 내장되어 있어야 합니다.최소한의 관리 오버헤드로 대규모 데이터 레코드 스트림을 실시간으로 수집하고 처리하는 데 사용할 수 있는 완전 관리형 AWS 서비스는 무엇입니까?,AWS 데이터 교환,Apache Kafka용 Amazon 관리형 스트리밍(Amazon MSK),Amazon Kinesis 데이터 스트림,Amazon S3 액세스 포인트,,,0,,
udemy,CLF-01,133,A Solutions Architect is working for a financial company. The manager wants to have the ability to automatically transfer obsolete data from their S3 bucket to a low-cost storage system in AWS.What is the best solution that the Architect can provide to them?,A,A,Use Lifecycle Policies in S3 to move obsolete data to Glacier.,Use CloudEndure Migration.,Use an EC2 instance and a scheduled job to transfer the obsolete data from their S3 location to Amazon S3 Glacier.,Use Amazon SQS.,,,,Solutions Architect는 금융 회사에서 근무하고 있습니다. 관리자는 사용하지 않는 데이터를 S3 버킷에서 AWS의 저비용 스토리지 시스템으로 자동 전송하는 기능을 원합니다.Architect가 그들에게 제공할 수 있는 최상의 솔루션은 무엇입니까?,S3에서 수명 주기 정책을 사용하여 더 이상 사용되지 않는 데이터를 Glacier로 이동합니다.,CloudEndure 마이그레이션을 사용합니다.,EC2 인스턴스와 예약된 작업을 사용하여 오래된 데이터를 S3 위치에서 Amazon S3 Glacier로 전송합니다.,Amazon SQS를 사용합니다.,,,0,,
udemy,CLF-01,134,"A company is hosting EC2 instances that are on non-production environment and processing non-priority batch loads, which can be interrupted at any time.   What is the best instance purchasing option which can be applied to your EC2 instances in this case?",B,B,On-Demand Instances,Spot Instances,Reserved Instances,On-Demand Capacity Reservations,,,,회사는 비프로덕션 환경에 있는 EC2 인스턴스를 호스팅하고 언제든지 중단될 수 있는 우선 순위가 아닌 배치 로드를 처리하고 있습니다.   이 경우 EC2 인스턴스에 적용할 수 있는 최상의 인스턴스 구매 옵션은 무엇입니까?,온디맨드 인스턴스,스팟 인스턴스,예약 인스턴스,온디맨드 용량 예약,,,0,,
udemy,CLF-01,135,"A document sharing website is using AWS as its cloud infrastructure. Free users can upload a total of 5 GB data while premium users can upload as much as 5 TB. Their application uploads the user files, which can have a max file size of 1 TB, to an S3 Bucket. In this scenario, what is the best way for the application to upload the large files in S3?",D,D,Use AWS Import/Export,Use a single PUT request to upload the large file,Use AWS Snowball,Use Multipart Upload,,,,문서 공유 웹 사이트는 AWS를 클라우드 인프라로 사용하고 있습니다. 무료 사용자는 총 5GB의 데이터를 업로드할 수 있으며 프리미엄 사용자는 최대 5TB의 데이터를 업로드할 수 있습니다. 그들의 애플리케이션은 최대 파일 크기가 1TB일 수 있는 사용자 파일을 S3 버킷에 업로드합니다.이 시나리오에서 애플리케이션이 S3에 대용량 파일을 업로드하는 가장 좋은 방법은 무엇입니까?,AWS 가져오기/내보내기 사용,단일 PUT 요청을 사용하여 대용량 파일 업로드,AWS Snowball 사용,멀티파트 업로드 사용,,,0,,
udemy,CLF-01,136,"A manufacturing company has EC2 instances running in AWS. The EC2 instances are configured with Auto Scaling. There are a lot of requests being lost because of too much load on the servers. The Auto Scaling is launching new EC2 instances to take the load accordingly yet, there are still some requests that are being lost. Which of the following is the MOST suitable solution that you should implement to avoid losing recently submitted requests?",A,A,Use an Amazon SQS queue to decouple the application components and scale-out the EC2 instances based upon the ApproximateNumberOfMessages metric in Amazon CloudWatch.,Replace the Auto Scaling group with a cluster placement group to achieve a low-latency network performance necessary for tightly-coupled node-to-node communication.,Use larger instances for your application with an attached Elastic Fabric Adapter (EFA).,"Set up Amazon Aurora Serverless for on-demand, auto-scaling configuration of your EC2 Instances and also enable Amazon Aurora Parallel Query feature for faster analytical queries over your current data.",,,,제조 회사에는 AWS에서 실행되는 EC2 인스턴스가 있습니다. EC2 인스턴스는 Auto Scaling으로 구성됩니다. 서버에 너무 많은 부하가 걸려 손실되는 요청이 많습니다. Auto Scaling은 새로운 EC2 인스턴스를 시작하여 이에 따라 부하를 처리하고 있지만 아직 일부 요청이 손실되고 있습니다.다음 중 최근에 제출한 요청이 손실되지 않도록 구현해야 하는 가장 적합한 솔루션은 무엇입니까?,ApproximateNumberOfMessagesAmazon SQS 대기열을 사용하여 애플리케이션 구성 요소를 분리하고 Amazon CloudWatch의 지표를 기반으로 EC2 인스턴스를 확장합니다 .,밀접하게 연결된 노드 간 통신에 필요한 지연 시간이 짧은 네트워크 성능을 달성하려면 Auto Scaling 그룹을 클러스터 배치 그룹으로 교체하십시오.,EFA(Elastic Fabric Adapter)가 연결된 애플리케이션에 더 큰 인스턴스를 사용하세요.,EC2 인스턴스의 온디맨드 자동 확장 구성을 위해 Amazon Aurora Serverless를 설정하고 현재 데이터에 대한 더 빠른 분석 쿼리를 위해 Amazon Aurora 병렬 쿼리 기능을 활성화합니다.,,,0,,
udemy,CLF-01,137,A large telecommunications company needs to run analytics against all combined log files from the Application Load Balancer as part of the regulatory requirements.Which AWS services can be used together to collect logs and then easily perform log analysis?,A,A,Amazon S3 for storing ELB log files and Amazon EMR for analyzing the log files.,Amazon DynamoDB for storing and EC2 for analyzing the logs.,Amazon EC2 with EBS volumes for storing and analyzing the log files.,Amazon S3 for storing the ELB log files and an EC2 instance for analyzing the log files using a custom-built application.,,,,대규모 통신 회사는 규제 요구 사항의 일부로 Application Load Balancer에서 결합된 모든 로그 파일에 대해 분석을 실행해야 합니다.어떤 AWS 서비스를 함께 사용하여 로그를 수집한 다음 쉽게 로그 분석을 수행할 수 있습니까?,ELB 로그 파일 저장을 위한 Amazon S3 및 로그 파일 분석을 위한 Amazon EMR.,저장을 위한 Amazon DynamoDB 및 로그 분석을 위한 EC2.,로그 파일을 저장하고 분석하기 위한 EBS 볼륨이 있는 Amazon EC2.,ELB 로그 파일을 저장하기 위한 Amazon S3 및 맞춤형 애플리케이션을 사용하여 로그 파일을 분석하기 위한 EC2 인스턴스.,,,0,,
udemy,CLF-01,138,"A call center wants to use Artificial Intelligence(AI) to extract insights from audio recordings to assess the quality of its customer service. The calls are available in both English and Hindi. A sentiment analysis report in English must be generated for each recording to assess whether or not the customer had a positive experience. Once the solution is completed, new languages will eventually be supported, such as Arabic, Mandarin, and Spanish.How can the solutions architect build the solution without maintaining any machine learning model?",B,B,Utilize the Amazon Lex service to convert audio recordings into text. Call the Amazon Translate API to translate Hindi texts into English and use Amazon Forecast for sentiment prediction and analysis.,Convert audio recordings into text using Amazon Transcribe. Set up Amazon Translate to translate Hindi texts into English and use Amazon Comprehend for sentiment analysis.,Transcribe audio recordings into text using Amazon Polly. Set up Amazon Rekognition to recognize and automatically translate Hindi texts into English. Use the combination of Amazon Fraud Detector and Amazon SageMaker BlazingText algorithm for sentiment analysis.,Set up Amazon Comprehend to convert audio recordings into text. Use Amazon Kendra to translate Hindi texts into English and utilize the Amazon Detective service to automatically detect negative user behaviors for sentiment analysis.,,,,"콜 센터는 AI(인공 지능)를 사용하여 오디오 녹음에서 통찰력을 추출하여 고객 서비스 품질을 평가하려고 합니다. 통화는 영어와 힌디어로 모두 가능합니다. 고객이 긍정적인 경험을 했는지 여부를 평가하기 위해 각 녹음에 대해 영어로 된 감정 분석 보고서를 생성해야 합니다. 솔루션이 완료되면 결국 아랍어, 만다린, 스페인어와 같은 새로운 언어가 지원될 것입니다.솔루션 설계자는 기계 학습 모델을 유지하지 않고 어떻게 솔루션을 구축할 수 있습니까?",Amazon Lex 서비스를 활용하여 오디오 녹음을 텍스트로 변환합니다. Amazon Translate API를 호출하여 힌디어 텍스트를 영어로 번역하고 감정 예측 및 분석에 Amazon Forecast를 사용합니다.,Amazon Transcribe를 사용하여 오디오 녹음을 텍스트로 변환합니다. 힌디어 텍스트를 영어로 번역하고 감정 분석을 위해 Amazon Comprehend를 사용하도록 Amazon Translate를 설정합니다.,Amazon Polly를 사용하여 오디오 녹음을 텍스트로 변환합니다. 힌디어 텍스트를 인식하고 자동으로 영어로 번역하도록 Amazon Rekognition을 설정합니다. 감정 분석을 위해 Amazon Fraud Detector와 Amazon SageMaker BlazingText 알고리즘의 조합을 사용합니다.,오디오 녹음을 텍스트로 변환하도록 Amazon Comprehend를 설정합니다. Amazon Kendra를 사용하여 힌디어 텍스트를 영어로 번역하고 Amazon Detective 서비스를 활용하여 감정 분석을 위해 부정적인 사용자 행동을 자동으로 감지합니다.,,,0,,
udemy,CLF-01,139,"A company that is rapidly growing in recent months has been in the process of setting up IAM users on its single AWS Account. A solutions architect has been tasked to handle the user management, which includes granting read-only access to users and denying permissions whenever an IAM user has no MFA setup. New users will be added frequently based on their respective departments.Which of the following action is the MOST secure way to grant permissions to the new users?",A,A,Launch an IAM Group for each department. Create an IAM Policy that enforces MFA authentication with the least privilege permission. Attach the IAM Policy to each IAM Group.,Create a Service Control Policy (SCP) that enforces MFA authentication for each department. Add a trust relationship to every SCP and attach it to each IAM User.,Set up IAM roles for each IAM user and associate a permissions boundary that defines the maximum permissions.,Create an IAM Role that enforces MFA authentication with the least privilege permission. Set up a corresponding IAM Group for each department. Attach the IAM Role to the IAM Groups.,,,,최근 몇 달 동안 빠르게 성장하고 있는 회사는 단일 AWS 계정에서 IAM 사용자를 설정하는 과정에 있었습니다. 솔루션 설계자는 IAM 사용자에게 MFA 설정이 없을 때마다 사용자에게 읽기 전용 액세스 권한을 부여하고 권한을 거부하는 것을 포함하는 사용자 관리를 처리하는 임무를 받았습니다. 새 사용자는 해당 부서에 따라 자주 추가됩니다.다음 중 새 사용자에게 권한을 부여하는 가장 안전한 방법은 무엇입니까?,부서별로 IAM 그룹을 시작합니다. 최소 권한으로 MFA 인증을 시행하는 IAM 정책을 생성합니다. IAM 정책을 각 IAM 그룹에 연결합니다.,각 부서에 대해 MFA 인증을 적용하는 SCP(서비스 제어 정책)를 만듭니다. 모든 SCP에 신뢰 관계를 추가하고 각 IAM 사용자에게 연결합니다.,각 IAM 사용자에 대한 IAM 역할을 설정하고 최대 권한을 정의하는 권한 경계를 연결합니다.,최소 권한 권한으로 MFA 인증을 시행하는 IAM 역할을 생성합니다. 각 부서에 해당하는 IAM 그룹을 설정합니다. IAM 역할을 IAM 그룹에 연결합니다.,,,0,,
udemy,CLF-01,140,An aerospace engineering company recently adopted a hybrid cloud infrastructure with AWS. One of the Solutions Architect’s tasks is to launch a VPC with both public and private subnets for their EC2 instances as well as their database instances.Which of the following statements are true regarding Amazon VPC subnets? (Select TWO.),AE,AE,Each subnet maps to a single Availability Zone.,Each subnet spans to 2 Availability Zones.,"The allowed block size in VPC is between a /16 netmask (65,536 IP addresses) and /27 netmask (32 IP addresses).",EC2 instances in a private subnet can communicate with the Internet only if they have an Elastic IP.,Every subnet that you create is automatically associated with the main route table for the VPC.,,,항공 우주 공학 회사는 최근 AWS와 함께 하이브리드 클라우드 인프라를 채택했습니다. Solutions Architect의 작업 중 하나는 EC2 인스턴스와 데이터베이스 인스턴스에 대한 퍼블릭 및 프라이빗 서브넷이 모두 있는 VPC를 시작하는 것입니다.다음 중 Amazon VPC 서브넷에 대한 설명으로 옳은 것은 무엇입니까? (2개를 선택하세요.),각 서브넷은 단일 가용 영역에 매핑됩니다.,각 서브넷은 2개의 가용 영역에 걸쳐 있습니다.,"VPC에서 허용되는 블록 크기는 /16 넷마스크(IP 주소 65,536개)와 /27 넷마스크(IP 주소 32개) 사이입니다.",프라이빗 서브넷의 EC2 인스턴스는 탄력적 IP가 있는 경우에만 인터넷과 통신할 수 있습니다.,생성한 모든 서브넷은 VPC의 기본 라우팅 테이블과 자동으로 연결됩니다.,,0,,
udemy,CLF-01,141,"A large financial firm needs to set up a Linux bastion host to allow access to the Amazon EC2 instances running in their VPC. For security purposes, only the clients connecting from the corporate external public IP address 175.45.116.100 should have SSH access to the host.Which is the best option that can meet the customer's requirement?",B,B,"Network ACL Inbound Rule: Protocol – TCP, Port Range-22, Source 175.45.116.100/0","Security Group Inbound Rule: Protocol – TCP. Port Range – 22, Source 175.45.116.100/32","Security Group Inbound Rule: Protocol – UDP, Port Range – 22, Source 175.45.116.100/32","Network ACL Inbound Rule: Protocol – UDP, Port Range – 22, Source 175.45.116.100/32",,,,대규모 금융 회사는 VPC에서 실행되는 Amazon EC2 인스턴스에 액세스할 수 있도록 Linux 배스천 호스트를 설정해야 합니다. 보안을 위해 회사 외부 공용 IP 주소 175.45.116.100에서 연결하는 클라이언트만 호스트에 대한 SSH 액세스 권한을 가져야 합니다.고객의 요구 사항을 충족할 수 있는 최상의 옵션은 무엇입니까?,"네트워크 ACL 인바운드 규칙: 프로토콜 – TCP, 포트 범위-22, 소스 175.45.116.100/0","보안 그룹 인바운드 규칙: 프로토콜 – TCP. 포트 범위 – 22, 소스 175.45.116.100/32","보안 그룹 인바운드 규칙: 프로토콜 – UDP, 포트 범위 – 22, 소스 175.45.116.100/32","네트워크 ACL 인바운드 규칙: 프로토콜 – UDP, 포트 범위 – 22, 소스 175.45.116.100/32",,,0,,
udemy,CLF-01,142,"A company is hosting an application on EC2 instances that regularly pushes and fetches data in Amazon S3. Due to a change in compliance, the instances need to be moved on a private subnet. Along with this change, the company wants to lower the data transfer costs by configuring its AWS resources.How can this be accomplished in the MOST cost-efficient manner?",B,B,Create an Amazon S3 interface endpoint to enable a connection between the instances and Amazon S3.,Create an Amazon S3 gateway endpoint to enable a connection between the instances and Amazon S3.,Set up an AWS Transit Gateway to access Amazon S3.,Set up a NAT Gateway in the public subnet to connect to Amazon S3.,,,,회사는 Amazon S3에서 정기적으로 데이터를 푸시하고 가져오는 EC2 인스턴스에서 애플리케이션을 호스팅하고 있습니다. 규정 준수 변경으로 인해 인스턴스를 프라이빗 서브넷으로 이동해야 합니다. 이러한 변화와 함께 회사는 AWS 리소스를 구성하여 데이터 전송 비용을 낮추려고 합니다.이를 가장 비용 효율적인 방식으로 수행할 수 있는 방법은 무엇입니까?,Amazon S3 인터페이스 엔드포인트를 생성하여 인스턴스와 Amazon S3 간의 연결을 활성화합니다.,Amazon S3 게이트웨이 엔드포인트를 생성하여 인스턴스와 Amazon S3 간의 연결을 활성화합니다.,Amazon S3에 액세스하도록 AWS Transit Gateway를 설정합니다.,퍼블릭 서브넷에서 NAT 게이트웨이를 설정하여 Amazon S3에 연결합니다.,,,0,,
udemy,CLF-01,143,A Solutions Architect is designing a highly available environment for an application. She plans to host the application on EC2 instances within an Auto Scaling Group. One of the conditions requires data stored on root EBS volumes to be preserved if an instance terminates.What should be done to satisfy the requirement?,D,D,Use AWS DataSync to replicate root volume data to Amazon S3.,Enable the Termination Protection option for all EC2 instances.,Configure ASG to suspend the health check process for each EC2 instance.,Set the value of DeleteOnTermination attribute of the EBS volumes to False.,,,,Solutions Architect는 애플리케이션을 위한 고가용성 환경을 설계하고 있습니다. 그녀는 Auto Scaling 그룹 내의 EC2 인스턴스에서 애플리케이션을 호스팅할 계획입니다. 조건 중 하나는 인스턴스가 종료되는 경우 루트 EBS 볼륨에 저장된 데이터를 보존해야 한다는 것입니다.요구 사항을 충족하려면 어떻게 해야 합니까?,AWS DataSync를 사용하여 루트 볼륨 데이터를 Amazon S3에 복제합니다.,모든 EC2 인스턴스에 대해 종료 보호 옵션을 활성화합니다.,각 EC2 인스턴스에 대한 상태 확인 프로세스를 일시 중단하도록 ASG를 구성합니다.,DeleteOnTerminationEBS 볼륨의 속성 값을 로 설정합니다 False.,,,0,,
udemy,CLF-01,144,"A data analytics company is setting up an innovative checkout-free grocery store. Their Solutions Architect developed a real-time monitoring application that uses smart sensors to collect the items that the customers are getting from the grocery’s refrigerators and shelves then automatically deduct it from their accounts. The company wants to analyze the items that are frequently being bought and store the results in S3 for durable storage to determine the purchase behavior of its customers. What service must be used to easily capture, transform, and load streaming data into Amazon S3, Amazon Elasticsearch Service, and Splunk?",A,A,Amazon Kinesis Data Firehose,Amazon SQS,Amazon Redshift,Amazon Kinesis,,,,"데이터 분석 회사는 계산대가 없는 혁신적인 식료품점을 만들고 있습니다. 그들의 Solutions Architect는 스마트 센서를 사용하여 고객이 식료품점의 냉장고와 선반에서 가져오는 품목을 수집한 다음 고객 계정에서 자동으로 공제하는 실시간 모니터링 애플리케이션을 개발했습니다. 회사는 자주 구매하는 품목을 분석하고 그 결과를 S3에 저장하여 내구성 있는 스토리지에 저장하여 고객의 구매 행동을 파악하려고 합니다.Amazon S3, Amazon Elasticsearch Service 및 Splunk로 스트리밍 데이터를 쉽게 캡처, 변환 및 로드하려면 어떤 서비스를 사용해야 합니까?",Amazon Kinesis Data Firehose,아마존 SQS,아마존 레드시프트,아마존 키네시스,,,0,,
udemy,CLF-01,145,"The media company that you are working for has a video transcoding application running on Amazon EC2. Each EC2 instance polls a queue to find out which video should be transcoded, and then runs a transcoding process. If this process is interrupted, the video will be transcoded by another instance based on the queuing system. This application has a large backlog of videos which need to be transcoded. Your manager would like to reduce this backlog by adding more EC2 instances, however, these instances are only needed until the backlog is reduced. In this scenario, which type of Amazon EC2 instance is the most cost-effective type to use?",B,B,Dedicated instances,Spot instances,On-demand instances,Reserved instances,,,,귀하가 근무하고 있는 미디어 회사에는 Amazon EC2에서 실행되는 비디오 트랜스코딩 애플리케이션이 있습니다. 각 EC2 인스턴스는 대기열을 폴링하여 어떤 비디오를 트랜스코딩해야 하는지 찾은 다음 트랜스코딩 프로세스를 실행합니다. 이 프로세스가 중단되면 비디오는 대기열 시스템을 기반으로 하는 다른 인스턴스에 의해 트랜스코딩됩니다. 이 애플리케이션에는 트랜스코딩해야 하는 비디오의 대량 백로그가 있습니다. 관리자는 더 많은 EC2 인스턴스를 추가하여 이 백로그를 줄이려고 하지만 이러한 인스턴스는 백로그가 줄어들 때까지만 필요합니다.이 시나리오에서 사용하기에 가장 비용 효율적인 Amazon EC2 인스턴스 유형은 무엇입니까?,전용 인스턴스,스팟 인스턴스,주문형 인스턴스,예약 인스턴스,,,0,,
udemy,CLF-01,146,A company currently has an Augment Reality (AR) mobile game that has a serverless backend. It is using a DynamoDB table which was launched using the AWS CLI to store all the user data and information gathered from the players and a Lambda function to pull the data from DynamoDB. The game is being used by millions of users each day to read and store data.How would you design the application to improve its overall performance and make it more scalable while keeping the costs low? (Select TWO.),AB,AB,Enable DynamoDB Accelerator (DAX) and ensure that the Auto Scaling is enabled and increase the maximum provisioned read and write capacity.,Use API Gateway in conjunction with Lambda and turn on the caching on frequently accessed data and enable DynamoDB global replication.,"Since Auto Scaling is enabled by default, the provisioned read and write capacity will adjust automatically. Also enable DynamoDB Accelerator (DAX) to improve the performance from milliseconds to microseconds.",Configure CloudFront with DynamoDB as the origin; cache frequently accessed data on the client device using ElastiCache.,Use AWS SSO and Cognito to authenticate users and have them directly access DynamoDB using single-sign on. Manually set the provisioned read and write capacity to a higher RCU and WCU.,,,회사는 현재 서버리스 백엔드가 있는 증강 현실(AR) 모바일 게임을 보유하고 있습니다. 플레이어에서 수집한 모든 사용자 데이터와 정보를 저장하기 위해 AWS CLI를 사용하여 시작된 DynamoDB 테이블과 DynamoDB에서 데이터를 가져오는 Lambda 함수를 사용하고 있습니다. 이 게임은 매일 수백만 명의 사용자가 데이터를 읽고 저장하는 데 사용하고 있습니다.비용을 낮게 유지하면서 전반적인 성능을 개선하고 확장성을 높이려면 애플리케이션을 어떻게 설계하시겠습니까? (2개를 선택하세요.),DynamoDB Accelerator(DAX)를 활성화하고 Auto Scaling이 활성화되었는지 확인하고 프로비저닝된 최대 읽기 및 쓰기 용량을 늘립니다.,Lambda와 함께 API Gateway를 사용하고 자주 액세스하는 데이터에 대한 캐싱을 켜고 DynamoDB 글로벌 복제를 활성화합니다.,Auto Scaling은 기본적으로 활성화되어 있으므로 프로비저닝된 읽기 및 쓰기 용량이 자동으로 조정됩니다. 또한 DynamoDB Accelerator(DAX)를 활성화하여 성능을 밀리초에서 마이크로초로 개선합니다.,DynamoDB를 오리진으로 사용하여 CloudFront를 구성합니다. ElastiCache를 사용하여 클라이언트 장치에서 자주 액세스하는 데이터를 캐시합니다.,AWS SSO 및 Cognito를 사용하여 사용자를 인증하고 Single Sign-On을 사용하여 DynamoDB에 직접 액세스하도록 합니다. 프로비저닝된 읽기 및 쓰기 용량을 더 높은 RCU 및 WCU에 수동으로 설정합니다.,,0,,
udemy,CLF-01,147,"A new online banking platform has been re-designed to have a microservices architecture in which complex applications are decomposed into smaller, independent services. The new platform uses Kubernetes, and the application containers are optimally configured for running small, decoupled services.The new solution should remove the need to provision and manage servers, let you specify and pay for resources per application as well as improve security through application isolation by design.Which of the following is the MOST suitable solution to implement to launch this new platform to AWS?",A,A,Use AWS Fargate on Amazon EKS with Service Auto Scaling to run the containerized banking platform,Deploy an Amazon EKS Cluster on AWS Outposts with Kubernetes Cluster Autoscaler and sync any orphaned pods with Amazon AppFlow,Use Amazon ECS to run the Kubernetes cluster on AWS Fargate,Host the application in Amazon EMR Serverless and an EBS storage with the fast snapshot restore feature enabled,,,,"새로운 온라인 뱅킹 플랫폼은 복잡한 애플리케이션이 더 작고 독립적인 서비스로 분해되는 마이크로서비스 아키텍처를 갖도록 재설계되었습니다. 새로운 플랫폼은 Kubernetes를 사용하며 애플리케이션 컨테이너는 분리된 소규모 서비스를 실행하도록 최적으로 구성됩니다.새로운 솔루션은 서버를 프로비저닝하고 관리할 필요가 없으며, 애플리케이션별로 리소스를 지정하고 비용을 지불할 수 있을 뿐만 아니라 설계에 따라 애플리케이션 격리를 통해 보안을 향상시킬 수 있습니다.다음 중 이 새로운 플랫폼을 AWS에 출시하기 위해 구현하기에 가장 적합한 솔루션은 무엇입니까?",Service Auto Scaling과 함께 Amazon EKS에서 AWS Fargate를 사용하여 컨테이너화된 뱅킹 플랫폼 실행,Kubernetes Cluster Autoscaler를 사용하여 AWS Outposts에 Amazon EKS 클러스터를 배포하고 Amazon AppFlow와 분리된 포드를 동기화합니다.,Amazon ECS를 사용하여 AWS Fargate에서 Kubernetes 클러스터 실행,빠른 스냅샷 복원 기능이 활성화된 Amazon EMR 서버리스 및 EBS 스토리지에서 애플리케이션 호스팅,,,0,,
udemy,CLF-01,148,A company requires that all AWS resources be tagged with a standard naming convention for better access control. The company’s solutions architect must implement a solution that checks for untagged AWS resources.Which solution requires the least amount of effort to implement?,A,A,Use an AWS Config rule to detect non-compliant tags.,Create a Lambda function that runs compliance checks on tagged resources. Schedule the function using Amazon EventBridge.,Use tag policies in AWS Organizations to standardize the naming of tags. Store all the tags in an Amazon S3 bucket with the S3 Object Lock feature enabled.,Use service control policies (SCP) to detect resources that are not tagged properly.,,,,회사는 더 나은 액세스 제어를 위해 모든 AWS 리소스에 표준 명명 규칙으로 태그를 지정하도록 요구합니다. 회사의 솔루션 설계자는 태그가 지정되지 않은 AWS 리소스를 확인하는 솔루션을 구현해야 합니다.구현하는 데 최소한의 노력이 필요한 솔루션은 무엇입니까?,비준수 태그를 감지하려면 AWS Config 규칙을 사용하십시오.,태그가 지정된 리소스에 대한 규정 준수 검사를 실행하는 Lambda 함수를 생성합니다. Amazon EventBridge를 사용하여 함수를 예약합니다.,AWS Organizations의 태그 정책을 사용하여 태그 이름 지정을 표준화합니다. S3 객체 잠금 기능이 활성화된 Amazon S3 버킷에 모든 태그를 저장합니다.,서비스 제어 정책(SCP)을 사용하여 적절하게 태그가 지정되지 않은 리소스를 감지합니다.,,,0,,
udemy,CLF-01,149,"A company is setting up a cloud architecture for an international money transfer service to be deployed in AWS which will have thousands of users around the globe. The service should be available 24/7 to avoid any business disruption and should be resilient enough to handle the outage of an entire AWS region. To meet this requirement, the Solutions Architect has deployed their AWS resources to multiple AWS Regions. He needs to use Route 53 and configure it to set all of the resources to be available all the time as much as possible. When a resource becomes unavailable, Route 53 should detect that it's unhealthy and stop including it when responding to queries.Which of the following is the most fault-tolerant routing configuration that the Solutions Architect should use in this scenario?",B,B,Configure an Active-Passive Failover with Multiple Primary and Secondary Resources.,Configure an Active-Active Failover with Weighted routing policy.,Configure an Active-Passive Failover with Weighted Records.,Configure an Active-Active Failover with One Primary and One Secondary Resource.,,,,한 회사가 전 세계 수천 명의 사용자를 보유하게 될 AWS에 배포할 국제 송금 서비스용 클라우드 아키텍처를 설정하고 있습니다. 서비스는 비즈니스 중단을 방지하기 위해 연중무휴 24시간 사용 가능해야 하며 전체 AWS 리전의 중단을 처리할 수 있을 만큼 탄력적이어야 합니다. 이 요구 사항을 충족하기 위해 Solutions Architect는 AWS 리소스를 여러 AWS 리전에 배포했습니다. 그는 Route 53을 사용하고 모든 리소스를 가능한 한 항상 사용할 수 있도록 구성해야 합니다. 리소스를 사용할 수 없게 되면 Route 53은 비정상임을 감지하고 쿼리에 응답할 때 리소스 포함을 중지해야 합니다.솔루션 아키텍트가 이 시나리오에서 사용해야 하는 내결함성 라우팅 구성은 다음 중 무엇입니까?,여러 기본 및 보조 리소스로 능동-수동 장애 조치를 구성합니다.,가중 라우팅 정책을 사용하여 활성-활성 장애 조치를 구성합니다.,가중 기록을 사용하여 능동-수동 장애 조치를 구성합니다.,하나의 기본 리소스와 하나의 보조 리소스로 활성-활성 장애 조치를 구성합니다.,,,0,,
udemy,CLF-01,150,"In Amazon EC2, you can manage your instances from the moment you launch them up to their termination. You can flexibly control your computing costs by changing the EC2 instance state. Which of the following statements is true regarding EC2 billing? (Select TWO.)",BD,BD,You will not be billed for any instance usage while an instance is not in the running state.,You will be billed when your On-Demand instance is preparing to hibernate with a stopping state.,You will be billed when your Spot instance is preparing to stop with a stopping state.,You will be billed when your Reserved instance is in terminated state.,You will be billed when your On-Demand instance is in pending state.,,,Amazon EC2에서는 인스턴스를 시작한 순간부터 종료될 때까지 인스턴스를 관리할 수 있습니다. EC2 인스턴스 상태를 변경하여 컴퓨팅 비용을 유연하게 제어할 수 있습니다. 다음 중 EC2 결제에 대한 설명으로 옳은 것은 무엇입니까? (2개를 선택하세요.),인스턴스가 상태에 있지 않은 동안에는 인스턴스 사용량에 대해 요금이 청구되지 않습니다 running.,온디맨드 인스턴스가 상태와 함께 최대 절전 모드를 준비할 때 요금이 청구됩니다 stopping.,스팟 인스턴스가 상태 중지를 준비할 때 요금이 청구됩니다 stopping.,예약 인스턴스가 상태가 되면 요금이 청구됩니다 terminated.,온디맨드 인스턴스가 상태가 되면 요금이 청구됩니다 pending.,,0,,
udemy,CLF-01,151,A company is building an automation tool for generating custom reports on its AWS usage. The company must be able to programmatically access and forecast usage costs on specific services.Which of the following would meet the requirements with the LEAST amount of operational overhead?,C,C,Utilize the downloadable AWS Cost Explorer report .csv files to access the cost-related data. Predict usage costs using Amazon Forecast.,Generate AWS Budgets reports for usage cost data and deliver them via Amazon Simple Queue Service (SQS).,Use the AWS Cost Explorer API with pagination to programmatically retrieve the usage cost-related data.,Configure AWS Budgets to send usage cost data to the company via Amazon SNS.,,,,회사에서 AWS 사용에 대한 사용자 지정 보고서를 생성하기 위한 자동화 도구를 구축하고 있습니다. 회사는 프로그래밍 방식으로 특정 서비스에 대한 사용 비용에 액세스하고 예측할 수 있어야 합니다.다음 중 운영 오버헤드가 가장 적은 요구 사항을 충족하는 것은 무엇입니까?,다운로드 가능한 AWS Cost Explorer 보고서 .csv 파일을 활용하여 비용 관련 데이터에 액세스하십시오. Amazon Forecast를 사용하여 사용 비용을 예측합니다.,사용 비용 데이터에 대한 AWS 예산 보고서를 생성하고 Amazon Simple Queue Service(SQS)를 통해 전달합니다.,페이지 매김과 함께 AWS Cost Explorer API를 사용하여 사용 비용 관련 데이터를 프로그래밍 방식으로 검색합니다.,Amazon SNS를 통해 회사에 사용 비용 데이터를 전송하도록 AWS 예산을 구성합니다.,,,0,,
udemy,CLF-01,152,A company hosted a web application on a Linux Amazon EC2 instance in the public subnet that uses a default network ACL. The instance uses a default security group and has an attached Elastic IP address. The network ACL has been configured to block all traffic to the instance. The Solutions Architect must allow incoming traffic on port 443 to access the application from any source.Which combination of steps will accomplish this requirement? (Select TWO.),CE,CE,"In the Security Group, create a new rule to allow TCP connection on port 443 to destination 0.0.0.0/0","In the Network ACL, update the rule to allow both inbound and outbound TCP connection on port 443 from source 0.0.0.0/0 and to destination 0.0.0.0/0","In the Network ACL, update the rule to allow inbound TCP connection on port 443 from source 0.0.0.0/0 and outbound TCP connection on port 32768 - 65535 to destination 0.0.0.0/0","In the Network ACL, update the rule to allow outbound TCP connection on port 32768 - 65535 to destination 0.0.0.0/0","In the Security Group, add a new rule to allow TCP connection on port 443 from source 0.0.0.0/0",,,회사는 기본 네트워크 ACL을 사용하는 퍼블릭 서브넷의 Linux Amazon EC2 인스턴스에서 웹 애플리케이션을 호스팅했습니다. 인스턴스는 기본 보안 그룹을 사용하고 탄력적 IP 주소가 연결되어 있습니다. 인스턴스에 대한 모든 트래픽을 차단하도록 네트워크 ACL이 구성되었습니다. Solutions Architect는 모든 소스에서 애플리케이션에 액세스하려면 포트 443에서 들어오는 트래픽을 허용해야 합니다.이 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개를 선택하세요.),보안 그룹에서 포트 443에서 대상에 대한 TCP 연결을 허용하는 새 규칙을 만듭니다.0.0.0.0/0,0.0.0.0/0네트워크 ACL에서 소스에서 대상으로 포트 443에서 인바운드 및 아웃바운드 TCP 연결을 모두 허용하도록 규칙을 업데이트합니다.0.0.0.0/0,네트워크 ACL에서 소스에서 포트 443의 인바운드 TCP 연결과 대상 0.0.0.0/0포트의 아웃바운드 TCP 연결을 허용하도록 규칙을 업데이트합니다.32768 - 655350.0.0.0/0,네트워크 ACL에서 32768 - 65535대상에 대한 포트에서 아웃바운드 TCP 연결을 허용하도록 규칙을 업데이트합니다.0.0.0.0/0,보안 그룹에서 소스의 포트 443에서 TCP 연결을 허용하는 새 규칙을 추가합니다.0.0.0.0/0,,0,,
udemy,CLF-01,153,"A company is deploying a Microsoft SharePoint Server environment on AWS using CloudFormation. The Solutions Architect needs to install and configure the architecture that is composed of Microsoft Active Directory (AD) domain controllers, Microsoft SQL Server 2012, multiple Amazon EC2 instances to host the Microsoft SharePoint Server and many other dependencies. The Architect needs to ensure that the required components are properly running before the stack creation proceeds.Which of the following should the Architect do to meet this requirement?",D,D,Configure the DependsOn attribute in the CloudFormation template. Send a success signal after the applications are installed and configured using the cfn-init helper script.,Configure a UpdatePolicy attribute to the instance in the CloudFormation template. Send a success signal after the applications are installed and configured using the cfn-signal helper script.,Configure the UpdateReplacePolicy attribute in the CloudFormation template. Send a success signal after the applications are installed and configured using the cfn-signal helper script.,Configure a CreationPolicy attribute to the instance in the CloudFormation template. Send a success signal after the applications are installed and configured using the cfn-signal helper script.,,,,"회사에서 CloudFormation을 사용하여 AWS에 Microsoft SharePoint Server 환경을 배포하고 있습니다. Solutions Architect는 Microsoft Active Directory(AD) 도메인 컨트롤러, Microsoft SQL Server 2012, Microsoft SharePoint Server 및 기타 여러 종속성을 호스팅하기 위한 여러 Amazon EC2 인스턴스로 구성된 아키텍처를 설치하고 구성해야 합니다. Architect는 스택 생성이 진행되기 전에 필요한 구성 요소가 제대로 실행되고 있는지 확인해야 합니다.Architect가 이 요구 사항을 충족하려면 다음 중 무엇을 해야 합니까?",DependsOnCloudFormation 템플릿에서 속성 을 구성합니다 . 도우미 스크립트를 사용하여 응용 프로그램을 설치하고 구성한 후 성공 신호를 보냅니다 cfn-init.,UpdatePolicyCloudFormation 템플릿에서 인스턴스에 대한 속성을 구성합니다 . 도우미 스크립트를 사용하여 응용 프로그램을 설치하고 구성한 후 성공 신호를 보냅니다 cfn-signal.,UpdateReplacePolicyCloudFormation 템플릿에서 속성 을 구성합니다 . 도우미 스크립트를 사용하여 응용 프로그램을 설치하고 구성한 후 성공 신호를 보냅니다 cfn-signal.,CreationPolicyCloudFormation 템플릿에서 인스턴스에 대한 속성을 구성합니다 . 도우미 스크립트를 사용하여 응용 프로그램을 설치하고 구성한 후 성공 신호를 보냅니다 cfn-signal.,,,0,,
udemy,CLF-01,154,"A company runs a multi-tier web application in the AWS Cloud. The application tier is hosted on Amazon EC2 instances and the backend database is hosted on an Amazon Aurora for MySQL DB cluster. For security compliance, all of the application variables such as DB hostnames, environment settings, product keys, and database passwords must be stored securely with encryption.Which of the following options is the most cost-effective solution to meet the requirements?",C,C,Store the values by creating secrets in AWS Secrets Manager. Use AWS Key Management Service (AWS KMS) for the encryption. Update the application to retrieve the value of the secrets.,"Store the values as key-value pairs in AWS Systems Manager OpsCenter. By default, the key-value pairs will be encrypted at rest. Configure the application to retrieve the variables when it starts.",Store the values by creating SecureString type parameters in AWS Systems Manager Parameter Store. Use AWS Key Management Service (AWS KMS) for the encryption. Update the application to retrieve the parameter values.,Store the values in a file saved in an Amazon S3 bucket. Enable encryption on the Amazon S3 bucket. Configure the application to download the file contents when it starts.,,,,"회사는 AWS 클라우드에서 다중 계층 웹 애플리케이션을 실행합니다. 애플리케이션 계층은 Amazon EC2 인스턴스에서 호스팅되고 백엔드 데이터베이스는 Amazon Aurora for MySQL DB 클러스터에서 호스팅됩니다. 보안 준수를 위해 DB 호스트 이름, 환경 설정, 제품 키, 데이터베이스 비밀번호와 같은 모든 애플리케이션 변수는 암호화되어 안전하게 저장되어야 합니다.다음 옵션 중 요구 사항을 충족하는 가장 비용 효율적인 솔루션은 무엇입니까?",AWS Secrets Manager에서 비밀을 생성하여 값을 저장합니다. 암호화에 AWS Key Management Service(AWS KMS)를 사용합니다. 비밀 값을 검색하도록 애플리케이션을 업데이트합니다.,값을 AWS Systems Manager OpsCenter에 키-값 페어로 저장합니다. 기본적으로 키-값 쌍은 유휴 상태에서 암호화됩니다. 시작할 때 변수를 검색하도록 응용 프로그램을 구성합니다.,AWS Systems Manager Parameter Store에서 SecureString 유형 파라미터를 생성하여 값을 저장합니다. 암호화에 AWS Key Management Service(AWS KMS)를 사용합니다. 매개변수 값을 검색하도록 애플리케이션을 업데이트하십시오.,Amazon S3 버킷에 저장된 파일에 값을 저장합니다. Amazon S3 버킷에서 암호화를 활성화합니다. 시작할 때 파일 콘텐츠를 다운로드하도록 애플리케이션을 구성합니다.,,,0,,
udemy,CLF-01,155,"A media company recently launched their newly created web application. Many users tried to visit the website, but they are receiving a 503 Service Unavailable Error. The system administrator tracked the EC2 instance status and saw the capacity is reaching its maximum limit and unable to process all the requests. To gain insights from the application's data, they need to launch a real-time analytics service.Which of the following allows you to read records in batches?",D,D,Create a Kinesis Data Firehose and use AWS Lambda to read records from the data stream.,Create an Amazon S3 bucket to store the captured data and use Amazon Redshift Spectrum to analyze the data.,Create an Amazon S3 bucket to store the captured data and use Amazon Athena to analyze the data.,Create a Kinesis Data Stream and use AWS Lambda to read records from the data stream.,,,,한 미디어 회사가 최근 새로 만든 웹 애플리케이션을 출시했습니다. 많은 사용자가 웹 사이트를 방문하려고 시도했지만 503 서비스를 사용할 수 없음 오류가 발생했습니다. 시스템 관리자는 EC2 인스턴스 상태를 추적했고 용량이 최대 한도에 도달하여 모든 요청을 처리할 수 없음을 확인했습니다. 애플리케이션 데이터에서 통찰력을 얻으려면 실시간 분석 서비스를 시작해야 합니다.다음 중 레코드를 일괄적으로 읽을 수 있는 것은 무엇입니까?,Kinesis Data Firehose를 생성하고 AWS Lambda를 사용하여 데이터 스트림에서 레코드를 읽습니다.,캡처된 데이터를 저장할 Amazon S3 버킷을 생성하고 Amazon Redshift Spectrum을 사용하여 데이터를 분석합니다.,캡처된 데이터를 저장할 Amazon S3 버킷을 생성하고 Amazon Athena를 사용하여 데이터를 분석합니다.,Kinesis Data Stream을 생성하고 AWS Lambda를 사용하여 데이터 스트림에서 레코드를 읽습니다.,,,0,,
udemy,CLF-01,156,"A company is using Amazon VPC that has a CIDR block of 10.31.0.0/27 that is connected to the on-premises data center. There was a requirement to create a Lambda function that will process massive amounts of cryptocurrency transactions every minute and then store the results to EFS. After setting up the serverless architecture and connecting the Lambda function to the VPC, the Solutions Architect noticed an increase in invocation errors with EC2 error types such as EC2ThrottledException at certain times of the day.Which of the following are the possible causes of this issue? (Select TWO.)",BE,BE,Your VPC does not have a NAT gateway.,You only specified one subnet in your Lambda function configuration. That single subnet runs out of available IP addresses and there is no other subnet or Availability Zone which can handle the peak load.,The associated security group of your function does not allow outbound connections.,The attached IAM execution role of your function does not have the necessary permissions to access the resources of your VPC.,Your VPC does not have sufficient subnet ENIs or subnet IPs.,,,회사는 10.31.0.0/27온프레미스 데이터 센터에 연결된 CIDR 블록이 있는 Amazon VPC를 사용하고 있습니다. 매분 엄청난 양의 암호화폐 트랜잭션을 처리한 다음 결과를 EFS에 저장하는 Lambda 함수를 생성해야 한다는 요구 사항이 있었습니다. 서버리스 아키텍처를 설정하고 Lambda 함수를 VPC에 연결한 후 Solutions Architect는 EC2ThrottledException하루 중 특정 시간과 같은 EC2 오류 유형의 호출 오류가 증가하는 것을 발견했습니다.다음 중 이 문제의 가능한 원인은 무엇입니까? (2개를 선택하세요.),VPC에 NAT 게이트웨이가 없습니다.,Lambda 함수 구성에서 하나의 서브넷만 지정했습니다. 해당 단일 서브넷은 사용 가능한 IP 주소가 부족하고 피크 로드를 처리할 수 있는 다른 서브넷이나 가용 영역이 없습니다.,함수의 연결된 보안 그룹이 아웃바운드 연결을 허용하지 않습니다.,함수의 연결된 IAM 실행 역할에 VPC 리소스에 액세스하는 데 필요한 권한이 없습니다.,VPC에 충분한 서브넷 ENI 또는 서브넷 IP가 없습니다.,,0,,
udemy,CLF-01,157,"A company deployed a high-performance computing (HPC) cluster that spans multiple EC2 instances across multiple Availability Zones and processes various wind simulation models. Currently, the Solutions Architect is experiencing a slowdown in their applications and upon further investigation, it was discovered that it was due to latency issues.Which is the MOST suitable solution that the Solutions Architect should implement to provide low-latency network performance necessary for tightly-coupled node-to-node communication of the HPC cluster?",D,D,Set up a spread placement group across multiple Availability Zones in multiple AWS Regions.,Use EC2 Dedicated Instances with elastic inference accelerator,Set up AWS Direct Connect connections across multiple Availability Zones for increased bandwidth throughput and more consistent network experience.,Set up a cluster placement group within a single Availability Zone in the same AWS Region.,,,,한 회사에서 여러 가용 영역에 걸쳐 여러 EC2 인스턴스에 걸쳐 있고 다양한 풍력 시뮬레이션 모델을 처리하는 HPC(고성능 컴퓨팅) 클러스터를 배포했습니다. 현재 Solutions Architect는 애플리케이션 속도 저하를 경험하고 있으며 추가 조사 결과 대기 시간 문제로 인한 것으로 밝혀졌습니다.HPC 클러스터의 긴밀하게 결합된 노드 간 통신에 필요한 지연 시간이 짧은 네트워크 성능을 제공하기 위해 Solutions Architect가 구현해야 하는 가장 적합한 솔루션은 무엇입니까?,여러 AWS 리전의 여러 가용 영역에 분산 배치 그룹을 설정합니다.,Elastic Inference Accelerator와 함께 EC2 전용 인스턴스 사용,증가된 대역폭 처리량과 보다 일관된 네트워크 경험을 위해 여러 가용 영역에서 AWS Direct Connect 연결을 설정합니다.,동일한 AWS 리전의 단일 가용 영역 내에서 클러스터 배치 그룹을 설정합니다.,,,0,,
udemy,CLF-01,158,"A company has a global online trading platform in which the users from all over the world regularly upload terabytes of transactional data to a centralized S3 bucket.What AWS feature should you use in your present system to improve throughput and ensure consistently fast data transfer to the Amazon S3 bucket, regardless of your user's location?",A,A,Amazon S3 Transfer Acceleration,AWS Direct Connect,FTP,Use CloudFront Origin Access Identity,,,,회사는 전 세계의 사용자가 정기적으로 테라바이트의 트랜잭션 데이터를 중앙 집중식 S3 버킷에 업로드하는 글로벌 온라인 거래 플랫폼을 보유하고 있습니다.사용자 위치에 관계없이 처리량을 개선하고 Amazon S3 버킷으로 일관되고 빠르게 데이터를 전송하려면 현재 시스템에서 어떤 AWS 기능을 사용해야 합니까?,Amazon S3 Transfer Acceleration,AWS 다이렉트 커넥트,FTP,CloudFront 오리진 액세스 ID 사용,,,0,,
udemy,CLF-01,159,"A solutions architect is writing an AWS Lambda function that will process encrypted documents from an Amazon FSx for NetApp ONTAP file system. The documents are protected by an AWS KMS customer key. After processing the documents, the Lambda function will store the results in an S3 bucket with an Amazon S3 Glacier Flexible Retrieval storage class. The solutions architect must ensure that the files can be decrypted by the Lambda function.Which action accomplishes the requirement?",B,B,Attach the kms:decrypt permission to the Lambda function’s resource policy. Add a statement to the AWS KMS key’s policy that grants the function’s execution role the kms:decrypt permission.,Attach the kms:decrypt permission to the Lambda function’s execution role. Add a statement to the AWS KMS key’s policy that grants the function’s execution role the kms:decrypt permission.,Attach the kms:decrypt permission to the Lambda function’s execution role. Add a statement to the AWS KMS key’s policy that grants the function’s ARN the kms:decrypt permission.,Attach the kms:decrypt permission to the Lambda function’s resource policy. Add a statement to the AWS KMS key’s policy that grants the function’s resource policy ARN the kms:decrypt permission.,,,,솔루션 설계자는 NetApp ONTAP 파일 시스템용 Amazon FSx에서 암호화된 문서를 처리할 AWS Lambda 함수를 작성하고 있습니다. 문서는 AWS KMS 고객 키로 보호됩니다. 문서를 처리한 후 Lambda 함수는 Amazon S3 Glacier Flexible Retrieval 스토리지 클래스가 있는 S3 버킷에 결과를 저장합니다. 솔루션 설계자는 파일이 Lambda 함수로 해독될 수 있는지 확인해야 합니다.어떤 작업이 요구 사항을 충족합니까?,kms:decryptLambda 함수의 리소스 정책에 권한 을 연결합니다 . 함수의 실행 역할에 권한을 부여하는 AWS KMS 키 정책에 명령문을 추가합니다 kms:decrypt.,kms:decryptLambda 함수의 실행 역할에 권한 을 연결합니다 . 함수의 실행 역할에 권한을 부여하는 AWS KMS 키 정책에 명령문을 추가합니다 kms:decrypt.,kms:decryptLambda 함수의 실행 역할에 권한 을 연결합니다 . 함수의 ARN에 권한을 부여하는 AWS KMS 키 정책에 명령문을 추가합니다 kms:decrypt.,kms:decryptLambda 함수의 리소스 정책에 권한 을 연결합니다 . 함수의 리소스 정책 ARN에 권한을 부여하는 AWS KMS 키의 정책에 명령문을 추가합니다 kms:decrypt.,,,0,,
udemy,CLF-01,160,"A Solutions Architect is unable to connect to the newly deployed EC2 instance via SSH using a home computer. However, the Architect was able to successfully access other existing instances in the VPC without any issues.Which of the following should the Architect check and possibly correct to restore connectivity?",D,D,Configure the Network Access Control List of your VPC to permit ingress traffic over port 22 from your IP.,Configure the Security Group of the EC2 instance to permit ingress traffic over port 3389 from your IP.,Use Amazon Data Lifecycle Manager.,Configure the Security Group of the EC2 instance to permit ingress traffic over port 22 from your IP.,,,,Solutions Architect는 가정용 컴퓨터를 사용하여 SSH를 통해 새로 배포된 EC2 인스턴스에 연결할 수 없습니다. 그러나 Architect는 아무 문제 없이 VPC의 다른 기존 인스턴스에 성공적으로 액세스할 수 있었습니다.다음 중 Architect가 연결을 복원하기 위해 확인하고 수정해야 하는 것은 무엇입니까?,IP에서 포트 22를 통한 수신 트래픽을 허용하도록 VPC의 네트워크 액세스 제어 목록을 구성합니다.,IP에서 포트 3389를 통한 수신 트래픽을 허용하도록 EC2 인스턴스의 보안 그룹을 구성합니다.,Amazon 데이터 수명 주기 관리자를 사용하십시오.,IP에서 포트 22를 통한 수신 트래픽을 허용하도록 EC2 인스턴스의 보안 그룹을 구성합니다.,,,0,,
udemy,CLF-01,161,A production MySQL database hosted on Amazon RDS is running out of disk storage. The management has consulted its solutions architect to increase the disk space without impacting the database performance.How can the solutions architect satisfy the requirement with the LEAST operational overhead?,D,D,Change the default_storage_engine of the DB instance’s parameter group to MyISAM.,Modify the DB instance storage type to Provisioned IOPS.,Increase the allocated storage for the DB instance.,Modify the DB instance settings and enable storage autoscaling.,,,,Amazon RDS에서 호스팅되는 프로덕션 MySQL 데이터베이스의 디스크 스토리지가 부족합니다. 경영진은 데이터베이스 성능에 영향을 주지 않으면서 디스크 공간을 늘리기 위해 솔루션 설계자와 상의했습니다.솔루션 설계자는 최소한의 운영 오버헤드로 요구 사항을 어떻게 충족할 수 있습니까?,default_storage_engineDB 인스턴스의 파라미터 그룹을 로 변경합니다 MyISAM.,DB 인스턴스 스토리지 유형을 프로비저닝된 IOPS로 수정합니다.,DB 인스턴스에 할당된 스토리지를 늘립니다.,DB 인스턴스 설정을 수정하고 스토리지 자동 확장을 활성화합니다.,,,0,,
udemy,CLF-01,162,An online stocks trading application that stores financial data in an S3 bucket has a lifecycle policy that moves older data to Glacier every month. There is a strict compliance requirement where a surprise audit can happen at anytime and you should be able to retrieve the required data in under 15 minutes under all circumstances. Your manager instructed you to ensure that retrieval capacity is available when you need it and should handle up to 150 MB/s of retrieval throughput.   Which of the following should you do to meet the above requirement? (Select TWO.),CD,CD,Retrieve the data using Amazon Glacier Select.,Use Bulk Retrieval to access the financial data.,Purchase provisioned retrieval capacity.,Use Expedited Retrieval to access the financial data.,"Specify a range, or portion, of the financial data archive to retrieve.",,,재무 데이터를 S3 버킷에 저장하는 온라인 주식 거래 애플리케이션에는 매달 오래된 데이터를 Glacier로 이동하는 수명 주기 정책이 있습니다. 언제든지 기습 감사가 발생할 수 있고 모든 상황에서 15분 이내에 필요한 데이터를 검색할 수 있어야 하는 엄격한 규정 준수 요구 사항이 있습니다. 관리자는 필요할 때 검색 용량을 사용할 수 있고 최대 150MB/s의 검색 처리량을 처리해야 한다고 지시했습니다.   위의 요구 사항을 충족하려면 다음 중 무엇을 해야 합니까? (2개를 선택하세요.),Amazon Glacier Select를 사용하여 데이터를 검색합니다.,대량 검색을 사용하여 재무 데이터에 액세스하십시오.,프로비저닝된 검색 용량을 구매합니다.,신속 검색을 사용하여 재무 데이터에 액세스하십시오.,검색할 재무 데이터 아카이브의 범위 또는 부분을 지정하십시오.,,0,,
udemy,CLF-01,163,A solutions architect is in charge of preparing the infrastructure for a serverless application. The application is built from a Docker image pulled from an Amazon Elastic Container Registry (ECR) repository. It is compulsory that the application has access to 5 GB of ephemeral storage.Which action satisfies the requirements?,D,D,Deploy the application in a Lambda function with Container image support. Attach an Amazon Elastic File System (EFS) volume to the function.,Deploy the application in a Lambda function with Container image support. Set the function’s storage to 5 GB.,Deploy the application Amazon ECS cluster with EC2 worker nodes and attach a 5 GB Amazon EBS volume.,Deploy the application to an Amazon ECS cluster that uses Fargate tasks.,,,,솔루션 설계자는 서버리스 애플리케이션을 위한 인프라 준비를 담당합니다. 애플리케이션은 Amazon Elastic Container Registry(ECR) 리포지토리에서 가져온 Docker 이미지에서 빌드됩니다. 애플리케이션이 5GB의 임시 스토리지에 액세스할 수 있어야 합니다.요구 사항을 충족하는 조치는 무엇입니까?,컨테이너 이미지를 지원하는 Lambda 함수에 애플리케이션을 배포합니다. 함수에 Amazon Elastic File System(EFS) 볼륨을 연결합니다.,컨테이너 이미지를 지원하는 Lambda 함수에 애플리케이션을 배포합니다. 함수의 스토리지를 5GB로 설정합니다.,EC2 작업자 노드가 있는 애플리케이션 Amazon ECS 클러스터를 배포하고 5GB Amazon EBS 볼륨을 연결합니다.,Fargate 작업을 사용하는 Amazon ECS 클러스터에 애플리케이션을 배포합니다.,,,0,,
udemy,CLF-01,164,"A large insurance company has an AWS account that contains three VPCs (DEV, UAT and PROD) in the same region. UAT is peered to both PROD and DEV using a VPC peering connection. All VPCs have non-overlapping CIDR blocks. The company wants to push minor code releases from Dev to Prod to speed up time to market. Which of the following options helps the company accomplish this?",B,B,"Do nothing. Since these two VPCs are already connected via UAT, they already have a connection to each other.",Create a new VPC peering connection between PROD and DEV with the appropriate routes.,Change the DEV and PROD VPCs to have overlapping CIDR blocks to be able to connect them.,Create a new entry to PROD in the DEV route table using the VPC peering connection as the target.,,,,"대규모 보험 회사에는 동일한 리전에 3개의 VPC(DEV, UAT 및 PROD)가 포함된 AWS 계정이 있습니다. UAT는 VPC 피어링 연결을 사용하여 PROD 및 DEV 모두에 피어링됩니다. 모든 VPC에는 겹치지 않는 CIDR 블록이 있습니다. 이 회사는 시장 출시 시간을 단축하기 위해 Dev에서 Prod로 마이너 코드 릴리스를 푸시하려고 합니다. 다음 중 회사가 이를 달성하는 데 도움이 되는 옵션은 무엇입니까?",아무것도하지 마세요. 이 두 VPC는 ​​이미 UAT를 통해 연결되어 있으므로 이미 서로 연결되어 있습니다.,적절한 경로를 사용하여 PROD와 DEV 간에 새 VPC 피어링 연결을 만듭니다.,겹치는 CIDR 블록을 갖도록 DEV 및 PROD VPC를 변경하여 연결할 수 있습니다.,VPC 피어링 연결을 대상으로 사용하여 DEV 라우팅 테이블에서 PROD에 대한 새 항목을 생성합니다.,,,0,,
udemy,CLF-01,165,"A Solutions Architect for a global news company is configuring a fleet of EC2 instances in a subnet that currently is in a VPC with an Internet gateway attached. All of these EC2 instances can be accessed from the Internet. The architect launches another subnet and deploys an EC2 instance in it, however, the architect is not able to access the EC2 instance from the Internet.What could be the possible reasons for this issue? (Select TWO.)",AE,AE,The Amazon EC2 instance does not have a public IP address associated with it.,The Amazon EC2 instance is not a member of the same Auto Scaling group.,The route table is not configured properly to send traffic from the EC2 instance to the Internet through the customer gateway (CGW).,The Amazon EC2 instance does not have an attached Elastic Fabric Adapter (EFA).,The route table is not configured properly to send traffic from the EC2 instance to the Internet through the Internet gateway.,,,글로벌 뉴스 회사의 Solutions Architect는 현재 인터넷 게이트웨이가 연결된 VPC에 있는 서브넷에서 EC2 인스턴스 플릿을 구성하고 있습니다. 이러한 모든 EC2 인스턴스는 인터넷에서 액세스할 수 있습니다. 설계자는 다른 서브넷을 시작하고 그 안에 EC2 인스턴스를 배포하지만 설계자는 인터넷에서 EC2 인스턴스에 액세스할 수 없습니다.이 문제의 가능한 원인은 무엇입니까? (2개를 선택하세요.),Amazon EC2 인스턴스에는 연결된 퍼블릭 IP 주소가 없습니다.,Amazon EC2 인스턴스는 동일한 Auto Scaling 그룹의 구성원이 아닙니다.,고객 게이트웨이(CGW)를 통해 EC2 인스턴스에서 인터넷으로 트래픽을 전송하도록 라우팅 테이블이 제대로 구성되지 않았습니다.,Amazon EC2 인스턴스에 EFA(Elastic Fabric Adapter)가 연결되어 있지 않습니다.,라우팅 테이블이 인터넷 게이트웨이를 통해 EC2 인스턴스에서 인터넷으로 트래픽을 전송하도록 제대로 구성되지 않았습니다.,,0,,
udemy,CLF-01,166,A major TV network has a web application running on eight Amazon T3 EC2 instances behind an application load balancer. The number of requests that the application processes are consistent and do not experience spikes. A Solutions Architect must configure an Auto Scaling group for the instances to ensure that the application is running at all times.Which of the following options can satisfy the given requirements?,B,B,Deploy four EC2 instances with Auto Scaling in one region and four in another region behind an Amazon Elastic Load Balancer.,Deploy four EC2 instances with Auto Scaling in one Availability Zone and four in another availability zone in the same region behind an Amazon Elastic Load Balancer.,Deploy eight EC2 instances with Auto Scaling in one Availability Zone behind an Amazon Elastic Load Balancer.,Deploy two EC2 instances with Auto Scaling in four regions behind an Amazon Elastic Load Balancer.,,,,주요 TV 네트워크에는 애플리케이션 로드 밸런서 뒤에 있는 8개의 Amazon T3 EC2 인스턴스에서 실행되는 웹 애플리케이션이 있습니다. 애플리케이션이 처리하는 요청 수는 일관되고 급증하지 않습니다. Solutions Architect는 애플리케이션이 항상 실행되도록 인스턴스에 대한 Auto Scaling 그룹을 구성해야 합니다.다음 옵션 중 주어진 요구 사항을 충족할 수 있는 옵션은 무엇입니까?,Amazon Elastic Load Balancer 뒤의 한 지역에 Auto Scaling을 사용하여 4개의 EC2 인스턴스를 배포하고 다른 지역에 4개를 배포합니다.,하나의 가용 영역에 Auto Scaling을 사용하여 4개의 EC2 인스턴스를 배포하고 Amazon Elastic Load Balancer 뒤의 동일한 지역에 있는 다른 가용 영역에 4개를 배포합니다.,Amazon Elastic Load Balancer 뒤에 있는 하나의 가용 영역에 Auto Scaling을 사용하여 8개의 EC2 인스턴스를 배포합니다 .,Amazon Elastic Load Balancer 뒤의 4개 리전에서 Auto Scaling을 사용하여 2개의 EC2 인스턴스를 배포합니다.,,,0,,
udemy,CLF-01,167,"A solutions architect is managing an application that runs on a Windows EC2 instance with an attached Amazon FSx for Windows File Server. To save cost, management has decided to stop the instance during off-hours and restart it only when needed. It has been observed that the application takes several minutes to become fully operational which impacts productivity.How can the solutions architect speed up the instance’s loading time without driving the cost up?",B,B,Migrate the application to a Linux-based EC2 instance.,Migrate the application to an EC2 instance with hibernation enabled.,Disable the Instance Metadata Service to reduce the things that need to be loaded at startup.,Enable the hibernation mode on the EC2 instance.,,,,솔루션 설계자는 Windows 파일 서버용 Amazon FSx가 연결된 Windows EC2 인스턴스에서 실행되는 애플리케이션을 관리하고 있습니다. 비용을 절약하기 위해 경영진은 근무 외 시간에 인스턴스를 중지하고 필요할 때만 다시 시작하기로 결정했습니다. 애플리케이션이 완전히 작동하는 데 몇 분이 걸리며 이는 생산성에 영향을 미치는 것으로 관찰되었습니다.솔루션 설계자가 비용을 높이지 않고 어떻게 인스턴스의 로딩 시간을 단축할 수 있습니까?,애플리케이션을 Linux 기반 EC2 인스턴스로 마이그레이션합니다.,최대 절전 모드가 활성화된 EC2 인스턴스로 애플리케이션을 마이그레이션합니다.,인스턴스 메타데이터 서비스를 비활성화하여 시작 시 로드해야 하는 항목을 줄입니다.,EC2 인스턴스에서 최대 절전 모드를 활성화합니다.,,,0,,
udemy,CLF-01,168,"A company plans to use a durable storage service to store on-premises database backups to the AWS cloud. To move their backup data, they need to use a service that can store and retrieve objects through standard file storage protocols for quick recovery.Which of the following options will meet this requirement?",C,C,Use the AWS Storage Gateway volume gateway to store the backup data and directly access it using Amazon S3 API actions.,Use Amazon EBS volumes to store all the backup data and attach it to an Amazon EC2 instance.,Use the AWS Storage Gateway file gateway to store all the backup data in Amazon S3.,Use AWS Snowball Edge to directly backup the data in Amazon S3 Glacier.,,,,회사는 내구성 있는 스토리지 서비스를 사용하여 온프레미스 데이터베이스 백업을 AWS 클라우드에 저장할 계획입니다. 백업 데이터를 이동하려면 빠른 복구를 위해 표준 파일 스토리지 프로토콜을 통해 개체를 저장하고 검색할 수 있는 서비스를 사용해야 합니다.다음 중 이 요구 사항을 충족하는 옵션은 무엇입니까?,AWS Storage Gateway 볼륨 게이트웨이를 사용하여 백업 데이터를 저장하고 Amazon S3 API 작업을 사용하여 직접 액세스하십시오.,Amazon EBS 볼륨을 사용하여 모든 백업 데이터를 저장하고 Amazon EC2 인스턴스에 연결합니다.,AWS Storage Gateway 파일 게이트웨이를 사용하여 모든 백업 데이터를 Amazon S3에 저장합니다.,AWS Snowball Edge를 사용하여 Amazon S3 Glacier의 데이터를 직접 백업합니다.,,,0,,
udemy,CLF-01,169,A company is using multiple AWS accounts that are consolidated using AWS Organizations. They want to copy several S3 objects to another S3 bucket that belonged to a different AWS account which they also own. The Solutions Architect was instructed to set up the necessary permissions for this task and to ensure that the destination account owns the copied objects and not the account it was sent from. How can the Architect accomplish this requirement?,B,B,Enable the Requester Pays feature in the source S3 bucket. The fees would be waived through Consolidated Billing since both AWS accounts are part of AWS Organizations.,Configure cross-account permissions in S3 by creating an IAM customer-managed policy that allows an IAM user or role to copy objects from the source bucket in one account to the destination bucket in the other account. Then attach the policy to the IAM user or role that you want to use to copy objects between accounts.,Set up cross-origin resource sharing (CORS) in S3 by creating a bucket policy that allows an IAM user or role to copy objects from the source bucket in one account to the destination bucket in the other account.,Connect the two S3 buckets from two different AWS accounts to Amazon WorkDocs. Set up cross-account access to integrate the two S3 buckets. Use the Amazon WorkDocs console to copy the objects from one account to the other with modified object ownership assigned to the destination account.,,,,회사에서 AWS Organizations를 사용하여 통합된 여러 AWS 계정을 사용하고 있습니다. 그들은 여러 S3 객체를 자신도 소유하고 있는 다른 AWS 계정에 속한 다른 S3 버킷에 복사하려고 합니다. Solutions Architect는 이 작업에 필요한 권한을 설정하고 대상 계정이 복사된 개체를 소유하고 있는 계정이 아닌지 확인하라는 지시를 받았습니다.Architect는 이 요구 사항을 어떻게 달성할 수 있습니까?,소스 S3 버킷에서 요청자 지불 기능을 활성화합니다. 두 AWS 계정이 모두 AWS Organizations에 속해 있으므로 통합 결제를 통해 수수료가 면제됩니다.,IAM 사용자 또는 역할이 한 계정의 소스 버킷에서 다른 계정의 대상 버킷으로 객체를 복사하도록 허용하는 IAM 고객 관리형 정책을 생성하여 S3에서 교차 계정 권한을 구성합니다. 그런 다음 계정 간에 객체를 복사하는 데 사용할 IAM 사용자 또는 역할에 정책을 연결합니다.,IAM 사용자 또는 역할이 한 계정의 소스 버킷에서 다른 계정의 대상 버킷으로 객체를 복사하도록 허용하는 버킷 정책을 생성하여 S3에서 CORS(교차 원본 리소스 공유)를 설정합니다.,서로 다른 두 AWS 계정의 S3 버킷 두 개를 Amazon WorkDocs에 연결합니다. 두 S3 버킷을 통합하기 위해 교차 계정 액세스를 설정합니다. Amazon WorkDocs 콘솔을 사용하여 한 계정에서 수정된 객체 소유권이 대상 계정에 할당된 다른 계정으로 객체를 복사합니다.,,,0,,
udemy,CLF-01,170,An online registration system hosted in an Amazon EKS cluster stores data to a db.t4g.medium Amazon Aurora DB cluster. The database performs well during regular hours but is unable to handle the traffic surge that occurs during flash sales. A solutions architect must move the database to Aurora Serverless while minimizing downtime and the impact on the operation of the application.Which change should be taken to meet the objective?,B,B,Change the Aurora Instance class to Serverless,Use AWS Database Migration Service (AWS DMS) to migrate to a new Aurora Serverless database.,Take a snapshot of the DB cluster. Use the snapshot to create a new Aurora DB cluster.,Add an Aurora Replica to the cluster and set its instance class to Serverless. Failover to the read replica and promote it to primary.,,,,Amazon EKS 클러스터에서 호스팅되는 온라인 등록 시스템은 데이터를 db.t4g.mediumAmazon Aurora DB 클러스터에 저장합니다. 데이터베이스는 정규 시간 동안 잘 작동하지만 플래시 판매 중에 발생하는 트래픽 급증을 처리할 수 없습니다. 솔루션 설계자는 가동 중지 시간과 애플리케이션 운영에 미치는 영향을 최소화하면서 데이터베이스를 Aurora Serverless로 이동해야 합니다.목표를 달성하기 위해 어떤 변화를 취해야 합니까?,Aurora 인스턴스 클래스를 서버리스로 변경,AWS Database Migration Service(AWS DMS)를 사용하여 새로운 Aurora 서버리스 데이터베이스로 마이그레이션하십시오.,DB 클러스터의 스냅샷을 생성합니다. 스냅샷을 사용하여 새 Aurora DB 클러스터를 생성합니다.,Aurora 복제본을 클러스터에 추가하고 해당 인스턴스 클래스를 서버리스로 설정합니다. 읽기 전용 복제본으로 장애 조치하고 기본 복제본으로 승격합니다.,,,0,,
udemy,CLF-01,171,"A company has established a dedicated network connection from its on-premises data center to AWS Cloud using AWS Direct Connect (DX). The core network services, such as the Domain Name System (DNS) service and Active Directory services, are all hosted on-premises. The company has new AWS accounts that will also require consistent and dedicated access to these network services.Which of the following can satisfy this requirement with the LEAST amount of operational overhead and in a cost-effective manner?",B,B,Set up a new Direct Connect gateway and integrate it with the existing Direct Connect connection. Configure a VPC peering connection between AWS accounts and associate it with Direct Connect gateway.,Create a new Direct Connect gateway and integrate it with the existing Direct Connect connection. Set up a Transit Gateway between AWS accounts and associate it with the Direct Connect gateway.,Create a new AWS VPN CloudHub. Set up a Virtual Private Network (VPN) connection for additional AWS accounts.,Set up another Direct Connect connection for each and every new AWS account that will be added.,,,,회사에서 AWS Direct Connect(DX)를 사용하여 온프레미스 데이터 센터에서 AWS 클라우드로의 전용 네트워크 연결을 설정했습니다. DNS(Domain Name System) 서비스 및 Active Directory 서비스와 같은 핵심 네트워크 서비스는 모두 온프레미스에서 호스팅됩니다. 회사에는 이러한 네트워크 서비스에 대한 일관되고 전용 액세스가 필요한 새로운 AWS 계정이 있습니다.다음 중 최소한의 운영 오버헤드와 비용 효율적인 방식으로 이 요구 사항을 충족할 수 있는 것은 무엇입니까?,새 Direct Connect 게이트웨이를 설정하고 기존 Direct Connect 연결과 통합하십시오. AWS 계정 간에 VPC 피어링 연결을 구성하고 이를 Direct Connect 게이트웨이와 연결합니다.,새 Direct Connect 게이트웨이를 생성하고 기존 Direct Connect 연결과 통합합니다. AWS 계정 간에 Transit Gateway를 설정하고 Direct Connect 게이트웨이와 연결합니다.,새 AWS VPN CloudHub를 생성합니다. 추가 AWS 계정에 대한 가상 사설망(VPN) 연결을 설정합니다.,추가할 모든 새 AWS 계정마다 다른 Direct Connect 연결을 설정하십시오.,,,0,,
udemy,CLF-01,172,A FinTech startup deployed an application on an Amazon EC2 instance with attached Instance Store volumes and an Elastic IP address. The server is only accessed from 8 AM to 6 PM and can be stopped from 6 PM to 8 AM for cost efficiency using Lambda with the script that automates this based on tags.Which of the following will occur when the EC2 instance is stopped and started? (Select TWO.),BE,BE,The ENI (Elastic Network Interface) is detached.,All data on the attached instance-store devices will be lost.,There will be no changes.,The Elastic IP address is disassociated with the instance.,The underlying host for the instance is possibly changed.,,,핀테크 스타트업은 인스턴스 스토어 볼륨과 탄력적 IP 주소가 연결된 Amazon EC2 인스턴스에 애플리케이션을 배포했습니다. 서버는 오전 8시부터 오후 6시까지만 액세스할 수 있으며 태그를 기반으로 이를 자동화하는 스크립트와 함께 Lambda를 사용하여 비용 효율성을 위해 오후 6시부터 오전 8시까지 서버를 중지할 수 있습니다.다음 중 EC2 인스턴스가 중지되었다가 시작될 때 발생하는 것은 무엇입니까? (2개를 선택하세요.),ENI(Elastic Network Interface)가 분리됩니다.,연결된 인스턴스 스토어 장치의 모든 데이터가 손실됩니다.,변경 사항이 없습니다.,탄력적 IP 주소가 인스턴스와 연결 해제됩니다.,인스턴스의 기본 호스트가 변경되었을 수 있습니다.,,0,,
udemy,CLF-01,173,"An on-premises server uses an SMB network file share to store application data. The application produces around 50 MB of data per day, but it only needs to access some of it for daily processes. To save on storage costs, the company plans to copy all the application data to AWS, however, they want to retain the ability to retrieve data with the same low-latency access as the local file share. The company does not have the capacity to develop the needed tool for this operation.Which AWS service should the company use?",A,A,AWS Storage Gateway,AWS Virtual Private Network (VPN),AWS Snowball Edge,Amazon FSx for Windows File Server,,,,온프레미스 서버는 SMB 네트워크 파일 공유를 사용하여 애플리케이션 데이터를 저장합니다. 이 애플리케이션은 하루에 약 50MB의 데이터를 생성하지만 일상적인 프로세스를 위해 그 중 일부에만 액세스하면 됩니다. 스토리지 비용을 절약하기 위해 회사는 모든 애플리케이션 데이터를 AWS로 복사할 계획이지만 로컬 파일 공유와 동일한 낮은 대기 시간 액세스로 데이터를 검색할 수 있는 기능을 유지하려고 합니다. 회사는 이 작업에 필요한 도구를 개발할 능력이 없습니다.회사는 어떤 AWS 서비스를 사용해야 합니까?,AWS 스토리지 게이트웨이,AWS 가상 사설망(VPN),AWS 스노우볼 에지,Windows 파일 서버용 Amazon FSx,,,0,,
udemy,CLF-01,174,A tech company is currently using Auto Scaling for their web application. A new AMI now needs to be used for launching a fleet of EC2 instances. Which of the following changes needs to be done?,C,C,Create a new target group.,Create a new target group and launch configuration.,Create a new launch configuration.,Do nothing. You can start directly launching EC2 instances in the Auto Scaling group with the same launch configuration.,,,,기술 회사는 현재 웹 애플리케이션에 Auto Scaling을 사용하고 있습니다. 이제 EC2 인스턴스 플릿을 시작하는 데 새 AMI를 사용해야 합니다. 다음 중 어떤 변경을 수행해야 합니까?,새 대상 그룹을 만듭니다.,새 대상 그룹을 생성하고 구성을 시작합니다.,새 시작 구성을 만듭니다.,아무것도하지 마세요. 동일한 시작 구성으로 Auto Scaling 그룹에서 EC2 인스턴스 시작을 직접 시작할 수 있습니다.,,,0,,
udemy,CLF-01,175,A Solutions Architect needs to deploy a mobile application that collects votes for a singing competition. Millions of users from around the world will submit votes using their mobile phones. These votes must be collected and stored in a highly scalable and highly available database which will be queried for real-time ranking. The database is expected to undergo frequent schema changes throughout the voting period.Which of the following combination of services should the architect use to meet this requirement?,C,C,Amazon Relational Database Service (RDS) and Amazon MQ,Amazon Aurora and Amazon Cognito,Amazon DynamoDB and AWS AppSync,Amazon DocumentDB (with MongoDB compatibility) and Amazon AppFlow,,,,Solutions Architect는 노래 대회에 대한 투표를 수집하는 모바일 애플리케이션을 배포해야 합니다. 전 세계 수백만 명의 사용자가 휴대폰을 사용하여 투표를 제출합니다. 이러한 투표는 실시간 순위를 위해 쿼리될 확장성과 가용성이 높은 데이터베이스에 수집 및 저장되어야 합니다. 데이터베이스는 투표 기간 내내 빈번한 스키마 변경을 겪을 것으로 예상됩니다.설계자가 이 요구 사항을 충족하기 위해 사용해야 하는 다음 서비스 조합은 무엇입니까?,Amazon 관계형 데이터베이스 서비스(RDS) 및 Amazon MQ,아마존 오로라와 아마존 코그니토,Amazon DynamoDB 및 AWS AppSync,Amazon DocumentDB(MongoDB와 호환) 및 Amazon AppFlow,,,0,,
udemy,CLF-01,176,A firm has a containerized application that runs on a self-managed Kubernetes cluster. The cluster writes data in an on-premises MongoDB database. A solutions architect is requested to move the service to AWS in order to minimize operational overhead. The firm prohibits any changes to the code.Which action meets these objectives?,A,A,Migrate the cluster to an Amazon Elastic Kubernetes Service (EKS) cluster and the database to an Amazon DocumentDB (with MongoDB compatibility) database.,Migrate the cluster to an Amazon Elastic Container Service (ECS) cluster using Amazon ECS Anywhere and the database to an Amazon Aurora Serverless database.,Migrate the cluster to an Amazon Elastic Container Service (ECS) cluster with the images stored in the Amazon Elastic Container Registry (Amazon ECR). Move the database to an Amazon Neptune database,Migrate the cluster to an Amazon Elastic Kubernetes Service (EKS) cluster using Amazon EKS Anywhere and the database to an Amazon DynamoDB table.,,,,회사에는 자체 관리형 Kubernetes 클러스터에서 실행되는 컨테이너화된 애플리케이션이 있습니다. 클러스터는 온프레미스 MongoDB 데이터베이스에 데이터를 씁니다. 솔루션 설계자는 운영 오버헤드를 최소화하기 위해 서비스를 AWS로 이전해야 합니다. 회사는 코드 변경을 금지합니다.어떤 조치가 이러한 목표를 충족합니까?,클러스터를 Amazon Elastic Kubernetes Service(EKS) 클러스터로 마이그레이션하고 데이터베이스를 Amazon DocumentDB(MongoDB와 호환) 데이터베이스로 마이그레이션합니다.,Amazon ECS Anywhere를 사용하여 클러스터를 Amazon Elastic Container Service(ECS) 클러스터로 마이그레이션하고 데이터베이스를 Amazon Aurora Serverless 데이터베이스로 마이그레이션합니다.,Amazon Elastic Container Registry(Amazon ECR)에 저장된 이미지를 사용하여 클러스터를 Amazon Elastic Container Service(ECS) 클러스터로 마이그레이션합니다. 데이터베이스를 Amazon Neptune 데이터베이스로 이동,Amazon EKS Anywhere를 사용하여 클러스터를 Amazon Elastic Kubernetes Service(EKS) 클러스터로 마이그레이션하고 데이터베이스를 Amazon DynamoDB 테이블로 마이그레이션합니다.,,,0,,
udemy,CLF-01,177,"A Data Analyst in a financial company is tasked to provide insights on stock market trends to the company's clients. The company uses AWS Glue extract, transform, and load (ETL) jobs in daily report generation, which involves fetching data from an Amazon S3 bucket. The analyst discovered that old data from previous runs were being reprocessed, causing the jobs to take longer to complete.Which solution would resolve the issue in the most operationally efficient way?",D,D,"Create a Lambda function that removes any data already processed. Then, use Amazon EventBridge to trigger this function whenever the ETL job's status switches to SUCCEEDED.",Parallelize the job by splitting the dataset into smaller partitions and processing them simultaneously using multiple EC2 instances.,Increase the size of the dataset used in the job to speed up the extraction and analysis process.,Enable job bookmark for the ETL job.,,,,"금융 회사의 데이터 분석가는 회사 고객에게 주식 시장 동향에 대한 통찰력을 제공해야 합니다. 이 회사는 Amazon S3 버킷에서 데이터를 가져오는 일과 관련된 일일 보고서 생성에 AWS Glue 추출, 변환 및 로드(ETL) 작업을 사용합니다. 분석가는 이전 실행의 이전 데이터가 재처리되어 작업을 완료하는 데 시간이 오래 걸린다는 사실을 발견했습니다.운영상 가장 효율적인 방식으로 문제를 해결하는 솔루션은 무엇입니까?",이미 처리된 모든 데이터를 제거하는 Lambda 함수를 생성합니다. 그런 다음 Amazon EventBridge를 사용하여 ETL 작업의 상태가 로 전환될 때마다 이 기능을 트리거합니다 SUCCEEDED.,데이터 세트를 더 작은 파티션으로 분할하고 여러 EC2 인스턴스를 사용하여 동시에 처리하여 작업을 병렬화합니다.,작업에 사용되는 데이터 세트의 크기를 늘려 추출 및 분석 프로세스 속도를 높입니다.,ETL 작업에 대한 작업 북마크를 활성화합니다.,,,0,,
udemy,CLF-01,178,A tech startup is launching an on-demand food delivery platform using Amazon ECS cluster with an AWS Fargate serverless compute engine and Amazon Aurora. It is expected that the database read queries will significantly increase in the coming weeks ahead. A Solutions Architect recently launched two Read Replicas to the database cluster to improve the platform's scalability. Which of the following is the MOST suitable configuration that the Architect should implement to load balance all of the incoming read requests equally to the two Read Replicas?,D,D,Create a new Network Load Balancer to evenly distribute the read queries to the Read Replicas of the Amazon Aurora database.,Use the built-in Cluster endpoint of the Amazon Aurora database.,Enable Amazon Aurora Parallel Query.,Use the built-in Reader endpoint of the Amazon Aurora database.,,,,한 기술 스타트업이 AWS Fargate 서버리스 컴퓨팅 엔진 및 Amazon Aurora와 함께 Amazon ECS 클러스터를 사용하여 주문형 음식 배달 플랫폼을 출시하고 있습니다. 앞으로 몇 주 동안 데이터베이스 읽기 쿼리가 크게 증가할 것으로 예상됩니다. Solutions Architect는 최근 플랫폼의 확장성을 개선하기 위해 데이터베이스 클러스터에 두 개의 읽기 전용 복제본을 출시했습니다.다음 중 수신되는 모든 읽기 요청을 2개의 읽기 전용 복제본에 균등하게 로드 밸런싱하기 위해 아키텍트가 구현해야 하는 가장 적합한 구성은 무엇입니까?,새 Network Load Balancer를 생성하여 Amazon Aurora 데이터베이스의 읽기 전용 복제본에 대한 읽기 쿼리를 고르게 분산합니다.,Amazon Aurora 데이터베이스의 기본 제공 클러스터 엔드포인트를 사용합니다.,Amazon Aurora 병렬 쿼리를 활성화합니다.,Amazon Aurora 데이터베이스의 기본 제공 리더 엔드포인트를 사용합니다.,,,0,,
udemy,CLF-01,179,"A company deployed several EC2 instances in a private subnet. The Solutions Architect needs to ensure the security of all EC2 instances. Upon checking the existing Inbound Rules of the Network ACL, she saw this configuration:If a computer with an IP address of 110.238.109.37 sends a request to the VPC, what will happen?",C,C,"Initially, it will be denied and then after a while, the connection will be allowed.",It will be denied.,It will be allowed.,"Initially, it will be allowed and then after a while, the connection will be denied.",,,,한 회사가 프라이빗 서브넷에 여러 EC2 인스턴스를 배포했습니다. Solutions Architect는 모든 EC2 인스턴스의 보안을 보장해야 합니다. 네트워크 ACL의 기존 인바운드 규칙을 확인한 후 그녀는 다음 구성을 확인했습니다.IP 주소가 110.238.109.37인 컴퓨터가 VPC에 요청을 보내면 어떻게 될까요?,처음에는 거부되고 잠시 후 연결이 허용됩니다.,거부됩니다.,허용됩니다.,처음에는 허용되고 잠시 후 연결이 거부됩니다.,,,0,,
udemy,CLF-01,180,"A company has clients all across the globe that access product files stored in several S3 buckets, which are behind each of their own CloudFront web distributions. They currently want to deliver their content to a specific client, and they need to make sure that only that client can access the data. Currently, all of their clients can access their S3 buckets directly using an S3 URL or through their CloudFront distribution. The Solutions Architect must serve the private content via CloudFront only, to secure the distribution of files.Which combination of actions should the Architect implement to meet the above requirements? (Select TWO.)",BD,BD,Enable the Origin Shield feature of the Amazon CloudFront distribution to protect the files from unauthorized access.,Require the users to access the private content by using special CloudFront signed URLs or signed cookies.,Create a custom CloudFront function to check and ensure that only their clients can access the files.,Restrict access to files in the origin by creating an origin access identity (OAI) and give it permission to read the files in the bucket.,Use S3 pre-signed URLs to ensure that only their client can access the files. Remove permission to use Amazon S3 URLs to read the files for anyone else.,,,회사에는 자체 CloudFront 웹 배포 뒤에 있는 여러 S3 버킷에 저장된 제품 파일에 액세스하는 전 세계 고객이 있습니다. 그들은 현재 자신의 콘텐츠를 특정 클라이언트에 전달하려고 하며 해당 클라이언트만 데이터에 액세스할 수 있도록 해야 합니다. 현재 모든 클라이언트는 S3 URL을 사용하거나 CloudFront 배포를 통해 S3 버킷에 직접 액세스할 수 있습니다. Solutions Architect는 파일 배포를 보호하기 위해 CloudFront를 통해서만 비공개 콘텐츠를 제공해야 합니다.Architect가 위의 요구 사항을 충족하기 위해 구현해야 하는 작업 조합은 무엇입니까? (2개를 선택하세요.),무단 액세스로부터 파일을 보호하려면 Amazon CloudFront 배포의 Origin Shield 기능을 활성화하십시오.,사용자가 특수 CloudFront 서명 URL 또는 서명 쿠키를 사용하여 비공개 콘텐츠에 액세스하도록 요구합니다.,사용자 지정 CloudFront 함수를 생성하여 클라이언트만 파일에 액세스할 수 있는지 확인합니다.,원본 액세스 ID(OAI)를 생성하여 원본의 파일에 대한 액세스를 제한하고 버킷의 파일을 읽을 수 있는 권한을 부여합니다.,S3 사전 서명된 URL을 사용하여 클라이언트만 파일에 액세스할 수 있도록 합니다. Amazon S3 URL을 사용하여 다른 사람을 위해 파일을 읽을 수 있는 권한을 제거합니다.,,0,,
udemy,CLF-01,181,A company needs to use Amazon Aurora as the Amazon RDS database engine of their web application. The Solutions Architect has been instructed to implement a 90-day backup retention policy.Which of the following options can satisfy the given requirement?,D,D,Configure an automated backup and set the backup retention period to 90 days.,Create a daily scheduled event using CloudWatch Events and AWS Lambda to directly download the RDS automated snapshot to an S3 bucket. Archive snapshots older than 90 days to Glacier.,Configure RDS to export the automated snapshot automatically to Amazon S3 and create a lifecycle policy to delete the object after 90 days.,Create an AWS Backup plan to take daily snapshots with a retention period of 90 days.,,,,회사는 Amazon Aurora를 웹 애플리케이션의 Amazon RDS 데이터베이스 엔진으로 사용해야 합니다. Solutions Architect는 90일 백업 보존 정책을 구현하도록 지시 받았습니다.다음 옵션 중 주어진 요구 사항을 충족할 수 있는 옵션은 무엇입니까?,자동 백업을 구성하고 백업 보존 기간을 90일로 설정합니다.,CloudWatch Events 및 AWS Lambda를 사용하여 일일 예약 이벤트를 생성하여 RDS 자동 스냅샷을 S3 버킷으로 직접 다운로드합니다. 90일보다 오래된 스냅샷을 Glacier에 아카이브합니다.,자동 스냅샷을 Amazon S3로 자동으로 내보내도록 RDS를 구성하고 90일 후에 객체를 삭제하는 수명 주기 정책을 생성합니다.,보존 기간이 90일인 일일 스냅샷을 생성하는 AWS Backup 계획을 생성합니다.,,,0,,
udemy,CLF-01,182,"A company intends to give each of its developers a personal AWS account through AWS Organizations. To enforce regulatory policies, preconfigured AWS Config rules will be set in the new accounts. A solutions architect must see to it that developers are unable to remove or modify any rules in AWS Config.Which solution meets the objective with the least operational overhead?",C,C,Use an IAM Role in the new accounts with an attached IAM trust relationship to disable the access of the root user to AWS Config.,Configure an AWS Config rule in the root account to detect if changes to the new account’s Config rules are made.,Add the developers' AWS account to an organization unit (OU). Attach a service control policy (SCP) to the OU that restricts access to AWS Config.,Set up an AWS Control Tower in the root account to detect if there were any changes to the new account’s AWS Config rules. Attach an IAM trust relationship to the IAM User of each developer which prevents any changes in AWS Config.,,,,회사는 AWS Organizations를 통해 각 개발자에게 개인 AWS 계정을 제공하려고 합니다. 규제 정책을 시행하기 위해 사전 구성된 AWS Config 규칙이 새 계정에 설정됩니다. 솔루션 설계자는 개발자가 AWS Config에서 어떤 규칙도 제거하거나 수정할 수 없도록 해야 합니다.최소한의 운영 오버헤드로 목표를 충족하는 솔루션은 무엇입니까?,연결된 IAM 신뢰 관계가 있는 새 계정에서 IAM 역할을 사용하여 AWS Config에 대한 루트 사용자의 액세스를 비활성화합니다.,루트 계정에서 AWS Config 규칙을 구성하여 새 계정의 Config 규칙이 변경되었는지 감지합니다.,조직 단위(OU)에 개발자의 AWS 계정을 추가합니다. AWS Config에 대한 액세스를 제한하는 서비스 제어 정책(SCP)을 OU에 연결합니다.,루트 계정에서 AWS Control Tower를 설정하여 새 계정의 AWS Config 규칙에 대한 변경 사항이 있는지 감지합니다. AWS Config의 변경을 방지하는 각 개발자의 IAM 사용자에 IAM 신뢰 관계를 연결합니다.,,,0,,
udemy,CLF-01,183,An application needs to retrieve a subset of data from a large CSV file stored in an Amazon S3 bucket by using simple SQL expressions. The queries are made within Amazon S3 and must only return the needed data. Which of the following actions should be taken?,D,D,Perform an S3 Select operation based on the bucket's name and object's metadata.,Perform an S3 Select operation based on the bucket's name and object tags.,Perform an S3 Select operation based on the bucket’s name.,Perform an S3 Select operation based on the bucket's name and object's key.,,,,애플리케이션은 간단한 SQL 표현식을 사용하여 Amazon S3 버킷에 저장된 대용량 CSV 파일에서 데이터 하위 집합을 검색해야 합니다. 쿼리는 Amazon S3 내에서 이루어지며 필요한 데이터만 반환해야 합니다. 다음 중 어떤 조치를 취해야 합니까?,버킷의 이름과 객체의 메타데이터를 기반으로 S3 Select 작업을 수행합니다.,버킷의 이름과 객체 태그를 기반으로 S3 Select 작업을 수행합니다.,버킷 이름을 기반으로 S3 Select 작업을 수행합니다.,버킷의 이름과 객체의 키를 기반으로 S3 Select 작업을 수행합니다.,,,0,,
udemy,CLF-01,184,"A travel company has a suite of web applications hosted in an Auto Scaling group of On-Demand EC2 instances behind an Application Load Balancer that handles traffic from various web domains such as i-love-manila.com, i-love-boracay.com, i-love-cebu.com and many others. To improve security and lessen the overall cost, you are instructed to secure the system by allowing multiple domains to serve SSL traffic without the need to reauthenticate and reprovision your certificate everytime you add a new domain. This migration from HTTP to HTTPS will help improve their SEO and Google search ranking. Which of the following is the most cost-effective solution to meet the above requirement?",A,A,Upload all SSL certificates of the domains in the ALB using the console and bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client using Server Name Indication (SNI).,Create a new CloudFront web distribution and configure it to serve HTTPS requests using dedicated IP addresses in order to associate your alternate domain names with a dedicated IP address in each CloudFront edge location.,Add a Subject Alternative Name (SAN) for each additional domain to your certificate.,Use a wildcard certificate to handle multiple sub-domains and different domains.,,,,"여행사에는 , 및 기타 여러 웹 도메인의 트래픽을 처리하는 Application Load Balancer 뒤에 있는 온디맨드 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅되는 웹 애플리케이션 제품군 i-love-manila.com이 있습니다 i-love-boracay.com. i-love-cebu.com보안을 강화하고 전체 비용을 줄이기 위해 새 도메인을 추가할 때마다 인증서를 재인증 및 재프로비저닝할 필요 없이 여러 도메인이 SSL 트래픽을 제공하도록 허용하여 시스템을 보호하라는 지시를 받습니다. HTTP에서 HTTPS로의 마이그레이션은 SEO 및 Google 검색 순위를 높이는 데 도움이 됩니다.다음 중 위의 요구 사항을 충족하는 가장 비용 효율적인 솔루션은 무엇입니까?",콘솔을 사용하여 ALB에 있는 도메인의 모든 SSL 인증서를 업로드하고 여러 인증서를 로드 밸런서의 동일한 보안 리스너에 바인딩합니다. ALB는 SNI(서버 이름 표시)를 사용하여 각 클라이언트에 대해 최적의 TLS 인증서를 자동으로 선택합니다.,새 CloudFront 웹 배포를 생성하고 대체 도메인 이름을 각 CloudFront 엣지 로케이션의 전용 IP 주소와 연결하기 위해 전용 IP 주소를 사용하여 HTTPS 요청을 제공하도록 구성합니다.,각 추가 도메인에 대한 SAN(주체 대체 이름)을 인증서에 추가하십시오.,와일드카드 인증서를 사용하여 여러 하위 도메인과 다른 도메인을 처리합니다.,,,0,,
udemy,CLF-01,185,"A Solutions Architect is working for a large insurance firm. To maintain compliance with HIPAA laws, all data that is backed up or stored on Amazon S3 needs to be encrypted at rest.Which encryption methods can be employed, assuming S3 is being used for storing financial-related data? (Select TWO.)",AB,AB,Enable SSE on an S3 bucket to make use of AES-256 encryption,Encrypt the data using your own encryption keys then copy the data to Amazon S3 over HTTPS endpoints.,Store the data on EBS volumes with encryption enabled instead of using Amazon S3,Store the data in encrypted EBS snapshots,Use AWS Shield to protect your data at rest,,,A Solutions Architect는 대형 보험 회사에서 일하고 있습니다. HIPAA 법률을 준수하려면 Amazon S3에 백업 또는 저장되는 모든 데이터를 유휴 상태에서 암호화해야 합니다.S3가 금융 관련 데이터를 저장하는 데 사용된다고 가정할 때 어떤 암호화 방법을 사용할 수 있습니까? (2개를 선택하세요.),AES-256 암호화를 사용하려면 S3 버킷에서 SSE를 활성화하십시오.,자체 암호화 키를 사용하여 데이터를 암호화한 다음 HTTPS 엔드포인트를 통해 Amazon S3에 데이터를 복사합니다.,Amazon S3를 사용하는 대신 암호화가 활성화된 EBS 볼륨에 데이터 저장,암호화된 EBS 스냅샷에 데이터 저장,AWS Shield를 사용하여 저장된 데이터 보호,,0,,
udemy,CLF-01,186,"A company has an On-Demand EC2 instance located in a subnet in AWS that hosts a web application. The security group attached to this EC2 instance has the following Inbound Rules:The Route table attached to the VPC is shown below. You can establish an SSH connection into the EC2 instance from the Internet. However, you are not able to connect to the web server using your Chrome browser.Which of the below steps would resolve the issue?",A,A,"In the Security Group, add an Inbound HTTP rule.","In the Route table, add this new route entry: 10.0.0.0/27 -> local","In the Route table, add this new route entry: 0.0.0.0 -> igw-b51618cc","In the Security Group, remove the SSH rule.",,,,회사에는 웹 애플리케이션을 호스팅하는 AWS의 서브넷에 있는 온디맨드 EC2 인스턴스가 있습니다. 이 EC2 인스턴스에 연결된 보안 그룹에는 다음과 같은 인바운드 규칙이 있습니다.VPC에 연결된 라우팅 테이블은 아래와 같습니다. 인터넷에서 EC2 인스턴스로 SSH 연결을 설정할 수 있습니다. 그러나 Chrome 브라우저를 사용하여 웹 서버에 연결할 수 없습니다.다음 중 어떤 단계로 문제를 해결할 수 있습니까?,보안 그룹에서 인바운드 HTTP 규칙을 추가합니다.,경로 테이블에서 다음 새 경로 항목을 추가합니다. 10.0.0.0/27 -> local,경로 테이블에서 다음 새 경로 항목을 추가합니다. 0.0.0.0 -> igw-b51618cc,보안 그룹에서 SSH 규칙을 제거합니다.,,,0,,
udemy,CLF-01,187,A Solutions Architect is managing a company's AWS account of approximately 300 IAM users. They have a new company policy that requires changing the associated permissions of all 100 IAM users that control the access to Amazon S3 buckets.What will the Solutions Architect do to avoid the time-consuming task of applying the policy to each user?,D,D,Create a new policy and apply it to multiple IAM users using a shell script.,Create a new S3 bucket access policy with unlimited access for each IAM user.,Create a new IAM role and add each user to the IAM role.,"Create a new IAM group and then add the users that require access to the S3 bucket. Afterward, apply the policy to the IAM group.",,,,Solutions Architect는 약 300명의 IAM 사용자로 구성된 회사의 AWS 계정을 관리하고 있습니다. Amazon S3 버킷에 대한 액세스를 제어하는 ​​IAM 사용자 100명 모두의 관련 권한을 변경해야 하는 새로운 회사 정책이 있습니다.솔루션 설계자는 각 사용자에게 정책을 적용하는 시간 소모적인 작업을 피하기 위해 무엇을 합니까?,새 정책을 생성하고 셸 스크립트를 사용하여 여러 IAM 사용자에게 적용합니다.,각 IAM 사용자에 대한 무제한 액세스 권한이 있는 새 S3 버킷 액세스 정책을 생성합니다.,새 IAM 역할을 생성하고 각 사용자를 IAM 역할에 추가합니다.,새 IAM 그룹을 생성한 다음 S3 버킷에 액세스해야 하는 사용자를 추가합니다. 그런 다음 정책을 IAM 그룹에 적용합니다.,,,0,,
udemy,CLF-01,188,"A company has an e-commerce application that saves the transaction logs to an S3 bucket. You are instructed by the CTO to configure the application to keep the transaction logs for one month for troubleshooting purposes, and then afterward, purge the logs.What should you do to accomplish this requirement?",A,A,Configure the lifecycle configuration rules on the Amazon S3 bucket to purge the transaction logs after a month,Add a new bucket policy on the Amazon S3 bucket.,Create a new IAM policy for the Amazon S3 bucket that automatically deletes the logs after a month,Enable CORS on the Amazon S3 bucket which will enable the automatic monthly deletion of data,,,,회사에는 트랜잭션 로그를 S3 버킷에 저장하는 전자 상거래 애플리케이션이 있습니다. CTO는 문제 해결을 위해 한 달 동안 트랜잭션 로그를 보관한 다음 나중에 로그를 제거하도록 애플리케이션을 구성하라는 지시를 받았습니다.이 요구 사항을 달성하려면 어떻게 해야 합니까?,한 달 후에 트랜잭션 로그를 제거하도록 Amazon S3 버킷에서 수명 주기 구성 규칙을 구성합니다.,Amazon S3 버킷에 새 버킷 정책을 추가합니다.,한 달 후 자동으로 로그를 삭제하는 Amazon S3 버킷에 대한 새 IAM 정책 생성,데이터의 월별 자동 삭제를 활성화하는 Amazon S3 버킷에서 CORS를 활성화합니다.,,,0,,
udemy,CLF-01,189,"A company is storing its financial reports and regulatory documents in an Amazon S3 bucket. To comply with the IT audit, they tasked their Solutions Architect to track all new objects added to the bucket as well as the removed ones. It should also track whether a versioned object is permanently deleted. The Architect must configure Amazon S3 to publish notifications for these events to a queue for post-processing and to an Amazon SNS topic that will notify the Operations team. Which of the following is the MOST suitable solution that the Architect should implement?",C,C,Create a new Amazon SNS topic and Amazon SQS queue. Add an S3 event notification configuration on the bucket to publish s3:ObjectCreated:* and ObjectRemoved:DeleteMarkerCreated event types to SQS and SNS.,Create a new Amazon SNS topic and Amazon MQ. Add an S3 event notification configuration on the bucket to publish s3:ObjectAdded:* and s3:ObjectRemoved:* event types to SQS and SNS.,Create a new Amazon SNS topic and Amazon SQS queue. Add an S3 event notification configuration on the bucket to publish s3:ObjectCreated:* and s3:ObjectRemoved:Delete event types to SQS and SNS.,Create a new Amazon SNS topic and Amazon MQ. Add an S3 event notification configuration on the bucket to publish s3:ObjectCreated:* and ObjectRemoved:DeleteMarkerCreated event types to SQS and SNS.,,,,회사는 재무 보고서와 규제 문서를 Amazon S3 버킷에 저장하고 있습니다. IT 감사를 준수하기 위해 솔루션 아키텍트에게 버킷에 추가된 모든 새 객체와 제거된 객체를 추적하도록 지시했습니다. 버전이 지정된 개체가 영구적으로 삭제되었는지 여부도 추적해야 합니다. 설계자는 이러한 이벤트에 대한 알림을 사후 처리를 위한 대기열과 운영 팀에 알릴 Amazon SNS 주제에 게시하도록 Amazon S3를 구성해야 합니다.다음 중 Architect가 구현해야 하는 가장 적합한 솔루션은 무엇입니까?,새 Amazon SNS 주제와 Amazon SQS 대기열을 생성합니다. 게시할 버킷에 S3 이벤트 알림 구성을 추가하고 SQS 및 SNS에 이벤트 유형을 s3:ObjectCreated:*추가 합니다.ObjectRemoved:DeleteMarkerCreated,새 Amazon SNS 주제와 Amazon MQ를 생성합니다. 게시할 버킷에 S3 이벤트 알림 구성을 추가하고 SQS 및 SNS에 이벤트 유형을 s3:ObjectAdded:*추가 합니다.s3:ObjectRemoved:*,새 Amazon SNS 주제와 Amazon SQS 대기열을 생성합니다. 게시할 버킷에 S3 이벤트 알림 구성을 추가하고 SQS 및 SNS에 이벤트 유형을 s3:ObjectCreated:*추가 합니다.s3:ObjectRemoved:Delete,새 Amazon SNS 주제와 Amazon MQ를 생성합니다. 게시할 버킷에 S3 이벤트 알림 구성을 추가하고 SQS 및 SNS에 이벤트 유형을 s3:ObjectCreated:*추가 합니다.ObjectRemoved:DeleteMarkerCreated,,,0,,
udemy,CLF-01,190,"An investment bank is working with an IT team to handle the launch of the new digital wallet system. The applications will run on multiple EBS-backed EC2 instances which will store the logs, transactions, and billing statements of the user in an S3 bucket. Due to tight security and compliance requirements, the IT team is exploring options on how to safely store sensitive data on the EBS volumes and S3.Which of the below options should be carried out when storing sensitive data on AWS? (Select TWO.)",CE,CE,Create an EBS Snapshot,Use AWS Shield and WAF,Enable EBS Encryption,Migrate the EC2 instances from the public to private subnet.,Enable Amazon S3 Server-Side or use Client-Side Encryption,,,"한 투자 은행이 새로운 디지털 지갑 시스템 출시를 처리하기 위해 IT 팀과 협력하고 있습니다. 애플리케이션은 사용자의 로그, 트랜잭션 및 청구서를 S3 버킷에 저장하는 여러 EBS 지원 EC2 인스턴스에서 실행됩니다. 엄격한 보안 및 규정 준수 요구 사항으로 인해 IT 팀은 민감한 데이터를 EBS 볼륨 및 S3에 안전하게 저장하는 방법에 대한 옵션을 모색하고 있습니다.민감한 데이터를 AWS에 저장할 때 다음 중 어떤 옵션을 수행해야 합니까? (2개를 선택하세요.)",EBS 스냅샷 생성,AWS Shield 및 WAF 사용,EBS 암호화 활성화,퍼블릭 서브넷에서 프라이빗 서브넷으로 EC2 인스턴스를 마이그레이션합니다.,Amazon S3 서버 측 활성화 또는 클라이언트 측 암호화 사용,,0,,
udemy,CLF-01,191,"Due to the large volume of query requests, the database performance of an online reporting application significantly slowed down. The Solutions Architect is trying to convince her client to use Amazon RDS Read Replica for their application instead of setting up a Multi-AZ Deployments configuration. What are two benefits of using Read Replicas over Multi-AZ that the Architect should point out? (Select TWO.)",AE,AE,It elastically scales out beyond the capacity constraints of a single DB instance for read-heavy database workloads.,Provides synchronous replication and automatic failover in the case of Availability Zone service failures.,Allows both read and write operations on the read replica to complement the primary database.,It enhances the read performance of your primary database by increasing its IOPS and accelerates its query processing via AWS Global Accelerator.,Provides asynchronous replication and improves the performance of the primary database by taking read-heavy database workloads from it.,,,대량의 쿼리 요청으로 인해 온라인 보고 애플리케이션의 데이터베이스 성능이 크게 저하되었습니다. Solutions Architect는 고객이 다중 AZ 배포 구성을 설정하는 대신 애플리케이션에 Amazon RDS 읽기 전용 복제본을 사용하도록 설득하려고 합니다.아키텍트가 지적해야 하는 다중 AZ에 대한 읽기 전용 복제본 사용의 두 가지 이점은 무엇입니까? (2개를 선택하세요.),읽기 작업이 많은 데이터베이스 워크로드에 대해 단일 DB 인스턴스의 용량 제약을 넘어 탄력적으로 확장됩니다.,가용 영역 서비스 장애 시 동기식 복제 및 자동 장애 조치를 제공합니다.,기본 데이터베이스를 보완하기 위해 읽기 복제본에 대한 읽기 및 쓰기 작업을 모두 허용합니다.,IOPS를 높여 기본 데이터베이스의 읽기 성능을 향상시키고 AWS Global Accelerator를 통해 쿼리 처리를 가속화합니다.,비동기식 복제를 제공하고 기본 데이터베이스에서 읽기가 많은 데이터베이스 워크로드를 가져옴으로써 기본 데이터베이스의 성능을 향상시킵니다.,,0,,
udemy,CLF-01,192,"You are automating the creation of EC2 instances in your VPC. Hence, you wrote a python script to trigger the Amazon EC2 API to request 50 EC2 instances in a single Availability Zone. However, you noticed that after 20 successful requests, subsequent requests failed. What could be a reason for this issue and how would you resolve it?",A,A,There is a vCPU-based On-Demand Instance limit per region which is why subsequent requests failed. Just submit the limit increase form to AWS and retry the failed requests once approved.,"By default, AWS allows you to provision a maximum of 20 instances per region. Select a different region and retry the failed request.","By default, AWS allows you to provision a maximum of 20 instances per Availability Zone. Select a different Availability Zone and retry the failed request.",There was an issue with the Amazon EC2 API. Just resend the requests and these will be provisioned successfully.,,,,VPC에서 EC2 인스턴스 생성을 자동화하고 있습니다. 따라서 Amazon EC2 API를 트리거하여 단일 가용 영역에서 50개의 EC2 인스턴스를 요청하는 Python 스크립트를 작성했습니다. 그러나 20개의 성공적인 요청 후에 후속 요청이 실패했음을 알게 되었습니다. 이 문제의 원인은 무엇이며 어떻게 해결할 수 있습니까?,"리전당 vCPU 기반 온디맨드 인스턴스 제한이 있으며, 이것이 후속 요청이 실패한 이유입니다. 한도 증가 양식을 AWS에 제출하고 승인되면 실패한 요청을 다시 시도하십시오.",기본적으로 AWS에서는 리전당 최대 20개의 인스턴스를 프로비저닝할 수 있습니다. 다른 지역을 선택하고 실패한 요청을 다시 시도하십시오.,기본적으로 AWS에서는 가용 영역당 최대 20개의 인스턴스를 프로비저닝할 수 있습니다. 다른 가용 영역을 선택하고 실패한 요청을 재시도하십시오.,Amazon EC2 API에 문제가 있습니다. 요청을 다시 보내면 성공적으로 프로비저닝됩니다.,,,0,,
udemy,CLF-01,193,"A large financial firm in the country has an AWS environment that contains several Reserved EC2 instances hosting a web application that has been decommissioned last week. To save costs, you need to stop incurring charges for the Reserved instances as soon as possible.What cost-effective steps will you take in this circumstance? (Select TWO.)",AC,AC,Terminate the Reserved instances as soon as possible to avoid getting billed at the on-demand price when it expires.,Go to the Amazon.com online shopping website and sell the Reserved instances.,Go to the AWS Reserved Instance Marketplace and sell the Reserved instances.,Stop the Reserved instances as soon as possible.,Contact AWS to cancel your AWS subscription.,,,이 나라의 한 대형 금융 회사는 지난주에 사용 중지된 웹 애플리케이션을 호스팅하는 여러 Reserved EC2 인스턴스가 포함된 AWS 환경을 보유하고 있습니다. 비용을 절약하려면 가능한 한 빨리 예약 인스턴스에 대한 요금 발생을 중지해야 합니다.이 상황에서 어떤 비용 효율적인 조치를 취하시겠습니까? (2개를 선택하세요.),예약 인스턴스가 만료될 때 온디맨드 가격으로 청구되지 않도록 가능한 한 빨리 예약 인스턴스를 종료하십시오.,Amazon.com 온라인 쇼핑 웹사이트로 이동하여 예약 인스턴스를 판매합니다.,AWS 예약 인스턴스 마켓플레이스로 이동하여 예약 인스턴스를 판매합니다.,가능한 한 빨리 예약 인스턴스를 중지하십시오.,AWS 구독을 취소하려면 AWS에 문의하십시오.,,0,,
udemy,CLF-01,194,"A company installed sensors to track the number of people who visit the park. The data is sent every day to an Amazon Kinesis stream with default settings for processing, in which a consumer is configured to process the data every other day. You noticed that the S3 bucket is not receiving all of the data that is being sent to the Kinesis stream. You checked the sensors if they are properly sending the data to Amazon Kinesis and verified that the data is indeed sent every day.What could be the reason for this?",B,B,Your AWS account was hacked and someone has deleted some data in your Kinesis stream.,"By default, the data records are only accessible for 24 hours from the time they are added to a Kinesis stream.","There is a problem in the sensors. They probably had some intermittent connection hence, the data is not sent to the stream.","By default, Amazon S3 stores the data for 1 day and moves it to Amazon Glacier.",,,,"한 회사는 공원을 방문하는 사람들의 수를 추적하기 위해 센서를 설치했습니다. 데이터는 처리를 위한 기본 설정이 있는 Amazon Kinesis 스트림으로 매일 전송되며, 소비자는 격일로 데이터를 처리하도록 구성됩니다. S3 버킷이 Kinesis 스트림으로 전송되는 모든 데이터를 수신하지 못하는 것을 확인했습니다. 센서가 Amazon Kinesis로 데이터를 제대로 전송하는지 확인하고 데이터가 실제로 매일 전송되는지 확인했습니다.그 이유는 무엇입니까?",AWS 계정이 해킹되어 누군가 Kinesis 스트림에서 일부 데이터를 삭제했습니다.,기본적으로 데이터 레코드는 Kinesis 스트림에 추가된 후 24시간 동안만 액세스할 수 있습니다.,센서에 문제가 있습니다. 간헐적으로 연결되었을 수 있으므로 데이터가 스트림으로 전송되지 않습니다.,기본적으로 Amazon S3는 1일 동안 데이터를 저장하고 Amazon Glacier로 옮깁니다.,,,0,,
udemy,CLF-01,195,"A company is generating confidential data that is saved on their on-premises data center. As a backup solution, the company wants to upload their data to an Amazon S3 bucket. In compliance with its internal security mandate, the encryption of the data must be done before sending it to Amazon S3. The company must spend time managing and rotating the encryption keys as well as controlling who can access those keys.Which of the following methods can achieve this requirement? (Select TWO.)",CD,CD,Set up Server-Side Encryption with keys stored in a separate S3 bucket.,Set up Client-Side Encryption with Amazon S3 managed encryption keys.,Set up Client-Side Encryption with a customer master key stored in AWS Key Management Service (AWS KMS).,Set up Client-Side Encryption using a client-side master key.,Set up Server-Side Encryption (SSE) with EC2 key pair.,,,회사는 온프레미스 데이터 센터에 저장된 기밀 데이터를 생성하고 있습니다. 백업 솔루션으로 회사는 데이터를 Amazon S3 버킷에 업로드하려고 합니다. 내부 보안 의무에 따라 Amazon S3로 데이터를 보내기 전에 데이터 암호화를 완료해야 합니다. 회사는 암호화 키를 관리 및 교체하고 해당 키에 액세스할 수 있는 사람을 제어하는 ​​데 시간을 투자해야 합니다.다음 중 이 요구 사항을 충족할 수 있는 방법은 무엇입니까? (2개를 선택하세요.),별도의 S3 버킷에 저장된 키로 서버 측 암호화를 설정합니다.,Amazon S3 관리형 암호화 키로 클라이언트 측 암호화를 설정합니다.,AWS Key Management Service(AWS KMS)에 저장된 고객 마스터 키로 클라이언트 측 암호화를 설정합니다.,클라이언트 측 마스터 키를 사용하여 클라이언트 측 암호화를 설정합니다.,EC2 키 쌍으로 서버 측 암호화(SSE)를 설정합니다.,,0,,
udemy,CLF-01,196,"A company has a High Performance Computing (HPC) cluster that is composed of EC2 Instances with Provisioned IOPS (io1) volume to process transaction-intensive, low-latency workloads. The Solutions Architect must maintain high IOPS while keeping the latency down by setting the optimal queue length for the volume. The size of each volume is 10 GiB.Which of the following is the MOST suitable configuration that the Architect should set up?",D,D,Set the IOPS to 800 then maintain a low queue length.,Set the IOPS to 600 then maintain a high queue length.,Set the IOPS to 400 then maintain a low queue length.,Set the IOPS to 500 then maintain a low queue length.,,,,회사에는 트랜잭션 집약적이고 지연 시간이 짧은 워크로드를 처리하기 위해 프로비저닝된 IOPS(io1) 볼륨이 있는 EC2 인스턴스로 구성된 HPC(고성능 컴퓨팅) 클러스터가 있습니다. Solutions Architect는 볼륨에 대한 최적의 대기열 길이를 설정하여 대기 시간을 줄이면서 높은 IOPS를 유지해야 합니다. 각 볼륨의 크기는 10GiB입니다.다음 중 Architect가 설정해야 하는 가장 적합한 구성은 무엇입니까?,IOPS를 800으로 설정한 다음 낮은 대기열 길이를 유지합니다.,IOPS를 600으로 설정한 다음 높은 대기열 길이를 유지합니다.,IOPS를 400으로 설정한 다음 낮은 대기열 길이를 유지합니다.,IOPS를 500으로 설정한 다음 낮은 대기열 길이를 유지합니다.,,,0,,
udemy,CLF-01,197,"A newly hired Solutions Architect is checking all of the security groups and network access control list rules of the company's AWS resources. For security purposes, the MS SQL connection via port 1433 of the database tier should be secured. Below is the security group configuration of their Microsoft SQL Server database:The application tier hosted in an Auto Scaling group of EC2 instances is the only identified resource that needs to connect to the database. The Architect should ensure that the architecture complies with the best practice of granting least privilege. Which of the following changes should be made to the security group configuration?",C,C,"For the MS SQL rule, change the Source to the static AnyCast IP address attached to the application tier.","For the MS SQL rule, change the Source to the EC2 instance IDs of the underlying instances of the Auto Scaling group.","For the MS SQL rule, change the Source to the security group ID attached to the application tier.","For the MS SQL rule, change the Source to the Network ACL ID attached to the application tier.",,,,새로 고용된 솔루션 설계자는 회사의 AWS 리소스에 대한 모든 보안 그룹 및 네트워크 액세스 제어 목록 규칙을 확인하고 있습니다. 보안을 위해 데이터베이스 계층의 포트 1433을 통한 MS SQL 연결을 보호해야 합니다. 다음은 Microsoft SQL Server 데이터베이스의 보안 그룹 구성입니다.EC2 인스턴스의 Auto Scaling 그룹에서 호스팅되는 애플리케이션 계층은 데이터베이스에 연결해야 하는 유일한 식별된 리소스입니다. 설계자는 아키텍처가 최소 권한을 부여하는 모범 사례를 준수하는지 확인해야 합니다.다음 중 보안 그룹 구성을 변경해야 하는 것은 무엇입니까?,MS SQL 규칙의 경우 Source애플리케이션 계층에 연결된 고정 AnyCast IP 주소로 변경합니다.,MS SQL 규칙의 경우 SourceAuto Scaling 그룹 기본 인스턴스의 EC2 인스턴스 ID로 변경합니다.,MS SQL 규칙의 경우 Source애플리케이션 계층에 연결된 보안 그룹 ID로 변경합니다.,MS SQL 규칙의 경우 Source애플리케이션 계층에 연결된 네트워크 ACL ID로 변경합니다.,,,0,,
udemy,CLF-01,198,"A company plans to deploy an application in an Amazon EC2 instance. The application will perform the following tasks: - Read large datasets from an Amazon S3 bucket. - Execute multi-stage analysis on the datasets. - Save the results to Amazon RDS.During multi-stage analysis, the application will store a large number of temporary files in the instance storage. As the Solutions Architect, you need to recommend the fastest storage option with high I/O performance for the temporary files.Which of the following options fulfills this requirement?",B,B,Configure RAID 1 in multiple instance store volumes.,Configure RAID 0 in multiple instance store volumes.,Attach multiple Provisioned IOPS SSD volumes in the instance.,Enable Transfer Acceleration in Amazon S3.,,,,회사에서 Amazon EC2 인스턴스에 애플리케이션을 배포할 계획입니다. 애플리케이션은 다음 작업을 수행합니다.- Amazon S3 버킷에서 대용량 데이터 세트를 읽습니다.- 데이터 세트에 대한 다단계 분석을 실행합니다.- 결과를 Amazon RDS에 저장합니다.다단계 분석 중에 애플리케이션은 인스턴스 스토리지에 많은 수의 임시 파일을 저장합니다. Solutions Architect는 임시 파일에 대해 높은 I/O 성능을 제공하는 가장 빠른 스토리지 옵션을 권장해야 합니다.다음 중 이 요구 사항을 충족하는 옵션은 무엇입니까?,여러 인스턴스 스토어 볼륨에서 RAID 1을 구성합니다.,여러 인스턴스 스토어 볼륨에서 RAID 0을 구성합니다.,인스턴스에 여러 개의 프로비저닝된 IOPS SSD 볼륨을 연결합니다.,Amazon S3에서 Transfer Acceleration을 활성화합니다.,,,0,,
udemy,CLF-01,199,A company needs to integrate the Lightweight Directory Access Protocol (LDAP) directory service from the on-premises data center to the AWS VPC using IAM. The identity store which is currently being used is not compatible with SAML.Which of the following provides the most valid approach to implement the integration?,D,D,Use IAM roles to rotate the IAM credentials whenever LDAP credentials are updated.,Use an IAM policy that references the LDAP identifiers and AWS credentials.,Use AWS Single Sign-On (SSO) service to enable single sign-on between AWS and your LDAP.,Develop an on-premises custom identity broker application and use STS to issue short-lived AWS credentials.,,,,회사는 IAM을 사용하여 온프레미스 데이터 센터에서 AWS VPC로 LDAP(Lightweight Directory Access Protocol) 디렉터리 서비스를 통합해야 합니다. 현재 사용 중인 ID 저장소가 SAML과 호환되지 않습니다.다음 중 통합 구현을 위한 가장 유효한 접근 방식은 무엇입니까?,IAM 역할을 사용하여 LDAP 자격 증명이 업데이트될 때마다 IAM 자격 증명을 교체합니다.,LDAP 식별자와 AWS 자격 증명을 참조하는 IAM 정책을 사용합니다.,AWS Single Sign-On(SSO) 서비스를 사용하여 AWS와 LDAP 간에 Single Sign-On을 활성화합니다.,온프레미스 사용자 지정 자격 증명 브로커 애플리케이션을 개발하고 STS를 사용하여 단기 AWS 자격 증명을 발급합니다.,,,0,,
udemy,CLF-01,200,"A company has a two-tier environment in its on-premises data center which is composed of an application tier and database tier. You are instructed to migrate their environment to the AWS cloud, and to design the subnets in their VPC with the following requirements:1. There is an application load balancer that would distribute the incoming traffic among the servers in the application tier.2. The application tier and the database tier must not be accessible from the public Internet. The application tier should only accept traffic coming from the load balancer.3. The database tier contains very sensitive data. It must not share the same subnet with other AWS resources and its custom route table with other instances in the environment.4. The environment must be highly available and scalable to handle a surge of incoming traffic over the Internet.How many subnets should you create to meet the above requirements?",D,D,4,3,2,6,,,,회사는 온프레미스 데이터 센터에 애플리케이션 계층과 데이터베이스 계층으로 구성된 2계층 환경을 가지고 있습니다. 환경을 AWS 클라우드로 마이그레이션하고 다음 요구 사항에 따라 VPC에서 서브넷을 설계하라는 지시를 받습니다.1. 애플리케이션 계층의 서버 간에 들어오는 트래픽을 분산시키는 애플리케이션 로드 밸런서가 있습니다. 2. 애플리케이션 계층과 데이터베이스 계층은 공용 인터넷에서 액세스할 수 없어야 합니다. 애플리케이션 계층은 로드 밸런서에서 오는 트래픽만 수락해야 합니다. 3. 데이터베이스 계층에는 매우 민감한 데이터가 포함되어 있습니다. 다른 AWS 리소스와 동일한 서브넷을 공유하고 환경의 다른 인스턴스와 사용자 지정 경로 테이블을 공유해서는 안 됩니다. 4. 환경은 인터넷을 통해 들어오는 트래픽 급증을 처리할 수 있도록 가용성과 확장성이 높아야 합니다.위의 요구 사항을 충족하려면 몇 개의 서브넷을 만들어야 합니까?,4,삼,2,6,,,0,,
udemy,CLF-01,201,A company has a web application hosted in an On-Demand EC2 instance. You are creating a shell script that needs the instance's public and private IP addresses.What is the best way to get the instance's associated IP addresses which your shell script can use?,D,D,By using IAM.,By using a CloudWatch metric.,By using a Curl or Get Command to get the latest user data information from http://169.254.169.254/latest/user-data/,By using a Curl or Get Command to get the latest metadata information from http://169.254.169.254/latest/meta-data/,,,,회사에는 온디맨드 EC2 인스턴스에서 호스팅되는 웹 애플리케이션이 있습니다. 인스턴스의 퍼블릭 및 프라이빗 IP 주소가 필요한 셸 스크립트를 생성하고 있습니다.셸 스크립트에서 사용할 수 있는 인스턴스의 연결된 IP 주소를 가져오는 가장 좋은 방법은 무엇입니까?,IAM을 사용합니다.,CloudWatch 지표를 사용합니다.,Curl 또는 Get Command를 사용하여 http://169.254.169.254/latest/user-data/에서 최신 사용자 데이터 정보를 가져옵니다.,Curl 또는 Get 명령을 사용하여 http://169.254.169.254/latest/meta-data/에서 최신 메타데이터 정보를 가져옵니다.,,,0,,
udemy,CLF-01,202,A financial firm is designing an application architecture for its online trading platform that must have high availability and fault tolerance. Their Solutions Architect configured the application to use an Amazon S3 bucket located in the us-east-1 region to store large amounts of intraday financial data. The stored financial data in the bucket must not be affected even if there is an outage in one of the Availability Zones or if there's a regional service failure. What should the Architect do to avoid any costly service disruptions and ensure data durability?,D,D,Create a new S3 bucket in another region and configure Cross-Account Access to the bucket located in us-east-1.,Copy the S3 bucket to an EBS-backed EC2 instance.,Create a Lifecycle Policy to regularly backup the S3 bucket to Amazon Glacier.,Enable Cross-Region Replication.,,,,금융 회사는 고가용성과 내결함성이 있어야 하는 온라인 거래 플랫폼용 애플리케이션 아키텍처를 설계하고 있습니다. 솔루션 설계자는 us-east-1 지역에 있는 Amazon S3 버킷을 사용하여 많은 양의 일중 금융 데이터를 저장하도록 애플리케이션을 구성했습니다. 가용 영역 중 하나에서 중단이 발생하거나 지역 서비스 장애가 발생하더라도 버킷에 저장된 재무 데이터는 영향을 받지 않아야 합니다.비용이 많이 드는 서비스 중단을 피하고 데이터 내구성을 보장하기 위해 Architect는 무엇을 해야 합니까?,다른 리전에 새 S3 버킷을 생성하고 us-east-1에 있는 버킷에 대한 교차 계정 액세스를 구성합니다.,S3 버킷을 EBS 지원 EC2 인스턴스에 복사합니다.,S3 버킷을 Amazon Glacier에 정기적으로 백업하는 수명 주기 정책을 생성합니다.,지역 간 복제를 활성화합니다.,,,0,,
udemy,CLF-01,203,"A company has a team of developers that provisions their own resources on the AWS cloud. The developers use IAM user access keys to automate their resource provisioning and application testing processes in AWS. To ensure proper security compliance, the security team wants to automate the process of deactivating and deleting any IAM user access key that is over 90 days old.Which solution will meet these requirements with the LEAST operational effort?",B,B,Create a custom AWS Config rule to check for the max-age of IAM access keys. Schedule an AWS Batch job that runs every 24 hours to delete all the non-compliant access keys.,"Use the AWS Config managed rule to check if the IAM user access keys are not rotated within 90 days. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the non-compliant keys, and define a target to invoke a custom Lambda function to deactivate and delete the keys.",Create an Amazon EventBridge (Amazon CloudWatch Events) rule to filter IAM user access keys older than 90 days. Define a target to invoke a Lambda function to deactivate and delete the old access keys.,Create an Amazon EventBridge (Amazon CloudWatch Events) rule to filter IAM user access keys older than 90 days. Schedule an AWS Batch job that runs every 24 hours to delete all the specified access keys.,,,,회사에는 AWS 클라우드에서 자체 리소스를 프로비저닝하는 개발자 팀이 있습니다. 개발자는 IAM 사용자 액세스 키를 사용하여 AWS에서 리소스 프로비저닝 및 애플리케이션 테스트 프로세스를 자동화합니다. 적절한 보안 준수를 보장하기 위해 보안 팀은 90일이 지난 모든 IAM 사용자 액세스 키를 비활성화하고 삭제하는 프로세스를 자동화하려고 합니다.최소한의 운영 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?,사용자 지정 AWS Config 규칙을 생성하여 IAM 액세스 키의 최대 수명을 확인합니다. 규정을 준수하지 않는 모든 액세스 키를 삭제하기 위해 24시간마다 실행되는 AWS Batch 작업을 예약합니다.,AWS Config 관리형 규칙을 사용하여 IAM 사용자 액세스 키가 90일 이내에 교체되지 않았는지 확인하십시오. 비준수 키에 대한 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성하고 대상을 정의하여 사용자 지정 Lambda 함수를 호출하여 키를 비활성화하고 삭제합니다.,90일보다 오래된 IAM 사용자 액세스 키를 필터링하는 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다. 이전 액세스 키를 비활성화하고 삭제하는 Lambda 함수를 호출할 대상을 정의합니다.,90일보다 오래된 IAM 사용자 액세스 키를 필터링하는 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다. 24시간마다 실행되는 AWS Batch 작업을 예약하여 지정된 모든 액세스 키를 삭제합니다.,,,0,,
udemy,CLF-01,204,A company launched an EC2 instance in the newly created VPC. They noticed that the generated instance does not have an associated DNS hostname.Which of the following options could be a valid reason for this issue?,A,A,The DNS resolution and DNS hostname of the VPC configuration should be enabled.,The security group of the EC2 instance needs to be modified.,Amazon Route53 is not enabled.,The newly created VPC has an invalid CIDR block.,,,,회사는 새로 생성된 VPC에서 EC2 인스턴스를 시작했습니다. 그들은 생성된 인스턴스에 연결된 DNS 호스트 이름이 없음을 확인했습니다.다음 옵션 중 이 문제의 유효한 원인은 무엇입니까?,VPC 구성의 DNS 확인 및 DNS 호스트 이름이 활성화되어야 합니다.,EC2 인스턴스의 보안 그룹을 수정해야 합니다.,Amazon Route53이 활성화되지 않았습니다.,새로 생성된 VPC에 잘못된 CIDR 블록이 있습니다.,,,0,,
udemy,CLF-01,205,"To save costs, your manager instructed you to analyze and review the setup of your AWS cloud infrastructure. You should also provide an estimate of how much your company will pay for all of the AWS resources that they are using. In this scenario, which of the following will incur costs? (Select TWO.)",DE,DE,A stopped On-Demand EC2 Instance,Public Data Set,Using an Amazon VPC,A running EC2 Instance,EBS Volumes attached to stopped EC2 Instances,,,비용을 절감하기 위해 관리자가 AWS 클라우드 인프라 설정을 분석하고 검토하도록 지시했습니다. 또한 회사에서 사용 중인 모든 AWS 리소스에 대해 지불할 추정 금액을 제공해야 합니다. 이 시나리오에서 다음 중 어떤 비용이 발생합니까? (2개를 선택하세요.),중지된 온디맨드 EC2 인스턴스,공개 데이터 세트,아마존 VPC 사용,실행 중인 EC2 인스턴스,중지된 EC2 인스턴스에 연결된 EBS 볼륨,,0,,
udemy,CLF-01,206,"A company has a global news website hosted in a fleet of EC2 Instances. Lately, the load on the website has increased which resulted in slower response time for the site visitors. This issue impacts the revenue of the company as some readers tend to leave the site if it does not load after 10 seconds.Which of the below services in AWS can be used to solve this problem? (Select TWO.)",AC,AC,Use Amazon CloudFront with website as the custom origin.,Deploy the website to all regions in different VPCs for faster processing.,Use Amazon ElastiCache for the website's in-memory data store or cache.,"For better read throughput, use AWS Storage Gateway to distribute the content across multiple regions.",,,,한 회사에 여러 EC2 인스턴스에서 호스팅되는 글로벌 뉴스 웹 사이트가 있습니다. 최근 웹사이트의 로드가 증가하여 사이트 방문자의 응답 시간이 느려졌습니다. 이 문제는 일부 독자가 10초 후에 로드되지 않으면 사이트를 떠나는 경향이 있으므로 회사의 수익에 영향을 미칩니다.AWS의 다음 서비스 중 이 문제를 해결하는 데 사용할 수 있는 것은 무엇입니까? (2개를 선택하세요.),웹사이트와 함께 Amazon CloudFront를 사용자 지정 오리진으로 사용합니다.,더 빠른 처리를 위해 서로 다른 VPC의 모든 지역에 웹 사이트를 배포합니다.,웹 사이트의 인 메모리 데이터 저장소 또는 캐시에 Amazon ElastiCache를 사용합니다.,더 나은 읽기 처리량을 위해 AWS Storage Gateway를 사용하여 여러 리전에 콘텐츠를 배포하십시오.,,,0,,
udemy,CLF-01,207,"There are a few, easily reproducible but confidential files that your client wants to store in AWS without worrying about storage capacity. For the first month, all of these files will be accessed frequently but after that, they will rarely be accessed at all. The old files will only be accessed by developers so there is no set retrieval time requirement. However, the files under a specific tdojo-finance prefix in the S3 bucket will be used for post-processing that requires millisecond retrieval time.Given these conditions, which of the following options would be the most cost-effective solution for your client's storage needs?",B,B,"Store the files in S3 then after a month, change the storage class of the tdojo-finance prefix to S3-IA while the remaining go to Glacier using lifecycle policy.","Store the files in S3 then after a month, change the storage class of the tdojo-finance prefix to One Zone-IA while the remaining go to Glacier using lifecycle policy.","Store the files in S3 then after a month, change the storage class of the bucket to Intelligent-Tiering using lifecycle policy.","Store the files in S3 then after a month, change the storage class of the bucket to S3-IA using lifecycle policy.",,,,쉽게 재현할 수 있지만 고객이 스토리지 용량에 대한 걱정 없이 AWS에 저장하기를 원하는 몇 가지 기밀 파일이 있습니다. 처음 한 달 동안은 이러한 모든 파일에 자주 액세스하지만 그 이후에는 거의 액세스하지 않습니다. 이전 파일은 개발자만 액세스할 수 있으므로 정해진 검색 시간 요구 사항이 없습니다. 그러나 S3 버킷의 특정 접두사 아래에 있는 파일은 tdojo-finance밀리초 검색 시간이 필요한 후처리에 사용됩니다.이러한 조건을 고려할 때 다음 옵션 중 고객의 스토리지 요구 사항에 가장 비용 효율적인 솔루션은 무엇입니까?,파일을 S3에 저장한 다음 한 달 후에 접두사의 스토리지 클래스를 tdojo-financeS3-IA로 변경하고 나머지는 수명 주기 정책을 사용하여 Glacier로 이동합니다.,파일을 S3에 저장한 다음 한 달 후에 접두사의 스토리지 클래스를 tdojo-financeOne Zone-IA로 변경하고 나머지는 수명 주기 정책을 사용하여 Glacier로 이동합니다.,파일을 S3에 저장하고 한 달 후에 수명 주기 정책을 사용하여 버킷의 스토리지 클래스를 Intelligent-Tiering으로 변경합니다.,파일을 S3에 저장하고 한 달 후에 수명 주기 정책을 사용하여 버킷의 스토리지 클래스를 S3-IA로 변경합니다.,,,0,,
udemy,CLF-01,208,"A company has several web applications with users all around the world. Each application is hosted in an Auto Scaling group of EC2 instances in multiple AZs behind an Application Load Balancer (ALB). All applications have their own fully qualified domain name. For added security, the applications must use a publicly trusted SSL certificate.Which solution will meet this requirement with the LEAST operational overhead?",D,D,Use OpenSSL to generate a self-signed certificate. Import the SSL/TLS certificate to the AWS Certificate Manager (ACM) and associate it with the HTTPS listener of the ALBs,Issue an SSL/TLS certificate using the AWS Certificate Manager Private Certificate Authority. Associate the new certificate on the HTTPS listener of the ALBs.,Launch a self-hosted certificate authority (CA) using the Let's Encrypt tool in an Amazon EC2 instance. Utilize the built-in ISRG Root X1 trusted root CA certificate. Generate a new SSL/TLS certificate using the certbot CLI utility. Associate the new certificate on the HTTPS listener of the ALBs.,Use the AWS Certificate Manager (ACM) to generate a public SSL/TLS certificate. Associate the new SSL/TLS certificate on the HTTPS listener of the ALBs.,,,,한 회사에 전 세계 사용자가 있는 여러 웹 응용 프로그램이 있습니다. 각 애플리케이션은 ALB(Application Load Balancer) 뒤의 여러 AZ에 있는 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅됩니다. 모든 애플리케이션에는 고유한 정규화된 도메인 이름이 있습니다. 보안 강화를 위해 애플리케이션은 공개적으로 신뢰할 수 있는 SSL 인증서를 사용해야 합니다.최소한의 운영 오버헤드로 이 요구 사항을 충족하는 솔루션은 무엇입니까?,OpenSSL을 사용하여 자체 서명된 인증서를 생성합니다. SSL/TLS 인증서를 AWS Certificate Manager(ACM)로 가져와 ALB의 HTTPS 리스너와 연결합니다.,AWS Certificate Manager Private Certificate Authority를 ​​사용하여 SSL/TLS 인증서를 발급합니다. ALB의 HTTPS 리스너에서 새 인증서를 연결하십시오.,Amazon EC2 인스턴스에서 Let's Encrypt 도구를 사용하여 자체 호스팅 인증 기관(CA)을 시작합니다. 내장된 ISRG Root X1 신뢰할 수 있는 루트 CA 인증서를 활용합니다. CLI 유틸리티 를 사용하여 새 SSL/TLS 인증서를 생성합니다 certbot. ALB의 HTTPS 리스너에서 새 인증서를 연결하십시오.,AWS Certificate Manager(ACM)를 사용하여 퍼블릭 SSL/TLS 인증서를 생성합니다. ALB의 HTTPS 리스너에서 새 SSL/TLS 인증서를 연결합니다.,,,0,,
udemy,CLF-01,209,"A data analytics company, which uses machine learning to collect and analyze consumer data, is using Redshift cluster as their data warehouse. You are instructed to implement a disaster recovery plan for their systems to ensure business continuity even in the event of an AWS region outage.   Which of the following is the best approach to meet this requirement?",B,B,Use Automated snapshots of your Redshift Cluster.,Enable Cross-Region Snapshots Copy in your Amazon Redshift Cluster.,Create a scheduled job that will automatically take the snapshot of your Redshift Cluster and store it to an S3 bucket. Restore the snapshot in case of an AWS region outage.,"Do nothing because Amazon Redshift is a highly available, fully-managed data warehouse which can withstand an outage of an entire AWS region.",,,,기계 학습을 사용하여 소비자 데이터를 수집하고 분석하는 데이터 분석 회사는 Redshift 클러스터를 데이터 웨어하우스로 사용하고 있습니다. AWS 리전이 중단되는 경우에도 비즈니스 연속성을 보장하기 위해 시스템에 대한 재해 복구 계획을 구현하라는 지시를 받았습니다.   다음 중 이 요구 사항을 충족하는 가장 좋은 방법은 무엇입니까?,Redshift 클러스터의 자동 스냅샷을 사용합니다.,Amazon Redshift 클러스터에서 교차 리전 스냅샷 복사를 활성화합니다.,Redshift 클러스터의 스냅샷을 자동으로 생성하여 S3 버킷에 저장하는 예약된 작업을 생성합니다. AWS 리전 중단 시 스냅샷을 복원합니다.,Amazon Redshift는 전체 AWS 지역의 중단을 견딜 수 있는 고가용성 완전 관리형 데이터 웨어하우스이므로 아무 조치도 취하지 마십시오.,,,0,,
udemy,CLF-01,210,"A popular augmented reality (AR) mobile game is heavily using a RESTful API which is hosted in AWS. The API uses Amazon API Gateway and a DynamoDB table with a preconfigured read and write capacity. Based on your systems monitoring, the DynamoDB table begins to throttle requests during high peak loads which causes the slow performance of the game.  Which of the following can you do to improve the performance of your app?",A,A,Use DynamoDB Auto Scaling,Integrate an Application Load Balancer with your DynamoDB table.,Create an SQS queue in front of the DynamoDB table.,Add the DynamoDB table to an Auto Scaling Group.,,,,인기 있는 증강 현실(AR) 모바일 게임은 AWS에서 호스팅되는 RESTful API를 많이 사용하고 있습니다. API는 읽기 및 쓰기 용량이 미리 구성된 Amazon API Gateway 및 DynamoDB 테이블을 사용합니다. 시스템 모니터링을 기반으로 DynamoDB 테이블은 최대 부하가 높을 때 요청을 조절하기 시작하여 게임 성능을 저하시킵니다.  다음 중 앱 성능을 개선하기 위해 수행할 수 있는 작업은 무엇인가요?,DynamoDB Auto Scaling 사용,Application Load Balancer를 DynamoDB 테이블과 통합합니다.,DynamoDB 테이블 앞에 SQS 대기열을 생성합니다.,Auto Scaling 그룹에 DynamoDB 테이블을 추가합니다.,,,0,,
udemy,CLF-01,211,A company conducts performance testing on a t3.large MySQL RDS DB instance twice a week. They use Performance Insights to analyze and fine-tune expensive queries. The company needs to reduce its operational expense in running the tests without compromising the tests' integrity.Which of the following is the most cost-effective solution?,B,B,Perform a mysqldump to get a copy of the database on a local machine. Use MySQL Workbench to analyze the queries.,"Once the testing is completed, take a snapshot of the database and terminate it. Restore the database from the snapshot when necessary.",Stop the database once the test is done and restart it only when necessary.,Downgrade the database instance to t3.small.,,,,한 회사에서 t3.large일주일에 두 번 MySQL RDS DB 인스턴스에 대한 성능 테스트를 수행합니다. 성능 개선 도우미를 사용하여 비용이 많이 드는 쿼리를 분석하고 미세 조정합니다. 회사는 테스트의 무결성을 손상시키지 않으면서 테스트를 실행하는 데 드는 운영 비용을 줄여야 합니다.다음 중 가장 비용 효율적인 솔루션은 무엇입니까?,mysqldump로컬 시스템에서 데이터베이스 사본을 가져오려면 a를 수행하십시오 . MySQL Workbench를 사용하여 쿼리를 분석합니다.,테스트가 완료되면 데이터베이스의 스냅샷을 찍고 종료합니다. 필요한 경우 스냅샷에서 데이터베이스를 복원합니다.,테스트가 완료되면 데이터베이스를 중지하고 필요한 경우에만 다시 시작하십시오.,데이터베이스 인스턴스를 t3.small.,,,0,,
udemy,CLF-01,212,"A company decided to change its third-party data analytics tool to a cheaper solution. They sent a full data export on a CSV file which contains all of their analytics information. You then save the CSV file to an S3 bucket for storage. Your manager asked you to do some validation on the provided data export.In this scenario, what is the most cost-effective and easiest way to analyze export data using standard SQL?",B,B,Use a migration tool to load the CSV export file from S3 to a database that is designed for online analytic processing (OLAP) such as AWS RedShift. Run some queries once the data has been loaded to complete your validation.,"To be able to run SQL queries, use AWS Athena to analyze the export data file in S3.",Use mysqldump client utility to load the CSV export file from S3 to a MySQL RDS instance. Run some SQL queries once the data has been loaded to complete your validation.,"Create a migration tool to load the CSV export file from S3 to a DynamoDB instance. Once the data has been loaded, run queries using DynamoDB.",,,,한 회사에서 타사 데이터 분석 도구를 더 저렴한 솔루션으로 변경하기로 결정했습니다. 모든 분석 정보가 포함된 CSV 파일로 전체 데이터 내보내기를 보냈습니다. 그런 다음 저장을 위해 CSV 파일을 S3 버킷에 저장합니다. 귀하의 관리자는 제공된 데이터 내보내기에 대해 몇 가지 검증을 수행하도록 요청했습니다.이 시나리오에서 표준 SQL을 사용하여 내보내기 데이터를 분석하는 가장 비용 효율적이고 쉬운 방법은 무엇입니까?,마이그레이션 도구를 사용하여 S3에서 AWS RedShift와 같은 OLAP(온라인 분석 처리)용으로 설계된 데이터베이스로 CSV 내보내기 파일을 로드합니다. 데이터가 로드되면 몇 가지 쿼리를 실행하여 유효성 검사를 완료합니다.,SQL 쿼리를 실행할 수 있으려면 AWS Athena를 사용하여 S3에서 내보내기 데이터 파일을 분석하십시오.,mysqldump 클라이언트 유틸리티를 사용하여 S3에서 MySQL RDS 인스턴스로 CSV 내보내기 파일을 로드합니다. 데이터가 로드되면 일부 SQL 쿼리를 실행하여 유효성 검사를 완료하십시오.,S3에서 DynamoDB 인스턴스로 CSV 내보내기 파일을 로드하는 마이그레이션 도구를 생성합니다. 데이터가 로드되면 DynamoDB를 사용하여 쿼리를 실행합니다.,,,0,,
udemy,CLF-01,213,"A company has a regional API Gateway in the us-east-2 region that serves as a proxy to a backend service. Clients connect to the service using the invoke URL of the API stage. To improve usability, the company wants to associate a custom domain name (api.tutorialsdojo.com) with the API. Moreover, the domain name must support HTTPS to ensure secure connections. The company has an existing hosted zone for its domain on Amazon Route 53.Which of the following would be the next step to achieve the company's objective?",B,B,Use the AWS Certificate Manager Private Certificate Authority (ACM PCA) to generate a private certificate for api.tutorialsdojo.com. Override the invoke URL using stage variables.,"Request a public certificate in the us-east-2 region for api.tutorialsdojo.com using AWS Certificate Manager (ACM). Create a regional API Gateway domain name and associate it with api.tutorialsdojo.com and the ACM certificate. In Route 53, create an alias record for api.tutorialsdojo.com that points to the API Gateway domain name.","Request a public certificate in the us-east-1 region for api.tutorialsdojo.com using AWS Certificate Manager (ACM). Create a regional API Gateway domain name and associate it with api.tutorialsdojo.com and the ACM certificate. In Route 53, create an alias record for api.tutorialsdojo.com that points to the API Gateway domain name.","Import an existing public certificate for api.tutorialsdojo.com into AWS Certificate Manager (ACM) in the us-east-2. In Route 53, create a CNAME record for api.tutorialsdojo.com that points to the invoke URL of the API Gateway stage.",,,,회사에는 백엔드 서비스에 대한 프록시 역할을 하는 us-east-2 지역에 지역 API 게이트웨이가 있습니다. 클라이언트는 API 단계의 호출 URL을 사용하여 서비스에 연결합니다. 사용성을 개선하기 위해 회사는 사용자 지정 도메인 이름( api.tutorialsdojo.com)을 API와 연결하려고 합니다. 또한 보안 연결을 보장하려면 도메인 이름이 HTTPS를 지원해야 합니다. 이 회사는 Amazon Route 53에 도메인에 대한 기존 호스팅 영역을 가지고 있습니다.다음 중 회사의 목표를 달성하기 위한 다음 단계는 무엇입니까?,AWS Certificate Manager Private Certificate Authority(ACM PCA)를 사용하여 api.tutorialsdojo.com. 단계 변수를 사용하여 호출 URL을 재정의합니다.,api.tutorialsdojo.comAWS Certificate Manager(ACM)를 사용하기 위해 us-east-2 리전에서 공인 인증서를 요청합니다 . 리전 API 게이트웨이 도메인 이름을 생성하고 이를 api.tutorialsdojo.comACM 인증서와 연결합니다. api.tutorialsdojo.comRoute 53에서 API 게이트웨이 도메인 이름을 가리키는 별칭 레코드를 생성합니다 .,api.tutorialsdojo.comAWS Certificate Manager(ACM)를 사용하기 위해 us-east-1 리전에서 공인 인증서를 요청합니다 . 리전 API 게이트웨이 도메인 이름을 생성하고 이를 api.tutorialsdojo.comACM 인증서와 연결합니다. api.tutorialsdojo.comRoute 53에서 API 게이트웨이 도메인 이름을 가리키는 별칭 레코드를 생성합니다 .,api.tutorialsdojo.comus-east-2의 AWS Certificate Manager(ACM)로 기존 공인 인증서를 가져옵니다 . api.tutorialsdojo.comRoute 53에서 API 게이트웨이 단계의 호출 URL을 가리키는 CNAME 레코드를 생성합니다 .,,,0,,
udemy,CLF-01,214,A Solutions Architect is designing a monitoring application which generates audit logs of all operational activities of the company's cloud infrastructure. Their IT Security and Compliance team mandates that the application retain the logs for 5 years before the data can be deleted. How can the Architect meet the above requirement?,A,A,Store the audit logs in a Glacier vault and use the Vault Lock feature.,Store the audit logs in an Amazon S3 bucket and enable Multi-Factor Authentication Delete (MFA Delete) on the S3 bucket.,Store the audit logs in an EBS volume and then take EBS snapshots every month.,Store the audit logs in an EFS volume and use Network File System version 4 (NFSv4) file-locking mechanism.,,,,Solutions Architect는 회사 클라우드 인프라의 모든 운영 활동에 대한 감사 로그를 생성하는 모니터링 애플리케이션을 설계하고 있습니다. 그들의 IT 보안 및 규정 준수 팀은 데이터가 삭제되기 전에 애플리케이션이 5년 동안 로그를 유지하도록 요구합니다.Architect는 위의 요구 사항을 어떻게 충족할 수 있습니까?,Glacier 볼트에 감사 로그를 저장하고 볼트 잠금 기능을 사용하십시오.,감사 로그를 Amazon S3 버킷에 저장하고 S3 버킷에서 MFA 삭제(Multi-Factor Authentication Delete)를 활성화합니다.,감사 로그를 EBS 볼륨에 저장한 다음 매월 EBS 스냅샷을 생성합니다.,감사 로그를 EFS 볼륨에 저장하고 NFSv4(Network File System version 4) 파일 잠금 메커니즘을 사용합니다.,,,0,,
udemy,CLF-01,215,"A company is running a web application on AWS. The application is made up of an Auto-Scaling group that sits behind an Application Load Balancer and an Amazon DynamoDB table where user data is stored. The solutions architect must design the application to remain available in the event of a regional failure. A solution to automatically monitor the status of your workloads across your AWS account, conduct architectural reviews and check for AWS best practices.Which configuration meets the requirement with the least amount of downtime possible?",A,A,"In a secondary region, create a global table of the DynamoDB table and replicate the auto-scaling group and application load balancer. Use Route 53 DNS failover to automatically route traffic to the resources in the secondary region. Set up the AWS Well-Architected Tool to easily get recommendations for improving your workloads based on the AWS best practices","Write a CloudFormation template that includes the auto-scaling group, application load balancer, and DynamoDB table. In the event of a failure, deploy the template in a secondary region. Use Route 53 DNS failover to automatically route traffic to the resources in the secondary region. Set up and configure the Amazon Managed Service for Prometheus service to receive insights for improving your workloads based on the AWS best practices.","Write a CloudFormation template that includes the auto-scaling group, application load balancer, and DynamoDB table. In the event of a failure, deploy the template in a secondary region. Configure Amazon EventBridge to trigger a Lambda function that updates the application’s Route 53 DNS record. Launch an Amazon Managed Grafana workspace to automatically receive tips and action items for improving your workloads based on the AWS best practices","In a secondary region, create a global secondary index of the DynamoDB table and replicate the auto-scaling group and application load balancer. Use Route 53 DNS failover to automatically route traffic to the resources in the secondary region. Set up the AWS Compute Optimizer to automatically get recommendations for improving your workloads based on the AWS best practices",,,,회사가 AWS에서 웹 애플리케이션을 실행하고 있습니다. 애플리케이션은 Application Load Balancer 뒤에 있는 Auto-Scaling 그룹과 사용자 데이터가 저장되는 Amazon DynamoDB 테이블로 구성됩니다. 솔루션 설계자는 지역 장애가 발생한 경우에도 계속 사용할 수 있도록 애플리케이션을 설계해야 합니다. AWS 계정 전체에서 워크로드 상태를 자동으로 모니터링하고 아키텍처 검토를 수행하며 AWS 모범 사례를 확인하는 솔루션입니다.가동 중지 시간을 최소화하면서 요구 사항을 충족하는 구성은 무엇입니까?,보조 리전에서 DynamoDB 테이블의 글로벌 테이블을 생성하고 Auto-Scaling 그룹 및 애플리케이션 로드 밸런서를 복제합니다. Route 53 DNS 장애 조치를 사용하여 트래픽을 보조 리전의 리소스로 자동 라우팅합니다. AWS 모범 사례를 기반으로 워크로드를 개선하기 위한 권장 사항을 쉽게 얻을 수 있도록 AWS Well-Architected 도구를 설정합니다.,"Auto-Scaling 그룹, 애플리케이션 로드 밸런서 및 DynamoDB 테이블을 포함하는 CloudFormation 템플릿을 작성합니다. 실패할 경우 템플릿을 보조 지역에 배포합니다. Route 53 DNS 장애 조치를 사용하여 트래픽을 보조 리전의 리소스로 자동 라우팅합니다. Amazon Managed Service for Prometheus 서비스를 설정하고 구성하여 AWS 모범 사례를 기반으로 워크로드를 개선하기 위한 통찰력을 얻으십시오.","Auto-Scaling 그룹, 애플리케이션 로드 밸런서 및 DynamoDB 테이블을 포함하는 CloudFormation 템플릿을 작성합니다. 실패할 경우 템플릿을 보조 지역에 배포합니다. 애플리케이션의 Route 53 DNS 레코드를 업데이트하는 Lambda 함수를 트리거하도록 Amazon EventBridge를 구성합니다. Amazon Managed Grafana 작업 공간을 시작하여 AWS 모범 사례를 기반으로 워크로드를 개선하기 위한 팁과 작업 항목을 자동으로 수신합니다.",보조 리전에서 DynamoDB 테이블의 글로벌 보조 인덱스를 생성하고 Auto-Scaling 그룹 및 애플리케이션 로드 밸런서를 복제합니다. Route 53 DNS 장애 조치를 사용하여 트래픽을 보조 리전의 리소스로 자동 라우팅합니다. AWS 모범 사례를 기반으로 워크로드를 개선하기 위한 권장 사항을 자동으로 가져오도록 AWS Compute Optimizer를 설정합니다.,,,0,,
udemy,CLF-01,216,"A company needs to implement a solution that will process real-time streaming data of its users across the globe. This will enable them to track and analyze globally-distributed user activity on their website and mobile applications, including clickstream analysis. The solution should process the data in close geographical proximity to their users and respond to user requests at low latencies.Which of the following is the most suitable solution for this scenario?",B,B,Integrate CloudFront with Lambda@Edge in order to process the data in close geographical proximity to users and respond to user requests at low latencies. Process real-time streaming data using Amazon Athena and durably store the results to an Amazon S3 bucket.,Integrate CloudFront with Lambda@Edge in order to process the data in close geographical proximity to users and respond to user requests at low latencies. Process real-time streaming data using Kinesis and durably store the results to an Amazon S3 bucket.,"Use a CloudFront web distribution and Route 53 with a latency-based routing policy, in order to process the data in close geographical proximity to users and respond to user requests at low latencies. Process real-time streaming data using Kinesis and durably store the results to an Amazon S3 bucket.",Use a CloudFront web distribution and Route 53 with a Geoproximity routing policy in order to process the data in close geographical proximity to users and respond to user requests at low latencies. Process real-time streaming data using Kinesis and durably store the results to an Amazon S3 bucket.,,,,회사는 전 세계 사용자의 실시간 스트리밍 데이터를 처리할 솔루션을 구현해야 합니다. 이를 통해 클릭스트림 분석을 포함하여 웹사이트 및 모바일 애플리케이션에서 전 세계적으로 분산된 사용자 활동을 추적하고 분석할 수 있습니다. 솔루션은 사용자와 지리적으로 가까운 거리에서 데이터를 처리하고 짧은 대기 시간으로 사용자 요청에 응답해야 합니다.다음 중 이 시나리오에 가장 적합한 솔루션은 무엇입니까?,CloudFront를 Lambda@Edge와 통합하여 사용자와 지리적으로 가까운 거리에서 데이터를 처리하고 지연 시간이 짧은 사용자 요청에 응답합니다. Amazon Athena를 사용하여 실시간 스트리밍 데이터를 처리하고 결과를 Amazon S3 버킷에 안정적으로 저장합니다.,CloudFront를 Lambda@Edge와 통합하여 사용자와 지리적으로 가까운 거리에서 데이터를 처리하고 지연 시간이 짧은 사용자 요청에 응답합니다. Kinesis를 사용하여 실시간 스트리밍 데이터를 처리하고 결과를 Amazon S3 버킷에 안정적으로 저장합니다.,지연 시간 기반 라우팅 정책과 함께 CloudFront 웹 배포 및 Route 53을 사용하여 사용자에게 지리적으로 근접한 데이터를 처리하고 낮은 지연 시간으로 사용자 요청에 응답합니다. Kinesis를 사용하여 실시간 스트리밍 데이터를 처리하고 결과를 Amazon S3 버킷에 안정적으로 저장합니다.,CloudFront 웹 배포 및 Route 53을 Geoproximity 라우팅 정책과 함께 사용하여 사용자와 지리적으로 가까운 위치에서 데이터를 처리하고 지연 시간이 짧은 사용자 요청에 응답합니다. Kinesis를 사용하여 실시간 스트리밍 데이터를 처리하고 결과를 Amazon S3 버킷에 안정적으로 저장합니다.,,,0,,
udemy,CLF-01,217,"A company has stored 200 TB of backup files in Amazon S3. The files are in a vendor-proprietary format. The Solutions Architect needs to use the vendor's proprietary file conversion software to retrieve the files from their Amazon S3 bucket, transform the files to an industry-standard format, and re-upload the files back to Amazon S3. The solution must minimize the data transfer costs.Which of the following options can satisfy the given requirement?",C,C,Install the file conversion software in Amazon S3. Use S3 Batch Operations to perform data transformation.,Deploy the EC2 instance in a different Region. Install the conversion software on the instance. Perform data transformation and re-upload it to Amazon S3.,Deploy the EC2 instance in the same Region as Amazon S3. Install the file conversion software on the instance. Perform data transformation and re-upload it to Amazon S3.,Export the data using AWS Snowball Edge device. Install the file conversion software on the device. Transform the data and re-upload it to Amazon S3.,,,,한 회사에서 Amazon S3에 200TB의 백업 파일을 저장했습니다. 파일은 공급업체 독점 형식입니다. Solutions Architect는 공급업체의 독점 파일 변환 소프트웨어를 사용하여 Amazon S3 버킷에서 파일을 검색하고 파일을 업계 표준 형식으로 변환하고 파일을 다시 Amazon S3에 다시 업로드해야 합니다. 솔루션은 데이터 전송 비용을 최소화해야 합니다.다음 옵션 중 주어진 요구 사항을 충족할 수 있는 옵션은 무엇입니까?,Amazon S3에 파일 변환 소프트웨어를 설치합니다. S3 배치 작업을 사용하여 데이터 변환을 수행합니다.,다른 지역에 EC2 인스턴스를 배포합니다. 인스턴스에 변환 소프트웨어를 설치합니다. 데이터 변환을 수행하고 Amazon S3에 다시 업로드합니다.,Amazon S3와 동일한 리전에 EC2 인스턴스를 배포합니다. 인스턴스에 파일 변환 소프트웨어를 설치합니다. 데이터 변환을 수행하고 Amazon S3에 다시 업로드합니다.,AWS Snowball Edge 디바이스를 사용하여 데이터를 내보냅니다. 장치에 파일 변환 소프트웨어를 설치합니다. 데이터를 변환하고 Amazon S3에 다시 업로드합니다.,,,0,,
udemy,CLF-01,218,A leading IT consulting company has an application which processes a large stream of financial data by an Amazon ECS Cluster then stores the result to a DynamoDB table. You have to design a solution to detect new entries in the DynamoDB table then automatically trigger a Lambda function to run some tests to verify the processed data. What solution can be easily implemented to alert the Lambda function of new entries while requiring minimal configuration change to your architecture?,A,A,Enable DynamoDB Streams to capture table activity and automatically trigger the Lambda function.,Invoke the Lambda functions using SNS each time that the ECS Cluster successfully processed financial data.,Use CloudWatch Alarms to trigger the Lambda function whenever a new entry is created in the DynamoDB table.,Use Systems Manager Automation to detect new entries in the DynamoDB table then automatically invoke the Lambda function for processing.,,,,선도적인 IT 컨설팅 회사에는 Amazon ECS 클러스터에서 대규모 재무 데이터 스트림을 처리한 다음 그 결과를 DynamoDB 테이블에 저장하는 애플리케이션이 있습니다. DynamoDB 테이블에서 새 항목을 감지한 다음 Lambda 함수를 자동으로 트리거하여 일부 테스트를 실행하여 처리된 데이터를 확인하는 솔루션을 설계해야 합니다.아키텍처에 대한 구성 변경을 최소화하면서 Lambda 함수에 새 항목을 알리기 위해 쉽게 구현할 수 있는 솔루션은 무엇입니까?,DynamoDB Streams를 활성화하여 테이블 활동을 캡처하고 Lambda 함수를 자동으로 트리거합니다.,ECS 클러스터가 재무 데이터를 성공적으로 처리할 때마다 SNS를 사용하여 Lambda 함수를 호출합니다.,CloudWatch 경보를 사용하여 DynamoDB 테이블에 새 항목이 생성될 때마다 Lambda 함수를 트리거합니다.,Systems Manager Automation을 사용하여 DynamoDB 테이블의 새 항목을 감지한 다음 처리를 위해 Lambda 함수를 자동으로 호출합니다.,,,0,,
udemy,CLF-01,219,A company is looking to store their confidential financial files in AWS which are accessed every week. The Architect was instructed to set up the storage system which uses envelope encryption and automates key rotation. It should also provide an audit trail that shows who used the encryption key and by whom for security purposes.Which combination of actions should the Architect implement to satisfy the requirement in the most cost-effective way? (Select TWO.),AD,AD,Use Amazon S3 to store the data.,Configure Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3).,Use Amazon S3 Glacier Deep Archive to store the data.,Configure Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS).,Amazon Certificate Manager,Configure Server-Side Encryption with Customer-Provided Keys (SSE-C).,,"한 회사에서 매주 액세스하는 AWS에 기밀 재무 파일을 저장하려고 합니다.  설계자는 엔벨로프 암호화를 사용하고 키 순환을 자동화하는 스토리지 시스템을 설정하라는 지시를 받았습니다. 또한 누가 보안 목적으로 암호화 키를 사용했는지, 누가 사용했는지 보여주는 감사 추적을 제공해야 합니다.가장 비용 효율적인 방식으로 요구 사항을 충족하기 위해 Architect가 구현해야 하는 작업 조합은 무엇입니까? (2개를 선택하세요.)",Amazon S3를 사용하여 데이터를 저장합니다.,Amazon S3 관리형 키(SSE-S3)로 서버 측 암호화를 구성합니다.,Amazon S3 Glacier Deep Archive를 사용하여 데이터를 저장합니다.,AWS KMS 관리형 키(SSE-KMS)로 서버 측 암호화를 구성합니다.,아마존 인증서 관리자,,0,,고객 제공 키(SSE-C)로 서버 측 암호화를 구성합니다.
udemy,CLF-01,220,"A multinational bank is storing its confidential files in an S3 bucket. The security team recently performed an audit, and the report shows that multiple files have been uploaded without 256-bit Advanced Encryption Standard (AES) server-side encryption. For added protection, the encryption key must be automatically rotated every year. The solutions architect must ensure that there would be no other unencrypted files uploaded in the S3 bucket in the future.Which of the following will meet these requirements with the LEAST operational overhead?",D,D,"Create a Service Control Policy (SCP) for the S3 bucket that rejects any object uploads unless the request includes the s3:x-amz-server-side-encryption"": ""AES256"" header. Enable server-side encryption with Amazon S3-managed encryption keys (SSE-S3) and modify the built-in key rotation feature of the SSE-S3 encryption keys to rotate the key yearly.","Create an S3 bucket policy for the S3 bucket that rejects any object uploads unless the request includes the s3:x-amz-server-side-encryption"":""aws:kms"" header. Enable the S3 Object Lock in compliance mode for all objects to automatically rotate the built-in AES256 customer-managed key of the bucket.",Create a new customer-managed key (CMK) from the AWS Key Management Service (AWS KMS). Configure the default encryption behavior of the bucket to use the customer-managed key. Manually rotate the CMK each and every year.,"Create an S3 bucket policy that denies permissions to upload an object unless the request includes the s3:x-amz-server-side-encryption"": ""AES256"" header. Enable server-side encryption with Amazon S3-managed encryption keys (SSE-S3) and rely on the built-in key rotation feature of the SSE-S3 encryption keys.",,,,다국적 은행은 기밀 파일을 S3 버킷에 저장하고 있습니다. 보안 팀은 최근 감사를 수행했으며 보고서에 따르면 256비트 AES(Advanced Encryption Standard) 서버 측 암호화 없이 여러 파일이 업로드된 것으로 나타났습니다. 추가 보호를 위해 암호화 키는 매년 자동으로 순환되어야 합니다. 솔루션 설계자는 향후 S3 버킷에 암호화되지 않은 다른 파일이 업로드되지 않도록 해야 합니다.다음 중 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 것은 무엇입니까?,"요청에 헤더가 포함되지 않은 경우 객체 업로드를 거부하는 S3 버킷에 대한 서비스 제어 정책(SCP)을 생성합니다 s3:x-amz-server-side-encryption"": ""AES256"". Amazon S3 관리형 암호화 키(SSE-S3)로 서버 측 암호화를 활성화하고 매년 키를 교체하도록 SSE-S3 암호화 키의 기본 제공 키 교체 기능을 수정합니다.","요청에 헤더가 포함되지 않은 경우 객체 업로드를 거부하는 S3 버킷에 대한 S3 버킷 정책을 생성합니다 s3:x-amz-server-side-encryption"":""aws:kms"". 모든 객체에 대해 규정 준수 모드에서 S3 객체 잠금을 활성화하여 버킷의 기본 제공 AES256 고객 관리형 키를 자동으로 교체합니다.",AWS Key Management Service(AWS KMS)에서 새 고객 관리형 키(CMK)를 생성합니다. 고객 관리형 키를 사용하도록 버킷의 기본 암호화 동작을 구성합니다. 매년 CMK를 수동으로 교체합니다.,"요청에 헤더가 포함되지 않은 경우 객체 업로드 권한을 거부하는 S3 버킷 정책을 생성합니다 s3:x-amz-server-side-encryption"": ""AES256"". Amazon S3 관리형 암호화 키(SSE-S3)로 서버 측 암호화를 활성화하고 SSE-S3 암호화 키의 기본 제공 키 교체 기능을 사용합니다.",,,0,,
udemy,CLF-01,221,"A hospital has a mission-critical application that uses a RESTful API powered by Amazon API Gateway and AWS Lambda. The medical officers upload PDF reports to the system which are then stored as static media content in an Amazon S3 bucket.The security team wants to improve its visibility when it comes to cyber-attacks and ensure HIPAA (Health Insurance Portability and Accountability Act) compliance. The company is searching for a solution that continuously monitors object-level S3 API operations and identifies protected health information (PHI) in the reports, with minimal changes in their existing Lambda function.Which of the following solutions will meet these requirements with the LEAST operational overhead?",A,A,Use Amazon Textract to extract the text from the PDF reports. Integrate Amazon Comprehend Medical with the existing Lambda function to identify the PHI from the extracted text.,Use Amazon Rekognition to extract the text data from the PDF reports. Integrate the Amazon Comprehend Medical service with the existing Lambda functions to identify the PHI from the extracted text.,Use Amazon Transcribe to read and analyze the PDF reports using the StartTranscriptionJob API operation. Use Amazon SageMaker Ground Truth to label and detect protected health information (PHI) content with low-confidence predictions.,Use Amazon Textract Medical with PII redaction turned on to extract and filter sensitive text from the PDF reports. Create a new Lambda function that calls the regular Amazon Comprehend API to identify the PHI from the extracted text.,,,,병원에는 Amazon API Gateway 및 AWS Lambda에서 제공하는 RESTful API를 사용하는 미션 크리티컬 애플리케이션이 있습니다. 의료진은 PDF 보고서를 시스템에 업로드한 다음 Amazon S3 버킷에 정적 미디어 콘텐츠로 저장합니다.보안 팀은 사이버 공격에 대한 가시성을 개선하고 HIPAA(Health Insurance Portability and Accountability Act) 규정을 준수하고자 합니다. 이 회사는 기존 Lambda 기능을 최소한으로 변경하면서 개체 수준 S3 API 작업을 지속적으로 모니터링하고 보고서에서 PHI(보호된 건강 정보)를 식별하는 솔루션을 찾고 있습니다.다음 중 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?,Amazon Textract를 사용하여 PDF 보고서에서 텍스트를 추출합니다. Amazon Comprehend Medical을 기존 Lambda 함수와 통합하여 추출된 텍스트에서 PHI를 식별합니다.,Amazon Rekognition을 사용하여 PDF 보고서에서 텍스트 데이터를 추출합니다. Amazon Comprehend Medical 서비스를 기존 Lambda 함수와 통합하여 추출된 텍스트에서 PHI를 식별합니다.,Amazon Transcribe를 사용하여 작업을 통해 PDF 보고서를 읽고 분석합니다 StartTranscriptionJob API. Amazon SageMaker Ground Truth를 사용하여 신뢰도가 낮은 예측으로 보호 건강 정보(PHI) 콘텐츠에 레이블을 지정하고 감지합니다.,PII 교정을 켠 상태로 Amazon Textract Medical을 사용하여 PDF 보고서에서 민감한 텍스트를 추출하고 필터링합니다. 일반 Amazon Comprehend API를 호출하여 추출된 텍스트에서 PHI를 식별하는 새 Lambda 함수를 생성합니다.,,,0,,
udemy,CLF-01,222,A startup launched a new FTP server using an On-Demand EC2 instance in a newly created VPC with default settings. The server should not be accessible publicly but only through the IP address 175.45.116.100 and nowhere else.Which of the following is the most suitable way to implement this requirement?,A,A,Create a new inbound rule in the security group of the EC2 instance with the following details: Protocol: TCP Port Range: 20 - 21 Source: 175.45.116.100/32,Create a new Network ACL inbound rule in the subnet of the EC2 instance with the following details: Protocol: TCP Port Range: 20 - 21 Source: 175.45.116.100/0Allow/Deny: ALLOW,Create a new Network ACL inbound rule in the subnet of the EC2 instance with the following details: Protocol: UDP Port Range: 20 - 21 Source: 175.45.116.100/0 Allow/Deny: ALLOW,Create a new inbound rule in the security group of the EC2 instance with the following details:Protocol: UDP Port Range: 20 - 21 Source: 175.45.116.100/32,,,,스타트업에서 새로 생성된 VPC에서 기본 설정으로 온디맨드 EC2 인스턴스를 사용하여 새 FTP 서버를 시작했습니다. 서버는 공개적으로 액세스할 수 없어야 하며 IP 주소를 통해서만 액세스할 수 있어야 합니다 175.45.116.100.다음 중 이 요구 사항을 구현하는 가장 적합한 방법은 무엇입니까?,다음 세부 정보를 사용하여 EC2 인스턴스의 보안 그룹에서 새 인바운드 규칙을 생성합니다.프로토콜: TCP포트 범위: 20 - 21원천:175.45.116.100/32,다음 세부 정보를 사용하여 EC2 인스턴스의 서브넷에서 새 네트워크 ACL 인바운드 규칙을 생성합니다.프로토콜: TCP포트 범위: 20 - 21원천:175.45.116.100/0허용/거부: 허용,다음 세부 정보를 사용하여 EC2 인스턴스의 서브넷에서 새 네트워크 ACL 인바운드 규칙을 생성합니다.프로토콜: UDP포트 범위: 20 - 21원천:175.45.116.100/0 허용/거부: 허용,다음 세부 정보를 사용하여 EC2 인스턴스의 보안 그룹에서 새 인바운드 규칙을 생성합니다.프로토콜: UDP포트 범위: 20 - 21원천:175.45.116.100/32,,,0,,
udemy,CLF-01,223,"A company recently launched an e-commerce application that is running in eu-east-2 region, which strictly requires six EC2 instances running at all times. In that region, there are 3 Availability Zones (AZ) that you can use - eu-east-2a, eu-east-2b, and eu-east-2c.Which of the following deployments provide 100% fault tolerance if any single AZ in the region becomes unavailable? (Select TWO.)",AD,AD,"eu-east-2a with six EC2 instances, eu-east-2b with six EC2 instances, and eu-east-2c with no EC2 instances","eu-east-2a with two EC2 instances, eu-east-2b with two EC2 instances, and eu-east-2c with two EC2 instances","eu-east-2a with four EC2 instances, eu-east-2b with two EC2 instances, and eu-east-2c with two EC2 instances","eu-east-2a with three EC2 instances, eu-east-2b with three EC2 instances, and eu-east-2c with three EC2 instances","eu-east-2a with two EC2 instances, eu-east-2b with four EC2 instances, and eu-east-2c with two EC2 instances",,,"한 회사는 최근 eu-east-2 지역에서 실행되는 전자 상거래 애플리케이션을 출시했으며, 이는 항상 실행 중인 6개의 EC2 인스턴스를 엄격히 요구합니다. 해당 리전에는 eu-east-2a, eu-east-2b, eu-east-2c 등 3개의 가용 영역(AZ)을 사용할 수 있습니다.다음 중 리전의 단일 AZ를 사용할 수 없게 될 경우 100% 내결함성을 제공하는 배포는 무엇입니까? (2개를 선택하세요.)","6개의 EC2 인스턴스가 있는 eu-east-2a, 6개의 EC2 인스턴스가 있는 eu-east-2b 및 EC2 인스턴스가 없는 eu-east-2c","2개의 EC2 인스턴스가 있는 eu-east-2a, 2개의 EC2 인스턴스가 있는 eu-east-2b 및 2개의 EC2 인스턴스가 있는 eu-east-2c","4개의 EC2 인스턴스가 있는 eu-east-2a, 2개의 EC2 인스턴스가 있는 eu-east-2b 및 2개의 EC2 인스턴스가 있는 eu-east-2c","3개의 EC2 인스턴스가 있는 eu-east-2a, 3개의 EC2 인스턴스가 있는 eu-east-2b 및 3개의 EC2 인스턴스가 있는 eu-east-2c","2개의 EC2 인스턴스가 있는 eu-east-2a, 4개의 EC2 인스턴스가 있는 eu-east-2b 및 2개의 EC2 인스턴스가 있는 eu-east-2c",,0,,
udemy,CLF-01,224,"A company is using the AWS Directory Service to integrate their on-premises Microsoft Active Directory (AD) domain with their Amazon EC2 instances via an AD connector. The below identity-based policy is attached to the IAM Identities that use the AWS Directory service:{ ""Version"":""2012-10-17"", ""Statement"":[  {   ""Sid"":""DirectoryTutorialsDojo1234"",   ""Effect"":""Allow"",   ""Action"":[    ""ds:*""   ],   ""Resource"":""arn:aws:ds:us-east-1:987654321012:directory/d-1234567890""  },  {   ""Effect"":""Allow"",   ""Action"":[   ""ec2:*""   ],   ""Resource"":""*""  } ]}Which of the following BEST describes what the above resource policy does?",A,A,Allows all AWS Directory Service (ds) calls as long as the resource contains the directory ID: d-1234567890,Allows all AWS Directory Service (ds) calls as long as the resource contains the directory ID: DirectoryTutorialsDojo1234,Allows all AWS Directory Service (ds) calls as long as the resource contains the directory ID:  987654321012,Allows all AWS Directory Service (ds) calls as long as the resource contains the directory name of: DirectoryTutorialsDojo1234,,,,"회사는 AWS Directory Service를 사용하여 온프레미스 Microsoft Active Directory(AD) 도메인을 AD 커넥터를 통해 Amazon EC2 인스턴스와 통합합니다. 아래 자격 증명 기반 정책은 AWS 디렉터리 서비스를 사용하는 IAM 자격 증명에 연결됩니다.{ ""버전"" : ""2012-10-17"" , ""문"" :[  {   ""시드"" : ""DirectoryTutorialsDojo1234"" ,   ""효과"" : ""허용"" ,   ""액션"" :[    ""ds:*""   ],   ""리소스"" : ""arn:aws:ds:us-east-1:987654321012:directory/d-1234567890""  },  {   ""효과"" : ""허용"" ,   ""액션"" :[   ""ec2:*""   ],   ""자원"" : ""*""  } ]}다음 중 위의 리소스 정책이 수행하는 작업을 가장 잘 설명하는 것은 무엇입니까?",ds리소스에 디렉터리 ID가 포함되어 있는 한 모든 AWS 디렉터리 서비스( ) 호출을 허용합니다 .d-1234567890,ds리소스에 디렉터리 ID가 포함되어 있는 한 모든 AWS 디렉터리 서비스( ) 호출을 허용합니다 .DirectoryTutorialsDojo1234,ds리소스에 디렉터리 ID가 포함되어 있는 한   모든 AWS 디렉터리 서비스( ) 호출을 허용합니다 .987654321012,ds리소스에 다음 디렉터리 이름이 포함되어 있는 한 모든 AWS Directory Service( ) 호출을 허용합니다 .DirectoryTutorialsDojo1234,,,0,,
udemy,CLF-01,225,A company plans to migrate a NoSQL database to an EC2 instance. The database is configured to replicate the data automatically to keep multiple copies of data for redundancy. The Solutions Architect needs to launch an instance that has a high IOPS and sequential read/write access.Which of the following options fulfills the requirement if I/O throughput is the highest priority?,A,A,Use Storage optimized instances with instance store volume.,Use Compute optimized instance with instance store volume.,Use Memory optimized instances with EBS volume.,Use General purpose instances with EBS volume.,,,,회사에서 NoSQL 데이터베이스를 EC2 인스턴스로 마이그레이션할 계획입니다. 데이터베이스는 중복성을 위해 여러 데이터 복사본을 유지하기 위해 자동으로 데이터를 복제하도록 구성됩니다. Solutions Architect는 IOPS가 높고 순차 읽기/쓰기 액세스 권한이 있는 인스턴스를 시작해야 합니다.다음 중 I/O 처리량이 가장 높은 우선 순위인 경우 요구 사항을 충족하는 옵션은 무엇입니까?,인스턴스 스토어 볼륨이 있는 스토리지 최적화 인스턴스를 사용합니다.,인스턴스 스토어 볼륨이 있는 컴퓨팅 최적화 인스턴스를 사용합니다.,EBS 볼륨과 함께 메모리 최적화 인스턴스를 사용합니다.,EBS 볼륨과 함께 범용 인스턴스를 사용합니다.,,,0,,
udemy,CLF-01,226,"A web application requires a minimum of six Amazon Elastic Compute Cloud (EC2) instances running at all times. You are tasked to deploy the application to three availability zones in the EU Ireland region (eu-west-1a, eu-west-1b, and eu-west-1c). It is required that the system is fault-tolerant up to the loss of one Availability Zone. Which of the following setup is the most cost-effective solution which also maintains the fault-tolerance of your system?",A,A,"3 instances in eu-west-1a, 3 instances in eu-west-1b, and 3 instances in eu-west-1c","6 instances in eu-west-1a, 6 instances in eu-west-1b, and no instances in eu-west-1c","2 instances in eu-west-1a, 2 instances in eu-west-1b, and 2 instances in eu-west-1c","6 instances in eu-west-1a, 6 instances in eu-west-1b, and 6 instances in eu-west-1c",,,,"웹 애플리케이션에는 항상 실행되는 최소 6개의 Amazon Elastic Compute Cloud(EC2) 인스턴스가 필요합니다. 귀하는 EU 아일랜드 리전(eu-west-1a, eu-west-1b 및 eu-west-1c)의 3개 가용 영역에 애플리케이션을 배포해야 합니다. 시스템은 하나의 가용 영역이 손실될 때까지 내결함성이 있어야 합니다.다음 설정 중 시스템의 내결함성을 유지하는 가장 비용 효율적인 솔루션은 무엇입니까?","eu-west-1a의 인스턴스 3개, eu-west-1b의 인스턴스 3개 및 eu-west-1c의 인스턴스 3개","eu-west-1a의 인스턴스 6개, eu-west-1b의 인스턴스 6개, eu-west-1c의 인스턴스 없음","eu-west-1a에 인스턴스 2개, eu-west-1b에 인스턴스 2개, eu-west-1c에 인스턴스 2개","eu-west-1a의 인스턴스 6개, eu-west-1b의 인스턴스 6개 및 eu-west-1c의 인스턴스 6개",,,0,,
udemy,CLF-01,227,"A company launched a global news website that is deployed to AWS and is using MySQL RDS. The website has millions of viewers from all over the world, which means that the website has a read-heavy database workload. All database transactions must be ACID compliant to ensure data integrity.In this scenario, which of the following is the best option to use to increase the read-throughput on the MySQL database?",B,B,Enable Amazon RDS Standby Replicas,Enable Amazon RDS Read Replicas,Use SQS to queue up the requests,Enable Multi-AZ deployments,,,,한 회사에서 AWS에 배포되고 MySQL RDS를 사용하는 글로벌 뉴스 웹 사이트를 시작했습니다. 이 웹사이트에는 전 세계 수백만 명의 시청자가 있습니다. 이는 웹사이트에 읽기가 많은 데이터베이스 워크로드가 있음을 의미합니다. 모든 데이터베이스 트랜잭션은 데이터 무결성을 보장하기 위해 ACID를 준수해야 합니다.이 시나리오에서 다음 중 MySQL 데이터베이스의 읽기 처리량을 늘리기 위해 사용할 수 있는 가장 좋은 옵션은 무엇입니까?,Amazon RDS 대기 복제본 활성화,Amazon RDS 읽기 전용 복제본 활성화,SQS를 사용하여 요청 대기열에 넣기,다중 AZ 배포 활성화,,,0,,
udemy,CLF-01,228,"A company deployed an online enrollment system database on a prestigious university, which is hosted in RDS. The Solutions Architect is required to monitor the database metrics in Amazon CloudWatch to ensure the availability of the enrollment system.What are the enhanced monitoring metrics that Amazon CloudWatch gathers from Amazon RDS DB instances which provide more accurate information? (Select TWO.)",BD,BD,CPU Utilization,OS processes,Database Connections,RDS child processes.,Freeable Memory,,,한 회사는 RDS에서 호스팅되는 명문 대학에 온라인 등록 시스템 데이터베이스를 배포했습니다. Solutions Architect는 등록 시스템의 가용성을 보장하기 위해 Amazon CloudWatch에서 데이터베이스 지표를 모니터링해야 합니다.보다 정확한 정보를 제공하는 Amazon CloudWatch가 Amazon RDS DB 인스턴스에서 수집하는 향상된 모니터링 지표는 무엇입니까? (2개를 선택하세요.),CPU 사용률,OS 프로세스,데이터베이스 연결,RDS 하위 프로세스.,사용 가능한 메모리,,0,,
udemy,CLF-01,229,A company has a web-based order processing system that is currently using a standard queue in Amazon SQS. The IT Manager noticed that there are a lot of cases where an order was processed twice. This issue has caused a lot of trouble in processing and made the customers very unhappy. The manager has asked you to ensure that this issue will not recur.What can you do to prevent this from happening again in the future? (Select TWO.),CD,CD,Change the message size in SQS.,Alter the retention period in Amazon SQS.,Use an Amazon SQS FIFO Queue instead.,"Replace Amazon SQS and instead, use Amazon Simple Workflow service.",Alter the visibility timeout of SQS.,,,회사에는 현재 Amazon SQS의 표준 대기열을 사용하는 웹 기반 주문 처리 시스템이 있습니다. IT 담당자는 주문이 두 번 처리되는 경우가 많다는 사실을 알게 되었습니다. 이 문제는 처리에 많은 문제를 일으켜 고객을 매우 불행하게 만들었습니다. 관리자는 이 문제가 다시 발생하지 않도록 확인을 요청했습니다.앞으로 이런 일이 다시 발생하지 않도록 하려면 어떻게 해야 합니까? (2개를 선택하세요.),SQS에서 메시지 크기를 변경합니다.,Amazon SQS에서 보존 기간을 변경합니다.,대신 Amazon SQS FIFO 대기열을 사용하십시오.,Amazon SQS를 교체하고 대신 Amazon Simple Workflow 서비스를 사용하십시오.,SQS의 가시성 제한 시간을 변경합니다.,,0,,
udemy,CLF-01,230,"An Auto Scaling group (ASG) of Linux EC2 instances has an Amazon FSx for OpenZFS file system with basic monitoring enabled in CloudWatch. The Solutions Architect noticed that the legacy web application hosted in the ASG takes a long time to load. After checking the instances, the Architect noticed that the ASG is not launching more instances as it should be, even though the servers already have high memory usage.Which of the following options should the Architect implement to solve this issue?",D,D,Set up Amazon Rekognition to automatically identify and recognize the cause of the high memory usage. Use the AWS Well-Architected Tool to automatically trigger the scale-out event in the ASG based on the overall memory usage.,Enable detailed monitoring on the Amazon EC2 instances of the Auto Scaling group. Use Amazon Forecast to automatically scale out the Auto Scaling group based on the aggregated memory usage of Amazon EC2 instances.,Implement an AI solution that leverages Amazon Comprehend to track the near-real-time memory usage of each and every EC2 instance? Use Amazon SageMaker to automatically trigger the Auto Scaling event if there is high memory usage.,Install the CloudWatch unified agent to the EC2 instances. Set up a custom parameter in AWS Systems Manager Parameter Store with the CloudWatch agent configuration to create an aggregated metric on memory usage percentage. Scale the Auto Scaling group based on the aggregated metric.,,,,Linux EC2 인스턴스의 Auto Scaling 그룹(ASG)에는 CloudWatch에서 기본 모니터링이 활성화된 OpenZFS용 Amazon FSx 파일 시스템이 있습니다. Solutions Architect는 ASG에서 호스팅되는 레거시 웹 애플리케이션을 로드하는 데 시간이 오래 걸린다는 사실을 알게 되었습니다. 인스턴스를 확인한 후 Architect는 서버가 이미 높은 메모리 사용량을 가지고 있음에도 불구하고 ASG가 더 많은 인스턴스를 시작해야 하는 대로 시작하지 않는다는 것을 알았습니다.Architect가 이 문제를 해결하기 위해 구현해야 하는 옵션은 다음 중 무엇입니까?,높은 메모리 사용량의 원인을 자동으로 식별하고 인식하도록 Amazon Rekognition을 설정합니다. AWS Well-Architected Tool을 사용하여 전체 메모리 사용량에 따라 ASG에서 확장 이벤트를 자동으로 트리거합니다.,Auto Scaling 그룹의 Amazon EC2 인스턴스에 대한 자세한 모니터링을 활성화합니다. Amazon Forecast를 사용하여 Amazon EC2 인스턴스의 집계된 메모리 사용량을 기반으로 Auto Scaling 그룹을 자동으로 확장합니다.,Amazon Comprehend를 활용하여 각 EC2 인스턴스의 거의 실시간 메모리 사용량을 추적하는 AI 솔루션을 구현하시겠습니까? 메모리 사용량이 많은 경우 Amazon SageMaker를 사용하여 Auto Scaling 이벤트를 자동으로 트리거하십시오.,EC2 인스턴스에 CloudWatch 통합 에이전트를 설치합니다. CloudWatch 에이전트 구성을 사용하여 AWS Systems Manager Parameter Store에서 사용자 지정 파라미터를 설정하여 메모리 사용률에 대한 집계 지표를 생성합니다. 집계된 지표를 기반으로 Auto Scaling 그룹을 조정합니다.,,,0,,
udemy,CLF-01,231,"A Solutions Architect is designing the cloud architecture for the enterprise application suite of the company. Both the web and application tiers need to access the Internet to fetch data from public APIs. However, these servers should be inaccessible from the Internet. Which of the following steps should the Architect implement to meet the above requirements?",C,C,Deploy the web and application tier instances to a private subnet and then allocate an Elastic IP address to each EC2 instance.,Deploy a NAT gateway in the private subnet and add a route to it from the public subnet where the web and application tiers are hosted.,Deploy a NAT gateway in the public subnet and add a route to it from the private subnet where the web and application tiers are hosted.,Deploy the web and application tier instances to a public subnet and then allocate an Elastic IP address to each EC2 instance.,,,,Solutions Architect는 회사의 엔터프라이즈 애플리케이션 제품군을 위한 클라우드 아키텍처를 설계하고 있습니다. 웹 및 애플리케이션 계층 모두 공용 API에서 데이터를 가져오기 위해 인터넷에 액세스해야 합니다. 그러나 이러한 서버는 인터넷에서 액세스할 수 없어야 합니다.위의 요구 사항을 충족하기 위해 Architect가 구현해야 하는 단계는 다음 중 무엇입니까?,웹 및 애플리케이션 계층 인스턴스를 프라이빗 서브넷에 배포한 다음 탄력적 IP 주소를 각 EC2 인스턴스에 할당합니다.,프라이빗 서브넷에 NAT 게이트웨이를 배포하고 웹 및 애플리케이션 계층이 호스팅되는 퍼블릭 서브넷에서 경로를 추가합니다.,퍼블릭 서브넷에 NAT 게이트웨이를 배포하고 웹 및 애플리케이션 계층이 호스팅되는 프라이빗 서브넷에서 경로를 추가합니다.,웹 및 애플리케이션 계층 인스턴스를 퍼블릭 서브넷에 배포한 다음 탄력적 IP 주소를 각 EC2 인스턴스에 할당합니다.,,,0,,
udemy,CLF-01,232,A Solutions Architect is designing a setup for a database that will run on Amazon RDS for MySQL. He needs to ensure that the database can automatically failover to an RDS instance to continue operating in the event of failure. The architecture should also be as highly available as possible.Which among the following actions should the Solutions Architect do?,C,C,"Create a read replica in the same region where the DB instance resides. In addition, create a read replica in a different region to survive a region’s failure. In the event of an Availability Zone outage, promote any replica to become the primary instance.","Create five cross-region read replicas in each region. In the event of an Availability Zone outage, promote any replica to become the primary instance.",Create a standby replica in another availability zone by enabling Multi-AZ deployment.,"Create five read replicas across different availability zones. In the event of an Availability Zone outage, promote any replica to become the primary instance.",,,,Solutions Architect는 MySQL용 Amazon RDS에서 실행할 데이터베이스에 대한 설정을 설계하고 있습니다. 그는 데이터베이스가 장애 발생 시 계속 작동하도록 RDS 인스턴스로 자동 장애 조치할 수 있는지 확인해야 합니다. 또한 아키텍처는 가능한 한 가용성이 높아야 합니다.솔루션 아키텍트는 다음 중 어떤 조치를 취해야 합니까?,DB 인스턴스가 상주하는 동일한 리전에 읽기 전용 복제본을 생성합니다. 또한 다른 지역에 읽기 전용 복제본을 생성하여 한 지역의 장애가 지속되도록 합니다. 가용 영역이 중단된 경우 모든 복제본을 기본 인스턴스로 승격합니다.,각 리전에 5개의 리전 간 읽기 전용 복제본을 만듭니다. 가용 영역이 중단된 경우 모든 복제본을 기본 인스턴스로 승격합니다.,다중 AZ 배포를 활성화하여 다른 가용 영역에 대기 복제본을 생성합니다.,서로 다른 가용 영역에 걸쳐 5개의 읽기 전용 복제본을 만듭니다. 가용 영역이 중단된 경우 모든 복제본을 기본 인스턴스로 승격합니다.,,,0,,
udemy,CLF-01,233,"An automotive company is working on an autonomous vehicle development and deployment project using AWS. The solution requires High Performance Computing (HPC) in order to collect, store and manage massive amounts of data as well as to support deep learning frameworks. The Linux EC2 instances that will be used should have a lower latency and higher throughput than the TCP transport traditionally used in cloud-based HPC systems. It should also enhance the performance of inter-instance communication and must include an OS-bypass functionality to allow the HPC to communicate directly with the network interface hardware to provide low-latency, reliable transport functionality. Which of the following is the MOST suitable solution that you should implement to achieve the above requirements?",B,B,Attach an Elastic Network Interface (ENI) on each Amazon EC2 instance to accelerate High Performance Computing (HPC).,Attach an Elastic Fabric Adapter (EFA) on each Amazon EC2 instance to accelerate High Performance Computing (HPC).,Attach an Elastic Network Adapter (ENA) on each Amazon EC2 instance to accelerate High Performance Computing (HPC).,Attach a Private Virtual Interface (VIF) on each Amazon EC2 instance to accelerate High Performance Computing (HPC).,,,,"자동차 회사는 AWS를 사용하여 자율 주행 차량 개발 및 배포 프로젝트를 진행하고 있습니다. 이 솔루션에는 방대한 양의 데이터를 수집, 저장 및 관리하고 딥 러닝 프레임워크를 지원하기 위해 고성능 컴퓨팅(HPC)이 필요합니다. 사용할 Linux EC2 인스턴스는 클라우드 기반 HPC 시스템에서 전통적으로 사용되는 TCP 전송보다 대기 시간이 짧고 처리량이 높아야 합니다. 또한 인스턴스 간 통신의 성능을 향상시키고 HPC가 네트워크 인터페이스 하드웨어와 직접 통신하여 대기 시간이 짧고 안정적인 전송 기능을 제공할 수 있도록 OS 바이패스 기능을 포함해야 합니다.다음 중 위의 요구 사항을 달성하기 위해 구현해야 하는 가장 적합한 솔루션은 무엇입니까?",각 Amazon EC2 인스턴스에 ENI(Elastic Network Interface)를 연결하여 고성능 컴퓨팅(HPC)을 가속화합니다.,각 Amazon EC2 인스턴스에 Elastic Fabric Adapter(EFA)를 연결하여 고성능 컴퓨팅(HPC)을 가속화합니다.,각 Amazon EC2 인스턴스에 ENA(Elastic Network Adapter)를 연결하여 고성능 컴퓨팅(HPC)을 가속화합니다.,고성능 컴퓨팅(HPC)을 가속화하기 위해 각 Amazon EC2 인스턴스에 프라이빗 가상 인터페이스(VIF)를 연결합니다.,,,0,,
udemy,CLF-01,234,A tech company currently has an on-premises infrastructure. They are currently running low on storage and want to have the ability to extend their storage using the AWS cloud.Which AWS service can help them achieve this requirement?,D,D,Amazon SQS,Amazon EC2,Amazon Elastic Block Storage,Amazon Storage Gateway,,,,기술 회사는 현재 온프레미스 인프라를 보유하고 있습니다. 그들은 현재 스토리지가 부족하고 AWS 클라우드를 사용하여 스토리지를 확장할 수 있기를 원합니다.이 요구 사항을 달성하는 데 도움이 되는 AWS 서비스는 무엇입니까?,아마존 SQS,아마존 EC2,Amazon 탄력적 블록 스토리지,아마존 스토리지 게이트웨이,,,0,,
udemy,CLF-01,235,A company has a set of Linux servers running on multiple On-Demand EC2 Instances. The Audit team wants to collect and process the application log files generated from these servers for their report.Which of the following services is best to use in this case?,D,D,A single On-Demand Amazon EC2 instance for both storing and processing the log files,Amazon S3 Glacier Deep Archive for storing the application log files and AWS ParallelCluster for processing the log files.,Amazon S3 Glacier for storing the application log files and Spot EC2 Instances for processing them.,Amazon S3 for storing the application log files and Amazon Elastic MapReduce for processing the log files.,,,,한 회사에 여러 온디맨드 EC2 인스턴스에서 실행되는 Linux 서버 세트가 있습니다. 감사 팀은 보고를 위해 이러한 서버에서 생성된 응용 프로그램 로그 파일을 수집하고 처리하려고 합니다.이 경우 다음 중 어떤 서비스를 사용하는 것이 가장 좋습니까?,로그 파일 저장 및 처리를 위한 단일 온디맨드 Amazon EC2 인스턴스,애플리케이션 로그 파일을 저장하기 위한 Amazon S3 Glacier Deep Archive 및 로그 파일 처리를 위한 AWS ParallelCluster.,애플리케이션 로그 파일을 저장하기 위한 Amazon S3 Glacier와 이를 처리하기 위한 스팟 EC2 인스턴스.,애플리케이션 로그 파일을 저장하기 위한 Amazon S3 및 로그 파일 처리를 위한 Amazon Elastic MapReduce.,,,0,,
udemy,CLF-01,236,"A company plans to develop a custom messaging service that will also be used to train their AI for an automatic response feature which they plan to implement in the future. Based on their research and tests, the service can receive up to thousands of messages a day, and all of these data are to be sent to Amazon EMR for further processing. It is crucial that none of the messages are lost, no duplicates are produced, and that they are processed in EMR in the same order as their arrival.Which of the following options can satisfy the given requirement?",A,A,Create an Amazon Kinesis Data Stream to collect the messages.,Set up a default Amazon SQS queue to handle the messages.,Create a pipeline using AWS Data Pipeline to handle the messages.,Set up an Amazon SNS Topic to handle the messages.,,,,회사는 향후 구현할 자동 응답 기능에 대해 AI를 교육하는 데에도 사용할 맞춤형 메시징 서비스를 개발할 계획입니다. 연구 및 테스트를 기반으로 서비스는 하루에 최대 수천 개의 메시지를 수신할 수 있으며 이러한 모든 데이터는 추가 처리를 위해 Amazon EMR로 전송됩니다. 메시지가 손실되지 않고 중복이 생성되지 않으며 도착한 순서대로 EMR에서 처리되는 것이 중요합니다.다음 옵션 중 주어진 요구 사항을 충족할 수 있는 옵션은 무엇입니까?,메시지를 수집할 Amazon Kinesis Data Stream을 생성합니다.,메시지를 처리할 기본 Amazon SQS 대기열을 설정합니다.,메시지를 처리하기 위해 AWS Data Pipeline을 사용하여 파이프라인을 생성합니다.,메시지를 처리할 Amazon SNS 주제를 설정합니다.,,,0,,
udemy,CLF-01,237,An organization plans to use an AWS Direct Connect connection to establish a dedicated connection between its on-premises network and AWS. The organization needs to launch a fully managed solution that will automate and accelerate the replication of data to and from various AWS storage services.Which of the following solutions would you recommend?,A,A,Use an AWS DataSync agent to rapidly move the data over a service endpoint.,Use an AWS DataSync agent to rapidly move the data over the Internet.,Use an AWS Storage Gateway file gateway to store and retrieve files directly using the SMB file system protocol.,Use an AWS Storage Gateway tape gateway to store data on virtual tape cartridges and asynchronously copy your backups to AWS.,,,,조직에서 AWS Direct Connect 연결을 사용하여 온프레미스 네트워크와 AWS 간의 전용 연결을 설정할 계획입니다. 조직은 다양한 AWS 스토리지 서비스와의 데이터 복제를 자동화하고 가속화하는 완전관리형 솔루션을 출시해야 합니다.다음 중 어떤 솔루션을 권장하시겠습니까?,AWS DataSync 에이전트를 사용하여 서비스 엔드포인트를 통해 데이터를 빠르게 이동합니다.,AWS DataSync 에이전트를 사용하여 인터넷을 통해 데이터를 빠르게 이동합니다.,AWS Storage Gateway 파일 게이트웨이를 사용하여 SMB 파일 시스템 프로토콜을 사용하여 직접 파일을 저장하고 검색합니다.,AWS Storage Gateway 테이프 게이트웨이를 사용하여 가상 테이프 카트리지에 데이터를 저장하고 백업을 AWS에 비동기식으로 복사합니다.,,,0,,
udemy,CLF-01,238,A food company bought 50 licenses of Windows Server to be used by the developers when launching Amazon EC2 instances to deploy and test applications. The developers are free to provision EC2 instances as long as there is a license available. The licenses are tied to the total CPU count of each virtual machine. The company wants to ensure that developers won’t be able to launch new instances once the licenses are exhausted. The company wants to receive notifications when all licenses are in use.Which of the following options is the recommended solution to meet the company's requirements?,A,A,Define licensing rules on AWS License Manager to track and control license usage. Enable the option to “Enforce license limit” to prevent going over the number of allocated licenses. Add an Amazon SNS topic to send notifications and alerts.,Configure AWS Resource Access Manager (AWS RAM) to track and control the licenses used by AWS resources. Configure AWS RAM to provide available licenses for Amazon EC2 instances. Set up an Amazon SNS to send notifications and alerts once all licenses are used.,Upload the licenses on AWS Systems Manager Fleet Manager to be encrypted and distributed to Amazon EC2 instances. Attach an IAM role on the EC2 instances to request a license from the Fleet Manager. Set up an Amazon SNS to send notifications and alerts once all licenses are used,Define license configuration rules on AWS Certificate Manager to track and control license usage. Enable the option to “Enforce certificate limit” to prevent going over the number of allocated licenses. Add an Amazon SQS queue with ChangeVisibility Timeout configured to send notifications and alerts.,,,,한 식품 회사는 애플리케이션을 배포하고 테스트하기 위해 Amazon EC2 인스턴스를 시작할 때 개발자가 사용할 Windows Server 라이선스 50개를 구입했습니다. 개발자는 사용 가능한 라이선스가 있는 한 EC2 인스턴스를 자유롭게 프로비저닝할 수 있습니다. 라이센스는 각 가상 머신의 총 CPU 수에 연결됩니다. 회사는 라이선스가 소진되면 개발자가 새 인스턴스를 시작할 수 없도록 하려고 합니다. 회사는 모든 라이선스가 사용 중일 때 알림을 받기를 원합니다.다음 옵션 중 회사의 요구 사항을 충족하기 위해 권장되는 솔루션은 무엇입니까?,"AWS License Manager에서 라이선스 규칙을 정의하여 라이선스 사용을 추적하고 제어합니다. 할당된 라이센스 수를 초과하지 않도록 ""Enforce license limit(라이선스 제한 적용)"" 옵션을 활성화합니다. 알림 및 경고를 보낼 Amazon SNS 주제를 추가합니다.",AWS 리소스에서 사용하는 라이선스를 추적하고 제어하도록 AWS RAM(AWS Resource Access Manager)을 구성합니다. Amazon EC2 인스턴스에 사용 가능한 라이선스를 제공하도록 AWS RAM을 구성합니다. 모든 라이선스가 사용되면 알림 및 경고를 보내도록 Amazon SNS를 설정합니다.,AWS Systems Manager Fleet Manager에 라이선스를 업로드하여 암호화하고 Amazon EC2 인스턴스에 배포합니다. Fleet Manager에서 라이선스를 요청하려면 EC2 인스턴스에 IAM 역할을 연결합니다. 모든 라이선스가 사용되면 알림 및 경고를 보내도록 Amazon SNS 설정,"AWS Certificate Manager에서 라이선스 구성 규칙을 정의하여 라이선스 사용을 추적하고 제어합니다. 할당된 라이선스 수를 초과하지 않도록 ""인증서 제한 적용"" 옵션을 활성화합니다. 알림 및 경고를 보내도록 ChangeVisibility Timeout이 구성된 Amazon SQS 대기열을 추가합니다.",,,0,,
udemy,CLF-01,239,"A Solutions Architect is working for a large global media company with multiple office locations all around the world. The Architect is instructed to build a system to distribute training videos to all employees.Using CloudFront, what method would be used to serve content that is stored in S3, but not publicly accessible from S3 directly?",B,B,Create an S3 bucket policy that lists the CloudFront distribution ID as the principal and the target bucket as the Amazon Resource Name (ARN).,Create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket to that OAI.,Create a web ACL in AWS WAF to block any public S3 access and attach it to the Amazon CloudFront distribution.,Create an Identity and Access Management (IAM) user for CloudFront and grant access to the objects in your S3 bucket to that IAM user.,,,,A Solutions Architect는 전 세계 여러 곳에 사무실이 있는 대규모 글로벌 미디어 회사에서 근무하고 있습니다. Architect는 모든 직원에게 교육 비디오를 배포하는 시스템을 구축하라는 지시를 받았습니다.CloudFront를 사용하여 S3에 저장되어 있지만 S3에서 직접 공개적으로 액세스할 수 없는 콘텐츠를 제공하는 데 사용되는 방법은 무엇입니까?,CloudFront 배포 ID를 보안 주체로 나열하고 대상 버킷을 Amazon 리소스 이름(ARN)으로 나열하는 S3 버킷 정책을 생성합니다.,CloudFront용 OAI(Origin Access Identity)를 생성하고 해당 OAI에 S3 버킷의 객체에 대한 액세스 권한을 부여합니다.,AWS WAF에서 웹 ACL을 생성하여 퍼블릭 S3 액세스를 차단하고 Amazon CloudFront 배포에 연결합니다.,CloudFront용 IAM(Identity and Access Management) 사용자를 생성하고 해당 IAM 사용자에게 S3 버킷의 객체에 대한 액세스 권한을 부여합니다.,,,0,,
udemy,CLF-01,240,"A company has a fixed set of Amazon EC2 instances inside a VPC in the AWS cloud. The instances run a mission-critical application. In a recent incident, one of the EC2 instances suddenly powered down which affected the availability of the application. To avoid this incident in the future, the management wants to get notified of any upcoming AWS events that may affect these EC2 instances.Which of the following options is the recommended action to meet the above requirements?",A,A,"Create an Amazon EventBridge (Amazon CloudWatch Events) rule to check for AWS Personal Health Dashboard events that are related to Amazon EC2 instances. To send notifications, set an Amazon SNS topic as a target for the rule.","Set up an Amazon EventBridge (Amazon CloudWatch Events) rule to check for AWS Service Health Dashboard events that are related to Amazon EC2 instances. To send notifications, set an Amazon SNS topic as a target for the rule.",Create an Amazon EventBridge (Amazon CloudWatch Events) rule that is scheduled to run every 24 hours. Set the target to an AWS Lambda function that will check AWS Service Health Dashboard and send notifications for any events that may affect Amazon EC2 instances.,Set up an Amazon EventBridge (Amazon CloudWatch Events) rule to check for any status change for Amazon EC2 instances. Set the target to an AWS Lambda function that will send a notification and restart the affected Amazon EC2 instances.,,,,회사는 AWS 클라우드의 VPC 내부에 고정된 Amazon EC2 인스턴스 세트를 가지고 있습니다. 인스턴스는 미션 크리티컬 애플리케이션을 실행합니다. 최근 사건에서 EC2 인스턴스 중 하나의 전원이 갑자기 꺼져 애플리케이션의 가용성에 영향을 미쳤습니다. 향후 이 사고를 방지하기 위해 경영진은 이러한 EC2 인스턴스에 영향을 미칠 수 있는 예정된 AWS 이벤트에 대한 알림을 받기를 원합니다.다음 중 위의 요구 사항을 충족하기 위해 권장되는 조치는 무엇입니까?,Amazon EC2 인스턴스와 관련된 AWS Personal Health Dashboard 이벤트를 확인하는 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다. 알림을 보내려면 Amazon SNS 주제를 규칙의 대상으로 설정합니다.,Amazon EC2 인스턴스와 관련된 AWS 서비스 상태 대시보드 이벤트를 확인하도록 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 설정합니다. 알림을 보내려면 Amazon SNS 주제를 규칙의 대상으로 설정합니다.,24시간마다 실행되도록 예약된 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다. 대상을 AWS 서비스 상태 대시보드를 확인하고 Amazon EC2 인스턴스에 영향을 줄 수 있는 모든 이벤트에 대한 알림을 보내는 AWS Lambda 함수로 설정합니다.,Amazon EC2 인스턴스의 상태 변경을 확인하도록 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 설정합니다. 알림을 보내고 영향을 받는 Amazon EC2 인스턴스를 다시 시작하는 AWS Lambda 함수로 대상을 설정합니다.,,,0,,
udemy,CLF-01,241,A large multinational investment bank has a web application that requires a minimum of 4 EC2 instances to run to ensure that it can cater to its users across the globe. You are instructed to ensure fault tolerance of this system.Which of the following is the best option?,C,C,Deploy an Auto Scaling group with 1 instance in each of 4 Availability Zones behind an Application Load Balancer.,Deploy an Auto Scaling group with 2 instances in each of 2 Availability Zones behind an Application Load Balancer.,Deploy an Auto Scaling group with 2 instances in each of 3 Availability Zones behind an Application Load Balancer.,Deploy  an Auto Scaling group with 4 instances in one Availability Zone behind an Application Load Balancer.,,,,대규모 다국적 투자 은행에는 전 세계 사용자를 수용할 수 있도록 실행하는 데 최소 4개의 EC2 인스턴스가 필요한 웹 애플리케이션이 있습니다. 이 시스템의 내결함성을 확인하라는 지시를 받았습니다.다음 중 최선의 선택은 무엇입니까?,Application Load Balancer 뒤에 있는 4개의 가용 영역 각각에 1개의 인스턴스가 있는 Auto Scaling 그룹을 배포합니다.,Application Load Balancer 뒤에 있는 2개의 가용 영역 각각에 2개의 인스턴스가 있는 Auto Scaling 그룹을 배포합니다.,Application Load Balancer 뒤에 있는 3개의 가용 영역 각각에 2개의 인스턴스가 있는 Auto Scaling 그룹을 배포합니다.,Application Load Balancer 뒤에 있는 하나의 가용 영역에 4개의 인스턴스가 있는 Auto Scaling 그룹을 배포합니다.,,,0,,
udemy,CLF-01,242,"A startup plans to develop a multiplayer game that uses UDP as the protocol for communication between clients and game servers. The data of the users will be stored in a key-value store. As the Solutions Architect, you need to implement a solution that will distribute the traffic across a number of servers.Which of the following could help you achieve this requirement?",D,D,Distribute the traffic using Network Load Balancer and store the data in Amazon Aurora.,Distribute the traffic using Application Load Balancer and store the data in Amazon DynamoDB.,Distribute the traffic using Application Load Balancer and store the data in Amazon RDS.,Distribute the traffic using Network Load Balancer and store the data in Amazon DynamoDB.,,,,한 스타트업이 클라이언트와 게임 서버 간의 통신을 위한 프로토콜로 UDP를 사용하는 멀티플레이어 게임을 개발할 계획입니다. 사용자의 데이터는 키-값 저장소에 저장됩니다. Solutions Architect는 트래픽을 여러 서버에 분산시키는 솔루션을 구현해야 합니다.다음 중 이 요건을 충족하는 데 도움이 되는 것은 무엇입니까?,Network Load Balancer를 사용하여 트래픽을 분산하고 Amazon Aurora에 데이터를 저장합니다.,Application Load Balancer를 사용하여 트래픽을 분산하고 Amazon DynamoDB에 데이터를 저장합니다.,Application Load Balancer를 사용하여 트래픽을 분산하고 Amazon RDS에 데이터를 저장합니다.,Network Load Balancer를 사용하여 트래픽을 분산하고 Amazon DynamoDB에 데이터를 저장합니다.,,,0,,
udemy,CLF-01,243,"A company has hundreds of VPCs with multiple VPN connections to their data centers spanning 5 AWS Regions. As the number of its workloads grows, the company must be able to scale its networks across multiple accounts and VPCs to keep up. A Solutions Architect is tasked to interconnect all of the company's on-premises networks, VPNs, and VPCs into a single gateway, which includes support for inter-region peering across multiple AWS regions.Which of the following is the BEST solution that the architect should set up to support the required interconnectivity?",C,C,Enable inter-region VPC peering that allows peering relationships to be established between multiple VPCs across different AWS regions. Set up a networking configuration that ensures that the traffic will always stay on the global AWS backbone and never traverse the public Internet.,"Set up an AWS VPN CloudHub for inter-region VPC access and a Direct Connect gateway for the VPN connections to the on-premises data centers. Create a virtual private gateway in each VPC, then create a private virtual interface for each AWS Direct Connect connection to the Direct Connect gateway.","Set up an AWS Transit Gateway in each region to interconnect all networks within it. Then, route traffic between the transit gateways through a peering connection.","Set up an AWS Direct Connect Gateway to achieve inter-region VPC access to all of the AWS resources and on-premises data centers. Set up a link aggregation group (LAG) to aggregate multiple connections at a single AWS Direct Connect endpoint in order to treat them as a single, managed connection. Launch a virtual private gateway in each VPC and then create a public virtual interface for each AWS Direct Connect connection to the Direct Connect Gateway.",,,,"회사에는 5개의 AWS 리전에 걸쳐 있는 데이터 센터에 대한 여러 VPN 연결이 있는 수백 개의 VPC가 있습니다. 워크로드 수가 증가함에 따라 회사는 이를 따라잡기 위해 여러 계정과 VPC에 걸쳐 네트워크를 확장할 수 있어야 합니다. Solutions Architect는 회사의 모든 온프레미스 네트워크, VPN 및 VPC를 여러 AWS 리전 간 리전 간 피어링 지원을 포함하는 단일 게이트웨이로 상호 연결하는 임무를 맡고 있습니다.다음 중 필요한 상호 연결을 지원하기 위해 설계자가 설정해야 하는 최상의 솔루션은 무엇입니까?",서로 다른 AWS 리전에 있는 여러 VPC 간에 피어링 관계를 설정할 수 있는 리전 간 VPC 피어링을 활성화합니다. 트래픽이 항상 글로벌 AWS 백본에 머물고 퍼블릭 인터넷을 통과하지 않도록 네트워킹 구성을 설정합니다.,리전 간 VPC 액세스를 위한 AWS VPN CloudHub와 온프레미스 데이터 센터에 대한 VPN 연결을 위한 Direct Connect 게이트웨이를 설정합니다. 각 VPC에서 가상 프라이빗 게이트웨이를 생성한 다음 Direct Connect 게이트웨이에 대한 각 AWS Direct Connect 연결을 위한 프라이빗 가상 인터페이스를 생성합니다.,각 리전에 AWS Transit Gateway를 설정하여 리전 내의 모든 네트워크를 상호 연결합니다. 그런 다음 피어링 연결을 통해 전송 게이트웨이 간에 트래픽을 라우팅합니다.,모든 AWS 리소스 및 온프레미스 데이터 센터에 대한 리전 간 VPC 액세스를 달성하도록 AWS Direct Connect 게이트웨이를 설정합니다. 단일 관리형 연결로 처리하기 위해 단일 AWS Direct Connect 엔드포인트에서 여러 연결을 집계하도록 링크 집계 그룹(LAG)을 설정합니다. 각 VPC에서 가상 프라이빗 게이트웨이를 시작한 다음 Direct Connect 게이트웨이에 대한 각 AWS Direct Connect 연결에 대한 퍼블릭 가상 인터페이스를 생성합니다.,,,0,,
udemy,CLF-01,244,A company has an OLTP (Online Transactional Processing) application that is hosted in an Amazon ECS cluster using the Fargate launch type. It has an Amazon RDS database that stores data of its production website. The Data Analytics team needs to run queries against the database to track and audit all user transactions. These query operations against the production database must not impact application performance in any way.Which of the following is the MOST suitable and cost-effective solution that you should implement?,B,B,Upgrade the instance type of the RDS database to a large instance.,Set up a new Amazon RDS Read Replica of the production database. Direct the Data Analytics team to query the production data from the replica.,Set up a new Amazon Redshift database cluster. Migrate the product database into Redshift and allow the Data Analytics team to fetch data from it.,Set up a Multi-AZ deployments configuration of your production database in RDS. Direct the Data Analytics team to query the production data from the standby instance.,,,,회사에는 Fargate 시작 유형을 사용하여 Amazon ECS 클러스터에서 호스팅되는 OLTP(온라인 트랜잭션 처리) 애플리케이션이 있습니다. 프로덕션 웹 사이트의 데이터를 저장하는 Amazon RDS 데이터베이스가 있습니다. 데이터 분석 팀은 모든 사용자 트랜잭션을 추적하고 감사하기 위해 데이터베이스에 대해 쿼리를 실행해야 합니다. 프로덕션 데이터베이스에 대한 이러한 쿼리 작업은 어떤 식으로든 애플리케이션 성능에 영향을 주어서는 안 됩니다.다음 중 구현해야 하는 가장 적합하고 비용 효율적인 솔루션은 무엇입니까?,RDS 데이터베이스의 인스턴스 유형을 대형 인스턴스로 업그레이드하십시오.,프로덕션 데이터베이스의 새 Amazon RDS 읽기 전용 복제본을 설정합니다. 복제본에서 생산 데이터를 쿼리하도록 데이터 분석 팀에 지시합니다.,새 Amazon Redshift 데이터베이스 클러스터를 설정합니다. 제품 데이터베이스를 Redshift로 마이그레이션하고 Data Analytics 팀이 여기에서 데이터를 가져오도록 허용합니다.,RDS에서 프로덕션 데이터베이스의 다중 AZ 배포 구성을 설정합니다. 대기 인스턴스에서 생산 데이터를 쿼리하도록 데이터 분석 팀에 지시합니다.,,,0,,
udemy,CLF-01,245,"A company has an Application Load Balancer (ALB) that accepts HTTP and HTTPS traffic on ports 80 and 443, respectively. Recently, the company associated a new domain for its website, and they want to ensure that all HTTP traffic for this new domain is automatically redirected to HTTPS to improve security.Which ALB configuration should be done to satisfy the requirement?",B,B,Create a new ALB listener on port 443 and configure it to redirect HTTP traffic to HTTPS.,Configure the existing HTTP listener to redirect traffic to port 443.,Configure the existing on port 443 and add a redirect action to HTTP on port 80.,Create a new HTTP listener on port 80 and add a redirect action to the HTTPS protocol on port 443.,,,,회사에는 각각 포트 80 및 443에서 HTTP 및 HTTPS 트래픽을 허용하는 ALB(Application Load Balancer)가 있습니다. 최근 회사는 웹 사이트에 새 도메인을 연결했으며 이 새 도메인에 대한 모든 HTTP 트래픽이 자동으로 HTTPS로 리디렉션되어 보안을 강화하기를 원합니다.요구 사항을 충족하려면 어떤 ALB 구성을 수행해야 합니까?,포트 443에서 새 ALB 리스너를 생성하고 HTTP 트래픽을 HTTPS로 리디렉션하도록 구성합니다.,트래픽을 포트 443으로 리디렉션하도록 기존 HTTP 수신기를 구성합니다.,포트 443에서 기존을 구성하고 포트 80에서 HTTP에 리디렉션 작업을 추가합니다.,포트 80에서 새 HTTP 수신기를 생성하고 포트 443에서 HTTPS 프로토콜에 리디렉션 작업을 추가합니다.,,,0,,
udemy,CLF-01,246,"A technical lead of the Cloud Infrastructure team was consulted by a software developer regarding the required AWS resources of the web application that he is building. The developer knows that an Instance Store only provides ephemeral storage where the data is automatically deleted when the instance is terminated. To ensure that the data of the web application persists, the app should be launched in an EC2 instance that has a durable, block-level storage volume attached. The developer knows that they need to use an EBS volume, but they are not sure what type they need to use.In this scenario, which of the following is true about Amazon EBS volume types and their respective usage? (Select TWO.)",AB,AB,"Magnetic volumes provide the lowest cost per gigabyte of all EBS volume types and are ideal for workloads where data is accessed infrequently, and applications where the lowest storage cost is important.","Provisioned IOPS volumes offer storage with consistent and low-latency performance, and are designed for I/O intensive applications such as large relational or NoSQL databases.","Single root I/O virtualization (SR-IOV) volumes are suitable for a broad range of workloads, including small to medium-sized databases, development and test environments, and boot volumes.","Spot volumes provide the lowest cost per gigabyte of all EBS volume types and are ideal for workloads where data is accessed infrequently, and applications where the lowest storage cost is important.","General Purpose SSD (gp3) volumes with multi-attach enabled offer consistent and low-latency performance, and are designed for applications requiring multi-az resiliency.",,,클라우드 인프라 팀의 기술 책임자는 자신이 구축하고 있는 웹 애플리케이션의 필수 AWS 리소스와 관련하여 소프트웨어 개발자와 상의했습니다. 개발자는 인스턴스 스토어가 인스턴스가 종료될 때 데이터가 자동으로 삭제되는 임시 저장소만 제공한다는 것을 알고 있습니다. 웹 애플리케이션의 데이터가 지속되도록 하려면 내구성 있는 블록 수준 스토리지 볼륨이 연결된 EC2 인스턴스에서 앱을 시작해야 합니다. 개발자는 EBS 볼륨을 사용해야 한다는 것을 알고 있지만 어떤 유형을 사용해야 하는지 잘 모릅니다.이 시나리오에서 다음 중 Amazon EBS 볼륨 유형 및 각각의 사용에 대해 참인 것은 무엇입니까? (2개를 선택하세요.),마그네틱 볼륨은 모든 EBS 볼륨 유형 중에서 기가바이트당 비용이 가장 저렴하며 데이터 액세스 빈도가 낮은 워크로드와 최저 스토리지 비용이 중요한 애플리케이션에 적합합니다.,프로비저닝된 IOPS 볼륨은 일관되고 지연 시간이 짧은 성능의 스토리지를 제공하며 대규모 관계형 또는 NoSQL 데이터베이스와 같은 I/O 집약적인 애플리케이션용으로 설계되었습니다.,"SR-IOV(Single Root I/O Virtualization) 볼륨은 중소형 데이터베이스, 개발 및 테스트 환경, 부트 볼륨을 비롯한 광범위한 워크로드에 적합합니다.",스팟 볼륨은 모든 EBS 볼륨 유형 중 기가바이트당 비용이 가장 낮고 데이터 액세스 빈도가 낮은 워크로드와 최저 스토리지 비용이 중요한 애플리케이션에 이상적입니다.,다중 연결이 활성화된 범용 SSD(gp3) 볼륨은 일관되고 짧은 대기 시간 성능을 제공하며 다중 Az 복원력이 필요한 애플리케이션용으로 설계되었습니다.,,0,,
udemy,CLF-01,247,"A real-time data analytics application is using AWS Lambda to process data and store results in JSON format to an S3 bucket. To speed up the existing workflow, you have to use a service where you can run sophisticated Big Data analytics on your data without moving them into a separate analytics system.   Which of the following group of services can you use to meet this requirement?",C,C,"S3 Select, Amazon Neptune, DynamoDB DAX","Amazon X-Ray, Amazon Neptune, DynamoDB","S3 Select, Amazon Athena, Amazon Redshift Spectrum","Amazon Glue, Glacier Select, Amazon Redshift",,,,실시간 데이터 분석 애플리케이션은 AWS Lambda를 사용하여 데이터를 처리하고 결과를 JSON 형식으로 S3 버킷에 저장합니다. 기존 워크플로의 속도를 높이려면 데이터를 별도의 분석 시스템으로 옮기지 않고 데이터에 대해 정교한 빅 데이터 분석을 실행할 수 있는 서비스를 사용해야 합니다.   다음 중 이 요구 사항을 충족하기 위해 사용할 수 있는 서비스 그룹은 무엇입니까?,"S3 셀렉트, Amazon Neptune, DynamoDB DAX","Amazon X-Ray, Amazon Neptune, DynamoDB","S3 셀렉트, 아마존 아테나, 아마존 레드시프트 스펙트럼","Amazon Glue, Glacier Select, Amazon Redshift",,,0,,
udemy,CLF-01,248,"Every week, an e-commerce company announces a sales promotion, causing its application hosted on an Auto Scaling group to experience intermittent downtime. Because of long initialization times, the application only becomes operational minutes before a new EC2 instance turns into RUNNING state. A solutions architect must devise a solution that launches capacity in advance based on a forecasted load in order to scale faster.Which solution meets the requirements with the least amount of effort?",D,D,Use Amazon Forecast to analyze and predict the workload pattern of the application. Create a scheduled scaling policy based on the prediction results.,Create a Scheduled Amazon EventBridge Rule that runs a scaling job on a Lambda function every midnight.,Create a dynamic scaling policy based on the historical average CPU load of the application.,Configure the Auto Scaling group to use predictive scaling.,,,,매주 전자 상거래 회사에서 판매 프로모션을 발표하여 Auto Scaling 그룹에서 호스팅되는 애플리케이션이 간헐적으로 다운타임을 경험하게 합니다. 초기화 시간이 길기 때문에 새 EC2 인스턴스가 RUNNING상태가 되기 몇 분 전에만 애플리케이션이 작동하게 됩니다. 솔루션 설계자는 더 빠르게 확장하기 위해 예측된 로드를 기반으로 사전에 용량을 시작하는 솔루션을 고안해야 합니다.최소한의 노력으로 요구 사항을 충족하는 솔루션은 무엇입니까?,Amazon Forecast를 사용하여 애플리케이션의 워크로드 패턴을 분석하고 예측합니다. 예측 결과를 기반으로 예약된 조정 정책을 만듭니다.,매일 자정에 Lambda 함수에서 조정 작업을 실행하는 예약된 Amazon EventBridge 규칙을 생성합니다.,애플리케이션의 과거 평균 CPU 로드를 기반으로 동적 조정 정책을 만듭니다.,예측 조정을 사용하도록 Auto Scaling 그룹을 구성합니다.,,,0,,
udemy,CLF-01,249,"An online survey startup is collecting real estate data in the United States for several years. The startup already has a total of 5 TB of data stored in an Amazon S3 bucket located in the us-east-1 Region. All real estate data must be shared with a European AWS Managed Service Provider (MSP) Partner which also uses Amazon S3 for storage. Due to budget constraints, the startup must keep its data transfer costs in S3 as low as possible and disable anonymous access.Which solution meets this requirement MOST cost-effectively?",B,B,Enable Cross-Region Replication(CRR) on the startup’s S3 bucket to automatically copy the S3 content to the partner’s S3 bucket in Europe.,Enable the Requester Pays feature on the Amazon S3 bucket to lower data transfer costs and disable anonymous access,Enable cross-account access of the startup’s S3 bucket to allow the data downloads and exclusive access from the partner’s AWS account,Enable S3 Object Lock in governance mode to lower data transfer costs and set a Legal Hold for each object to disable anonymous access,,,,한 온라인 설문조사 스타트업이 몇 년 동안 미국에서 부동산 데이터를 수집하고 있습니다. 이 스타트업은 이미 us-east-1 지역에 있는 Amazon S3 버킷에 총 5TB의 데이터를 저장하고 있습니다. 모든 부동산 데이터는 Amazon S3를 스토리지로 사용하는 유럽 AWS 관리형 서비스 공급자(MSP) 파트너와 공유해야 합니다. 예산 제약으로 인해 스타트업은 S3의 데이터 전송 비용을 가능한 한 낮게 유지하고 익명 액세스를 비활성화해야 합니다.이 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까?,스타트업의 S3 버킷에서 CRR(Cross-Region Replication)을 활성화하여 S3 콘텐츠를 유럽에 있는 파트너의 S3 버킷에 자동으로 복사합니다.,Amazon S3 버킷에서 요청자 지불 기능을 활성화하여 데이터 전송 비용을 낮추고 익명 액세스를 비활성화합니다.,스타트업 S3 버킷의 교차 계정 액세스를 활성화하여 파트너의 AWS 계정에서 데이터 다운로드 및 독점 액세스를 허용합니다.,거버넌스 모드에서 S3 객체 잠금을 활성화하여 데이터 전송 비용을 낮추고 각 객체에 법적 보존을 설정하여 익명 액세스를 비활성화합니다.,,,0,,
udemy,CLF-01,250,A media company needs to configure an Amazon S3 bucket to serve static assets for the public-facing web application. Which methods ensure that all of the objects uploaded to the S3 bucket can be read publicly all over the Internet? (Select TWO.),CD,CD,Do nothing. Amazon S3 objects are already public by default.,Create an IAM role to set the objects inside the S3 bucket to public read.,Grant public read access to the object when uploading it using the S3 Console.,Configure the S3 bucket policy to set all objects to public read.,Configure the cross-origin resource sharing (CORS) of the S3 bucket to allow objects to be publicly accessible from all domains.,,,미디어 회사는 공개 웹 애플리케이션을 위한 정적 자산을 제공하도록 Amazon S3 버킷을 구성해야 합니다. S3 버킷에 업로드된 모든 객체를 인터넷 전체에서 공개적으로 읽을 수 있도록 보장하는 방법은 무엇입니까? (2개를 선택하세요.),아무것도하지 마세요. Amazon S3 객체는 기본적으로 이미 공개되어 있습니다.,S3 버킷 내부의 객체를 공개 읽기로 설정하는 IAM 역할을 생성합니다.,S3 콘솔을 사용하여 객체를 업로드할 때 객체에 대한 퍼블릭 읽기 액세스 권한을 부여합니다.,모든 객체를 공개 읽기로 설정하도록 S3 버킷 정책을 구성합니다.,모든 도메인에서 개체에 공개적으로 액세스할 수 있도록 S3 버킷의 CORS(교차 원본 리소스 공유)를 구성합니다.,,0,,
udemy,CLF-01,251,"A media company is using Amazon EC2, ELB, and S3 for its video-sharing portal for filmmakers. They are using a standard S3 storage class to store all high-quality videos that are frequently accessed only during the first three months of posting.As a Solutions Architect, what should you do if the company needs to automatically transfer or archive media data from an S3 bucket to Glacier?",B,B,Use a custom shell script that transfers data from the S3 bucket to Glacier,Use Lifecycle Policies,Use Amazon SWF,Use Amazon SQS,,,,"한 미디어 회사는 영화 제작자를 위한 비디오 공유 포털에 Amazon EC2, ELB 및 S3를 사용하고 있습니다. 게시 첫 3개월 동안에만 자주 액세스되는 모든 고품질 비디오를 저장하기 위해 표준 S3 스토리지 클래스를 사용하고 있습니다.솔루션 아키텍트로서 회사가 S3 버킷에서 Glacier로 미디어 데이터를 자동으로 전송하거나 보관해야 하는 경우 어떻게 해야 합니까?",S3 버킷에서 Glacier로 데이터를 전송하는 사용자 지정 셸 스크립트 사용,수명 주기 정책 사용,아마존 SWF 사용,아마존 SQS 사용,,,0,,
udemy,CLF-01,252,"A technology company is building a new cryptocurrency trading platform that allows the buying and selling of Bitcoin, Ethereum, Ripple, Tether, and many others. You were hired as a Cloud Engineer to build the required infrastructure needed for this new trading platform. On your first week at work, you started to create CloudFormation YAML scripts that define all of the needed AWS resources for the application. Your manager was shocked that you haven't created the EC2 instances, S3 buckets, and other AWS resources straight away. He does not understand the text-based scripts that you have done and has asked for your clarification.In this scenario, what are the benefits of using the Amazon CloudFormation service that you should tell your manager to clarify his concerns? (Select TWO.)",BD,BD,A storage location for the code of your application,Allows you to model your entire infrastructure in a text file,"Using CloudFormation itself is free, including the AWS resources that have been created.","Enables modeling, provisioning, and version-controlling of your entire AWS infrastructure",Provides highly durable and scalable data storage,,,"한 기술 회사가 비트코인, 이더리움, 리플, 테더 등을 사고팔 수 있는 새로운 암호화폐 거래 플랫폼을 구축하고 있습니다. 귀하는 이 새로운 거래 플랫폼에 필요한 인프라를 구축하기 위해 클라우드 엔지니어로 고용되었습니다. 직장에서 첫 주에 애플리케이션에 필요한 모든 AWS 리소스를 정의하는 CloudFormation YAML 스크립트를 생성하기 시작했습니다. EC2 인스턴스, S3 버킷 및 기타 AWS 리소스를 바로 생성하지 않은 것에 관리자가 충격을 받았습니다. 그는 귀하가 수행한 텍스트 기반 스크립트를 이해하지 못하고 설명을 요청했습니다.이 시나리오에서 Amazon CloudFormation 서비스를 사용하여 관리자에게 문제를 명확히 하라고 알려야 하는 이점은 무엇입니까? (2개를 선택하세요.)",애플리케이션 코드의 저장 위치,전체 인프라를 텍스트 파일로 모델링할 수 있습니다.,생성된 AWS 리소스를 포함하여 CloudFormation 자체를 사용하는 것은 무료입니다.,"전체 AWS 인프라의 모델링, 프로비저닝 및 버전 제어를 지원합니다.",내구성과 확장성이 뛰어난 데이터 스토리지 제공,,0,,
udemy,CLF-01,253,"A company is using an Auto Scaling group which is configured to launch new t2.micro EC2 instances when there is a significant load increase in the application. To cope with the demand, you now need to replace those instances with a larger t2.2xlarge instance type.How would you implement this change?",C,C,Create another Auto Scaling Group and attach the new instance type.,Change the instance type of each EC2 instance manually.,Create a new launch configuration with the new instance type and update the Auto Scaling Group.,Just change the instance type to t2.2xlarge in the current launch configuration,,,,회사는 t2.micro애플리케이션에서 상당한 부하 증가가 있을 때 새 EC2 인스턴스를 시작하도록 구성된 Auto Scaling 그룹을 사용하고 있습니다. 수요에 대처하기 위해 이제 해당 인스턴스를 더 큰 t2.2xlarge인스턴스 유형으로 교체해야 합니다.이 변경 사항을 어떻게 구현하시겠습니까?,다른 Auto Scaling Group을 생성하고 새 인스턴스 유형을 연결합니다.,각 EC2 인스턴스의 인스턴스 유형을 수동으로 변경합니다.,새 인스턴스 유형으로 새 시작 구성을 생성하고 Auto Scaling 그룹을 업데이트합니다.,인스턴스 유형을 t2.2xlarge현재 시작 구성으로 변경하기만 하면 됩니다.,,,0,,
udemy,CLF-01,254,"A company is using an On-Demand EC2 instance to host a legacy web application that uses an Amazon Instance Store-Backed AMI. The web application should be decommissioned as soon as possible and hence, you need to terminate the EC2 instance.When the instance is terminated, what happens to the data on the root volume?",C,C,Data is automatically saved as an EBS volume.,Data is automatically saved as an EBS snapshot.,Data is automatically deleted.,Data is unavailable until the instance is restarted.,,,,회사는 온디맨드 EC2 인스턴스를 사용하여 Amazon 인스턴스 스토어 지원 AMI를 사용하는 레거시 웹 애플리케이션을 호스팅하고 있습니다. 웹 애플리케이션은 가능한 한 빨리 폐기해야 하므로 EC2 인스턴스를 종료해야 합니다.인스턴스가 종료되면 루트 볼륨의 데이터는 어떻게 됩니까?,데이터는 자동으로 EBS 볼륨으로 저장됩니다.,데이터는 EBS 스냅샷으로 자동 저장됩니다.,데이터는 자동으로 삭제됩니다.,인스턴스를 다시 시작할 때까지 데이터를 사용할 수 없습니다.,,,0,,
udemy,CLF-01,255,"There is a new compliance rule in your company that audits every Windows and Linux EC2 instances each month to view any performance issues. They have more than a hundred EC2 instances running in production, and each must have a logging function that collects various system details regarding that instance. The SysOps team will periodically review these logs and analyze their contents using AWS Analytics tools, and the result will need to be retained in an S3 bucket. In this scenario, what is the most efficient way to collect and analyze logs from the instances with minimal effort?",D,D,Install AWS SDK in each instance and create a custom daemon script that would collect and push data to CloudWatch Logs periodically. Enable CloudWatch detailed monitoring and use CloudWatch Logs Insights to analyze the log data of all instances.,Install AWS Inspector Agent in each instance which will collect and push data to CloudWatch Logs periodically. Set up a CloudWatch dashboard to properly analyze the log data of all instances.,Install the AWS Systems Manager Agent (SSM Agent) in each instance which will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.,Install the unified CloudWatch Logs agent in each instance which will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.,,,,성능 문제를 확인하기 위해 매월 모든 Windows 및 Linux EC2 인스턴스를 감사하는 새로운 규정 준수 규칙이 회사에 있습니다. 프로덕션 환경에서 실행되는 100개 이상의 EC2 인스턴스가 있으며 각 인스턴스에는 해당 인스턴스와 관련된 다양한 시스템 세부 정보를 수집하는 로깅 기능이 있어야 합니다. SysOps 팀은 주기적으로 이러한 로그를 검토하고 AWS Analytics 도구를 사용하여 내용을 분석하며 그 결과는 S3 버킷에 보관해야 합니다.이 시나리오에서 최소한의 노력으로 인스턴스에서 로그를 수집하고 분석하는 가장 효율적인 방법은 무엇입니까?,각 인스턴스에 AWS SDK를 설치하고 주기적으로 데이터를 수집하여 CloudWatch Logs로 푸시하는 사용자 지정 데몬 스크립트를 생성합니다. CloudWatch 상세 모니터링을 활성화하고 CloudWatch Logs Insights를 사용하여 모든 인스턴스의 로그 데이터를 분석하십시오.,주기적으로 데이터를 수집하고 CloudWatch Logs로 푸시할 각 인스턴스에 AWS Inspector 에이전트를 설치합니다. 모든 인스턴스의 로그 데이터를 올바르게 분석하도록 CloudWatch 대시보드를 설정합니다.,자동으로 데이터를 수집하고 CloudWatch Logs로 푸시할 각 인스턴스에 AWS Systems Manager 에이전트(SSM 에이전트)를 설치합니다. CloudWatch Logs Insights로 로그 데이터를 분석합니다.,자동으로 데이터를 수집하고 CloudWatch Logs로 푸시할 통합 CloudWatch Logs 에이전트를 각 인스턴스에 설치합니다. CloudWatch Logs Insights로 로그 데이터를 분석합니다.,,,0,,
udemy,CLF-01,256,"A new company policy requires IAM users to change their passwords’ minimum length to 12 characters. After a random inspection, you found out that there are still employees who do not follow the policy.How can you automatically check and evaluate whether the current password policy for an account complies with the company password policy?",C,C,Create a Scheduled Lambda Function that will run a custom script to check compliance against changes made to the passwords periodically.,Create a rule in the Amazon CloudWatch event. Build an event pattern to match events on IAM. Set the event name to “ChangePassword” in the event pattern. Configure SNS to send notifications to you whenever a user has made changes to his password.,Configure AWS Config to trigger an evaluation that will check the compliance for a user’s password periodically.,Create a CloudTrail trail. Filter the result by setting the attribute to “Event Name” and lookup value to “ChangePassword”. This easily gives you the list of users who have made changes to their passwords.,,,,새로운 회사 정책에 따라 IAM 사용자는 암호의 최소 길이를 12자로 변경해야 합니다. 무작위 검사 후 정책을 준수하지 않는 직원이 여전히 있음을 알게 되었습니다.계정에 대한 현재 비밀번호 정책이 회사 비밀번호 정책을 준수하는지 어떻게 자동으로 확인하고 평가할 수 있습니까?,사용자 지정 스크립트를 실행하여 정기적으로 암호 변경 사항에 대한 규정 준수를 확인하는 예약된 Lambda 함수를 생성합니다.,"Amazon CloudWatch 이벤트에서 규칙을 생성합니다. IAM의 이벤트와 일치하도록 이벤트 패턴을 구축합니다. 이벤트 패턴에서 이벤트 이름을 ""ChangePassword""로 설정합니다. 사용자가 비밀번호를 변경할 때마다 알림을 보내도록 SNS를 구성하십시오.",사용자 암호의 준수 여부를 주기적으로 확인하는 평가를 트리거하도록 AWS Config를 구성합니다.,"CloudTrail 추적을 생성합니다. 속성을 ""이벤트 이름""으로 설정하고 조회 값을 ""ChangePassword""로 설정하여 결과를 필터링합니다. 이렇게 하면 암호를 변경한 사용자 목록을 쉽게 얻을 수 있습니다.",,,0,,
udemy,CLF-01,257,"A company hosts all its applications on its data center on the US East coast. Most of the workloads are legacy applications that are hosted on individual virtual machines running in Linux and Windows operating systems. The company plans to migrate all of its VM workloads to the AWS cloud. To minimize changes in the applications during the migration process, it has been decided that the company will use a “lift-and-shift” strategy. The company also wants to minimize downtime during the migration process.Which of the following option should the Solutions Architect implement for this scenario?",B,B,"Utilize AWS DataSync to migrate the application workloads to AWS. Deploy the AWS DataSync VM on the on-premises data center. Once replication is completed, launch Amazon EC2 instances based on the created AMIs.",Install the AWS Replication Agent on each of the on-premises VMs to continuously replicate the servers to AWS. Use AWS Migration Service (AWS MGN) to launch test instances and perform cutover once testing is completed.,"Use the AWS Application Discovery Service for lift-and-shift migrations. Deploy the AWS Application Discovery Agent to the on-premises data center to start the replication process. After the replication task is completed, launch Amazon EC2 instances based on the created AMIs.",Export the on-premises VMs and upload the images to an Amazon S3 bucket. Use VM Import/Export service to import the images and launch them as Amazon EC2 instances.,,,,"회사는 미국 동부 해안의 데이터 센터에서 모든 애플리케이션을 호스팅합니다. 대부분의 워크로드는 Linux 및 Windows 운영 체제에서 실행되는 개별 가상 머신에서 호스팅되는 레거시 애플리케이션입니다. 이 회사는 모든 VM 워크로드를 AWS 클라우드로 마이그레이션할 계획입니다. 마이그레이션 프로세스 동안 애플리케이션의 변경을 최소화하기 위해 회사는 ""리프트 앤 시프트"" 전략을 사용하기로 결정했습니다. 회사는 또한 마이그레이션 프로세스 동안 다운타임을 최소화하기를 원합니다.다음 중 이 시나리오에 대해 Solutions Architect가 구현해야 하는 옵션은 무엇입니까?",AWS DataSync를 활용하여 애플리케이션 워크로드를 AWS로 마이그레이션합니다. 온프레미스 데이터 센터에 AWS DataSync VM을 배포합니다. 복제가 완료되면 생성된 AMI를 기반으로 Amazon EC2 인스턴스를 시작합니다.,각 온프레미스 VM에 AWS Replication Agent를 설치하여 서버를 AWS에 지속적으로 복제합니다. AWS Migration Service(AWS MGN)를 사용하여 테스트 인스턴스를 시작하고 테스트가 완료되면 컷오버를 수행합니다.,리프트 앤 시프트 마이그레이션에 AWS Application Discovery Service를 사용하십시오. 복제 프로세스를 시작하려면 AWS Application Discovery Agent를 온프레미스 데이터 센터에 배포하십시오. 복제 작업이 완료되면 생성된 AMI를 기반으로 Amazon EC2 인스턴스를 시작합니다.,온프레미스 VM을 내보내고 이미지를 Amazon S3 버킷에 업로드합니다. VM Import/Export 서비스를 사용하여 이미지를 가져와 Amazon EC2 인스턴스로 시작합니다.,,,0,,
udemy,CLF-01,258,"A startup is planning to set up and govern a secure, compliant, multi-account AWS environment in preparation for its upcoming projects. The IT Manager requires the solution to have a dashboard for continuous detection of policy non-conformance and non-compliant resources across the enterprise, as well as to comply with the AWS multi-account strategy best practices.Which of the following offers the easiest way to fulfill this task?",A,A,Use AWS Control Tower to launch a landing zone to automatically provision and configure new accounts through an Account Factory. Utilize the AWS Control Tower dashboard to monitor provisioned accounts across your enterprise. Set up preventive and detective guardrails for policy enforcement.,Use AWS Service Catalog to launch new AWS member accounts. Configure AWS Service Catalog Launch Constraints to continuously track configuration changes and monitor non-compliant resources. Set up a Multi-Account Multi-Region Data Aggregator to monitor compliance data for rules and accounts in an aggregated view,Launch new AWS member accounts using the AWS CloudFormation StackSets. Use AWS Config to continuously track the configuration changes and set rules to monitor non-compliant resources. Set up a Multi-Account Multi-Region Data Aggregator to monitor compliance data for rules and accounts in an aggregated view,Use AWS Organizations to build a landing zone to automatically provision new AWS accounts. Utilize the AWS Personal Health Dashboard to see provisioned accounts across your enterprise. Enable preventive and detective guardrails enabled for policy enforcement.,,,,한 스타트업이 향후 프로젝트를 준비하기 위해 안전하고 규정을 준수하는 다중 계정 AWS 환경을 설정하고 관리할 계획입니다. IT 관리자는 기업 전체에서 정책 비준수 및 비준수 리소스를 지속적으로 감지하고 AWS 다중 계정 전략 모범 사례를 준수하기 위한 대시보드를 갖춘 솔루션이 필요합니다.다음 중 이 작업을 수행하는 가장 쉬운 방법은 무엇입니까?,AWS Control Tower를 사용하여 랜딩 존을 시작하여 Account Factory를 통해 새 계정을 자동으로 프로비저닝하고 구성합니다. AWS Control Tower 대시보드를 활용하여 기업 전체에서 프로비저닝된 계정을 모니터링하십시오. 정책 시행을 위한 예방 및 탐지 가드레일을 설정합니다.,AWS Service Catalog를 사용하여 새 AWS 회원 계정을 시작하십시오. 구성 변경 사항을 지속적으로 추적하고 규정을 준수하지 않는 리소스를 모니터링하도록 AWS Service Catalog 시작 제약 조건을 구성합니다. 집계된 보기에서 규칙 및 계정에 대한 규정 준수 데이터를 모니터링하도록 다중 계정 다중 리전 데이터 집계기를 설정합니다.,AWS CloudFormation StackSets를 사용하여 새 AWS 회원 계정을 시작합니다. AWS Config를 사용하여 구성 변경 사항을 지속적으로 추적하고 규정을 준수하지 않는 리소스를 모니터링하는 규칙을 설정하십시오. 집계된 보기에서 규칙 및 계정에 대한 규정 준수 데이터를 모니터링하도록 다중 계정 다중 리전 데이터 집계기를 설정합니다.,AWS Organizations를 사용하여 새 AWS 계정을 자동으로 프로비저닝하는 랜딩 존을 구축합니다. AWS Personal Health Dashboard를 활용하여 기업 전체에서 프로비저닝된 계정을 확인하십시오. 정책 시행을 위해 활성화된 예방 및 탐지 가드레일을 활성화합니다.,,,0,,
udemy,CLF-01,259,A web application is hosted in an Auto Scaling group of EC2 instances deployed across multiple Availability Zones behind an Application Load Balancer. You need to implement an SSL solution for your system to improve its security which is why you requested an SSL/TLS certificate from a third-party certificate authority (CA).Where can you safely import the SSL/TLS certificate of your application? (Select TWO.),DE,DE,A private S3 bucket with versioning enabled,An S3 bucket configured with server-side encryption with customer-provided encryption keys (SSE-C),CloudFront,IAM certificate store,AWS Certificate Manager,,,웹 애플리케이션은 Application Load Balancer 뒤의 여러 가용 영역에 배포된 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅됩니다. 보안을 향상시키려면 시스템에 SSL 솔루션을 구현해야 합니다. 이것이 타사 인증 기관(CA)의 SSL/TLS 인증서를 요청한 이유입니다.애플리케이션의 SSL/TLS 인증서를 어디에서 안전하게 가져올 수 있습니까? (2개를 선택하세요.),버전 관리가 활성화된 프라이빗 S3 버킷,고객 제공 암호화 키(SSE-C)를 사용한 서버 측 암호화로 구성된 S3 버킷,클라우드프론트,IAM 인증서 저장소,AWS 인증서 관리자,,0,,
udemy,CLF-01,260,"A company has a web application hosted in AWS cloud where the application logs are sent to Amazon CloudWatch. Lately, the web application has recently been encountering some errors which can be resolved simply by restarting the instance.What will you do to automatically restart the EC2 instances whenever the same application error occurs?",B,B,"First, look at the existing Flow logs for keywords related to the application error to create a custom metric. Then, create a CloudWatch alarm for that custom metric which invokes an action to restart the EC2 instance.","First, look at the existing CloudWatch logs for keywords related to the application error to create a custom metric. Then, create a CloudWatch alarm for that custom metric which invokes an action to restart the EC2 instance.","First, look at the existing Flow logs for keywords related to the application error to create a custom metric. Then, create a CloudWatch alarm for that custom metric which calls a Lambda function that invokes an action to restart the EC2 instance.","First, look at the existing CloudWatch logs for keywords related to the application error to create a custom metric. Then, create an alarm in Amazon SNS for that custom metric which invokes an action to restart the EC2 instance.",,,,회사에는 애플리케이션 로그가 Amazon CloudWatch로 전송되는 AWS 클라우드에서 호스팅되는 웹 애플리케이션이 있습니다. 최근에 웹 애플리케이션에서 인스턴스를 다시 시작하면 간단히 해결할 수 있는 몇 가지 오류가 발생했습니다.동일한 애플리케이션 오류가 발생할 때마다 EC2 인스턴스를 자동으로 다시 시작하려면 어떻게 해야 합니까?,먼저 애플리케이션 오류와 관련된 키워드에 대한 기존 흐름 로그를 살펴보고 사용자 지정 메트릭을 만듭니다. 그런 다음 EC2 인스턴스를 다시 시작하는 작업을 호출하는 해당 사용자 지정 지표에 대한 CloudWatch 경보를 생성합니다.,먼저 기존 CloudWatch 로그에서 애플리케이션 오류와 관련된 키워드를 살펴보고 사용자 지정 지표를 생성합니다. 그런 다음 EC2 인스턴스를 다시 시작하는 작업을 호출하는 해당 사용자 지정 지표에 대한 CloudWatch 경보를 생성합니다.,먼저 애플리케이션 오류와 관련된 키워드에 대한 기존 흐름 로그를 살펴보고 사용자 지정 메트릭을 만듭니다. 그런 다음 EC2 인스턴스를 다시 시작하는 작업을 호출하는 Lambda 함수를 호출하는 해당 사용자 지정 지표에 대한 CloudWatch 경보를 생성합니다.,먼저 기존 CloudWatch 로그에서 애플리케이션 오류와 관련된 키워드를 살펴보고 사용자 지정 지표를 생성합니다. 그런 다음 Amazon SNS에서 EC2 인스턴스를 다시 시작하는 작업을 호출하는 사용자 지정 지표에 대한 경보를 생성합니다.,,,0,,
udemy,CLF-01,261,"A data analytics startup is collecting clickstream data and stores them in an S3 bucket. You need to launch an AWS Lambda function to trigger the ETL jobs to run as soon as new data becomes available in Amazon S3.Which of the following services can you use as an extract, transform, and load (ETL) service in this scenario?",D,D,Redshift Spectrum,S3 Select,AWS Step Functions,AWS Glue,,,,"데이터 분석 스타트업이 클릭스트림 데이터를 수집하여 S3 버킷에 저장합니다. Amazon S3에서 새 데이터를 사용할 수 있게 되는 즉시 실행할 ETL 작업을 트리거하려면 AWS Lambda 함수를 시작해야 합니다.다음 중 이 시나리오에서 추출, 변환 및 로드(ETL) 서비스로 사용할 수 있는 서비스는 무엇입니까?",적색편이 스펙트럼,S3 선택,AWS 단계 기능,AWS 글루,,,0,,
udemy,CLF-01,262,"A company is planning to launch a High Performance Computing (HPC) cluster in AWS that does Computational Fluid Dynamics (CFD) simulations. The solution should scale-out their simulation jobs to experiment with more tunable parameters for faster and more accurate results. The cluster is composed of Windows servers hosted on t3a.medium EC2 instances. As the Solutions Architect, you should ensure that the architecture provides higher bandwidth, higher packet per second (PPS) performance, and consistently lower inter-instance latencies. Which is the MOST suitable and cost-effective solution that the Architect should implement to achieve the above requirements?",B,B,Enable Enhanced Networking with Elastic Fabric Adapter (EFA) on the Windows EC2 Instances.,Enable Enhanced Networking with Elastic Network Adapter (ENA) on the Windows EC2 Instances.,Enable Enhanced Networking with Intel 82599 Virtual Function (VF) interface on the Windows EC2 Instances.,"Use AWS ParallelCluster to deploy and manage the HPC cluster to provide higher bandwidth, higher packet per second (PPS) performance, and lower inter-instance latencies.",,,,"한 회사에서 CFD(전산 유체 역학) 시뮬레이션을 수행하는 고성능 컴퓨팅(HPC) 클러스터를 AWS에서 시작할 계획입니다. 솔루션은 시뮬레이션 작업을 확장하여 더 빠르고 정확한 결과를 얻기 위해 더 조정 가능한 매개변수를 실험해야 합니다. 클러스터는 t3a.medium EC2 인스턴스에서 호스팅되는 Windows 서버로 구성됩니다. 솔루션 설계자는 아키텍처가 더 높은 대역폭, 더 높은 PPS(초당 패킷) 성능을 제공하고 지속적으로 더 낮은 인스턴스 간 대기 시간을 제공하는지 확인해야 합니다.Architect가 위의 요구 사항을 달성하기 위해 구현해야 하는 가장 적합하고 비용 효율적인 솔루션은 무엇입니까?",Windows EC2 인스턴스에서 EFA(Elastic Fabric Adapter)로 향상된 네트워킹을 활성화합니다.,Windows EC2 인스턴스에서 Elastic Network Adapter(ENA)로 향상된 네트워킹을 활성화합니다.,Windows EC2 인스턴스에서 Intel 82599 VF(Virtual Function) 인터페이스로 향상된 네트워킹을 활성화합니다.,"AWS ParallelCluster를 사용하여 HPC 클러스터를 배포 및 관리하여 더 높은 대역폭, 더 높은 PPS(초당 패킷) 성능 및 더 낮은 인스턴스 간 지연 시간을 제공합니다.",,,0,,
udemy,CLF-01,263,"A Solutions Architect is migrating several Windows-based applications to AWS that require a scalable file system storage for high-performance computing (HPC). The storage service must have full support for the SMB protocol and Windows NTFS, Active Directory (AD) integration, and Distributed File System (DFS). Which of the following is the MOST suitable storage service that the Architect should use to fulfill this scenario?",D,D,Amazon S3 Glacier Deep Archive,Amazon FSx for Lustre,AWS DataSync,Amazon FSx for Windows File Server,,,,"Solutions Architect는 고성능 컴퓨팅(HPC)을 위한 확장 가능한 파일 시스템 스토리지가 필요한 여러 Windows 기반 애플리케이션을 AWS로 마이그레이션하고 있습니다. 저장소 서비스는 SMB 프로토콜과 Windows NTFS, AD(Active Directory) 통합 및 DFS(분산 파일 시스템)를 완벽하게 지원해야 합니다.다음 중 Architect가 이 시나리오를 이행하기 위해 사용해야 하는 가장 적합한 스토리지 서비스는 무엇입니까?",Amazon S3 Glacier 딥 아카이브,Lustre용 Amazon FSx,AWS 데이터싱크,Windows 파일 서버용 Amazon FSx,,,0,,
udemy,CLF-01,264,"A financial company wants to store their data in Amazon S3 but at the same time, they want to store their frequently accessed data locally on their on-premises server. This is due to the fact that they do not have the option to extend their on-premises storage, which is why they are looking for a durable and scalable storage service to use in AWS. What is the best solution for this scenario?",D,D,Use a fleet of EC2 instance with EBS volumes to store the commonly used data.,Use both Elasticache and S3 for frequently accessed data.,Use Amazon Glacier.,Use the Amazon Storage Gateway -  Cached Volumes.,,,,금융 회사는 Amazon S3에 데이터를 저장하려고 하지만 동시에 자주 액세스하는 데이터를 온프레미스 서버에 로컬로 저장하려고 합니다. 온프레미스 스토리지를 확장할 수 있는 옵션이 없기 때문에 AWS에서 사용할 내구성 있고 확장 가능한 스토리지 서비스를 찾고 있습니다. 이 시나리오에 가장 적합한 솔루션은 무엇입니까?,EBS 볼륨이 있는 EC2 인스턴스 플릿을 사용하여 일반적으로 사용되는 데이터를 저장합니다.,자주 액세스하는 데이터에는 Elasticache와 S3를 모두 사용하십시오.,아마존 빙하를 사용하십시오.,Amazon Storage Gateway - 캐시된 볼륨을 사용합니다.,,,0,,
udemy,CLF-01,265,A business plans to deploy an application on EC2 instances within an Amazon VPC and is considering adopting a Network Load Balancer to distribute incoming traffic among the instances. A solutions architect needs to suggest a solution that will enable the security team to inspect traffic entering and exiting their VPC.Which approach satisfies the requirements?,B,B,Enable Traffic Mirroring on the Network Load Balancer and forward traffic to the instances. Create a traffic mirror filter to inspect the ingress and egress of data that traverses your Amazon VPC.,Create a firewall using the AWS Network Firewall service at the VPC level then add custom rule groups for inspecting ingress and egress traffic. Update the necessary VPC route tables.,Use the Network Access Analyzer service on the application’s VPC for inspecting ingress and egress traffic. Create a new Network Access Scope to filter and analyze all incoming and outgoing requests.,Create a firewall at the subnet level using the Amazon Detective service. Inspect the ingress and egress traffic using the VPC Reachability Analyzer.,,,,기업은 Amazon VPC 내의 EC2 인스턴스에 애플리케이션을 배포할 계획이며 수신 트래픽을 인스턴스 간에 분산하기 위해 Network Load Balancer 채택을 고려하고 있습니다. 솔루션 설계자는 보안 팀이 VPC에 들어오고 나가는 트래픽을 검사할 수 있는 솔루션을 제안해야 합니다.어떤 접근 방식이 요구 사항을 충족합니까?,Network Load Balancer에서 트래픽 미러링을 활성화하고 트래픽을 인스턴스로 전달합니다. 트래픽 미러 필터를 생성하여 Amazon VPC를 통과하는 데이터의 수신 및 송신을 검사합니다.,VPC 수준에서 AWS Network Firewall 서비스를 사용하여 방화벽을 만든 다음 수신 및 송신 트래픽을 검사하기 위한 사용자 지정 규칙 그룹을 추가합니다. 필요한 VPC 라우팅 테이블을 업데이트합니다.,인그레스 및 이그레스 트래픽을 검사하기 위해 애플리케이션의 VPC에서 네트워크 액세스 분석기 서비스를 사용합니다. 새 네트워크 액세스 범위를 생성하여 모든 수신 및 발신 요청을 필터링하고 분석합니다.,Amazon Detective 서비스를 사용하여 서브넷 수준에서 방화벽을 만듭니다. VPC 도달 가능성 분석기를 사용하여 수신 및 송신 트래픽을 검사합니다.,,,0,,
udemy,CLF-01,266,"A health organization is using a large Dedicated EC2 instance with multiple EBS volumes to host its health records web application. The EBS volumes must be encrypted due to the confidentiality of the data that they are handling and also to comply with the HIPAA (Health Insurance Portability and Accountability Act) standard.   In EBS encryption, what service does AWS use to secure the volume's data at rest? (Select TWO.)",AC,AC,By using Amazon-managed keys in AWS Key Management Service (KMS).,By using a password stored in CloudHSM.,By using your own keys in AWS Key Management Service (KMS).,By using S3 Server-Side Encryption.,By using the SSL certificates provided by the AWS Certificate Manager (ACM).,By using S3 Client-Side Encryption.,,의료 기관은 여러 EBS 볼륨이 있는 대규모 전용 ​​EC2 인스턴스를 사용하여 의료 기록 웹 애플리케이션을 호스팅하고 있습니다. EBS 볼륨은 취급하는 데이터의 기밀성과 HIPAA(Health Insurance Portability and Accountability Act) 표준을 준수하기 때문에 암호화해야 합니다.   EBS 암호화에서 AWS는 볼륨의 미사용 데이터를 보호하기 위해 어떤 서비스를 사용합니까? (2개를 선택하세요.),AWS Key Management Service(KMS)에서 Amazon 관리형 키를 사용합니다.,CloudHSM에 저장된 암호를 사용합니다.,AWS Key Management Service(KMS)에서 자체 키를 사용합니다.,S3 서버 측 암호화를 사용합니다.,AWS Certificate Manager(ACM)에서 제공하는 SSL 인증서를 사용합니다.,,0,,S3 클라이언트 측 암호화를 사용합니다.
udemy,CLF-01,267,An operations team has an application running on EC2 instances inside two custom VPCs. The VPCs are located in the Ohio and N.Virginia Region respectively. The team wants to transfer data between the instances without traversing the public internet.Which combination of steps will achieve this? (Select TWO.),BE,BE,Deploy a VPC endpoint on each region to enable a private connection.,Set up a VPC peering connection between the VPCs.,Create an Egress-only Internet Gateway.,Launch a NAT Gateway in the public subnet of each VPC.,Re-configure the route table’s target and destination of the instances’ subnet.,,,운영 팀에는 두 개의 사용자 지정 VPC 내부의 EC2 인스턴스에서 실행되는 애플리케이션이 있습니다. VPC는 각각 오하이오와 버지니아 북부 지역에 있습니다. 팀은 퍼블릭 인터넷을 거치지 않고 인스턴스 간에 데이터를 전송하려고 합니다.이를 달성할 수 있는 단계 조합은 무엇입니까? (2개를 선택하세요.),각 지역에 VPC 엔드포인트를 배포하여 프라이빗 연결을 활성화합니다.,VPC 간에 VPC 피어링 연결을 설정합니다.,외부 전용 인터넷 게이트웨이를 만듭니다.,각 VPC의 퍼블릭 서브넷에서 NAT 게이트웨이를 시작합니다.,경로 테이블의 대상과 인스턴스 서브넷의 대상을 재구성합니다.,,0,,
udemy,CLF-01,268,"A news company is planning to use a Hardware Security Module (CloudHSM) in AWS for secure key storage of their web applications. You have launched the CloudHSM cluster but after just a few hours, a support staff mistakenly attempted to log in as the administrator three times using an invalid password in the Hardware Security Module. This has caused the HSM to be zeroized, which means that the encryption keys on it have been wiped. Unfortunately, you did not have a copy of the keys stored anywhere else. How can you obtain a new copy of the keys that you have stored on Hardware Security Module?",C,C,Use the Amazon CLI to get a copy of the keys.,Restore a snapshot of the Hardware Security Module.,The keys are lost permanently if you did not have a copy.,Contact AWS Support and they will provide you a copy of the keys.,,,,뉴스 회사는 웹 애플리케이션의 안전한 키 스토리지를 위해 AWS에서 하드웨어 보안 모듈(CloudHSM)을 사용할 계획입니다. CloudHSM 클러스터를 시작했지만 불과 몇 시간 후 지원 담당자가 실수로 하드웨어 보안 모듈에서 잘못된 암호를 사용하여 관리자로 세 번 로그인을 시도했습니다. 이로 인해 HSM이 초기화되어 HSM의 암호화 키가 지워졌습니다. 안타깝게도 다른 곳에 저장된 키 사본이 없습니다.하드웨어 보안 모듈에 저장한 키의 새 사본을 어떻게 얻을 수 있습니까?,Amazon CLI를 사용하여 키 사본을 가져옵니다.,하드웨어 보안 모듈의 스냅샷을 복원합니다.,복사본이 없으면 키가 영구적으로 손실됩니다.,AWS Support에 문의하면 키 사본을 제공받을 것입니다.,,,0,,
udemy,CLF-01,269,A company needs to accelerate the performance of its AI-powered medical diagnostic application by running its machine learning workloads on the edge of telecommunication carriers' 5G networks. The application must be deployed to a Kubernetes cluster and have role-based access control (RBAC) access to IAM users and roles for cluster authentication.Which of the following should the Solutions Architect implement to ensure single-digit millisecond latency for the application?,B,B,Launch the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Create VPC endpoints for the AWS Wavelength Zones and apply them to the Amazon EKS cluster. Install the AWS IAM Authenticator for Kubernetes (aws-iam-authenticator) to your cluster.,Launch the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Create node groups in Wavelength Zones for the Amazon EKS cluster via the AWS Wavelength service. Apply the AWS authenticator configuration map (aws-auth ConfigMap) to your cluster.,Host the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Set up node groups in AWS Wavelength Zones for the Amazon EKS cluster. Attach the Amazon EKS connector agent role (AmazonECSConnectorAgentRole) to your cluster and use AWS Control Tower for RBAC access.,Host the application to an Amazon EKS cluster and run the Kubernetes pods on AWS Fargate. Create node groups in AWS Wavelength Zones for the Amazon EKS cluster. Add the EKS pod execution IAM role (AmazonEKSFargatePodExecutionRole) to your cluster and ensure that the Fargate profile has the same IAM role as your Amazon EC2 node groups.,,,,회사는 통신 사업자의 5G 네트워크 에지에서 머신 러닝 워크로드를 실행하여 AI 기반 의료 진단 애플리케이션의 성능을 가속화해야 합니다. 애플리케이션은 Kubernetes 클러스터에 배포되어야 하며 클러스터 인증을 위한 IAM 사용자 및 역할에 대한 역할 기반 액세스 제어(RBAC) 액세스 권한이 있어야 합니다.솔루션 아키텍트가 애플리케이션에 대해 한 자릿수 밀리초의 대기 시간을 보장하기 위해 구현해야 하는 것은 다음 중 무엇입니까?,Amazon Elastic Kubernetes Service(Amazon EKS) 클러스터에서 애플리케이션을 시작합니다. AWS Wavelength Zone에 대한 VPC 엔드포인트를 생성하고 Amazon EKS 클러스터에 적용합니다. Kubernetes용 AWS IAM Authenticator( aws-iam-authenticator)를 클러스터에 설치합니다.,Amazon Elastic Kubernetes Service(Amazon EKS) 클러스터에서 애플리케이션을 시작합니다. AWS Wavelength 서비스를 통해 Amazon EKS 클러스터의 Wavelength Zone에 노드 그룹을 생성합니다. aws-auth ConfigMap클러스터에 AWS 인증자 구성 맵( )을 적용합니다 .,Amazon Elastic Kubernetes Service(Amazon EKS) 클러스터에 애플리케이션을 호스팅합니다. Amazon EKS 클러스터에 대한 AWS Wavelength Zone에서 노드 그룹을 설정합니다. Amazon EKS 커넥터 에이전트 역할( AmazonECSConnectorAgentRole)을 클러스터에 연결하고 RBAC 액세스를 위해 AWS Control Tower를 사용합니다.,애플리케이션을 Amazon EKS 클러스터에 호스팅하고 AWS Fargate에서 Kubernetes 포드를 실행합니다. Amazon EKS 클러스터에 대한 AWS Wavelength Zones에서 노드 그룹을 생성합니다. AmazonEKSFargatePodExecutionRole클러스터에 EKS 포드 실행 IAM 역할( )을 추가하고 Fargate 프로필에 Amazon EC2 노드 그룹과 동일한 IAM 역할이 있는지 확인합니다.,,,0,,
udemy,CLF-01,270,"A company has a web application hosted in their on-premises infrastructure that they want to migrate to AWS cloud. Your manager has instructed you to ensure that there is no downtime while the migration process is on-going. In order to achieve this, your team decided to divert 50% of the traffic to the new application in AWS and the other 50% to the application hosted in their on-premises infrastructure. Once the migration is over and the application works with no issues, a full diversion to AWS will be implemented. The company's VPC is connected to its on-premises network via an AWS Direct Connect connection.Which of the following are the possible solutions that you can implement to satisfy the above requirement? (Select TWO.)",AE,AE,Use Route 53 with Weighted routing policy to divert the traffic between the on-premises and AWS-hosted application. Divert 50% of the traffic to the new application in AWS and the other 50% to the application hosted in their on-premises infrastructure.,Use Route 53 with Failover routing policy to divert and proportion the traffic between the on-premises and AWS-hosted application. Divert 50% of the traffic to the new application in AWS and the other 50% to the application hosted in their on-premises infrastructure.,Use AWS Global Accelerator to divert and proportion the HTTP and HTTPS traffic between the on-premises and AWS-hosted application. Ensure that the on-premises network has an AnyCast static IP address and is connected to your VPC via a Direct Connect Gateway.,Use a Network Load balancer with Weighted Target Groups to divert the traffic between the on-premises and AWS-hosted application. Divert 50% of the traffic to the new application in AWS and the other 50% to the application hosted in their on-premises infrastructure.,Use an Application Elastic Load balancer with Weighted Target Groups to divert and proportion the traffic between the on-premises and AWS-hosted application. Divert 50% of the traffic to the new application in AWS and the other 50% to the application hosted in their on-premises infrastructure.,,,회사에는 AWS 클라우드로 마이그레이션하려는 온프레미스 인프라에서 호스팅되는 웹 애플리케이션이 있습니다. 관리자는 마이그레이션 프로세스가 진행되는 동안 다운타임이 발생하지 않도록 지시했습니다. 이를 달성하기 위해 팀은 트래픽의 50%를 AWS의 새 애플리케이션으로 전환하고 나머지 50%는 온프레미스 인프라에서 호스팅되는 애플리케이션으로 전환하기로 결정했습니다. 마이그레이션이 끝나고 애플리케이션이 문제 없이 작동하면 AWS로의 전체 전환이 구현됩니다. 회사의 VPC는 ​​AWS Direct Connect 연결을 통해 온프레미스 네트워크에 연결됩니다.위의 요구 사항을 충족하기 위해 구현할 수 있는 솔루션은 다음 중 무엇입니까? (2개를 선택하세요.),가중치 기반 라우팅 정책과 함께 Route 53을 사용하여 온프레미스와 AWS 호스팅 애플리케이션 간에 트래픽을 전환합니다. 트래픽의 50%는 AWS의 새 애플리케이션으로 전환하고 나머지 50%는 온프레미스 인프라에서 호스팅되는 애플리케이션으로 전환합니다.,장애 조치 라우팅 정책과 함께 Route 53을 사용하여 온프레미스와 AWS 호스팅 애플리케이션 간에 트래픽을 전환하고 비율을 조정합니다. 트래픽의 50%는 AWS의 새 애플리케이션으로 전환하고 나머지 50%는 온프레미스 인프라에서 호스팅되는 애플리케이션으로 전환합니다.,AWS Global Accelerator를 사용하여 온프레미스와 AWS 호스팅 애플리케이션 간에 HTTP 및 HTTPS 트래픽을 전환하고 비율을 조정합니다. 온프레미스 네트워크에 AnyCast 정적 IP 주소가 있고 Direct Connect 게이트웨이를 통해 VPC에 연결되어 있는지 확인하십시오.,가중 대상 그룹과 함께 네트워크 로드 밸런서를 사용하여 온프레미스와 AWS 호스팅 애플리케이션 간에 트래픽을 전환합니다. 트래픽의 50%는 AWS의 새 애플리케이션으로 전환하고 나머지 50%는 온프레미스 인프라에서 호스팅되는 애플리케이션으로 전환합니다.,가중 대상 그룹과 함께 애플리케이션 Elastic Load Balancer를 사용하여 온프레미스와 AWS 호스팅 애플리케이션 간에 트래픽을 전환하고 비율을 조정합니다. 트래픽의 50%는 AWS의 새 애플리케이션으로 전환하고 나머지 50%는 온프레미스 인프라에서 호스팅되는 애플리케이션으로 전환합니다.,,0,,
udemy,CLF-01,271,"A company needs to use Amazon S3 to store irreproducible financial documents. For their quarterly reporting, the files are required to be retrieved after a period of 3 months. There will be some occasions when a surprise audit will be held, which requires access to the archived data that they need to present immediately.What will you do to satisfy this requirement in a cost-effective way?",C,C,Use Amazon S3 -Intelligent Tiering,Use Amazon S3 Standard,Use Amazon S3 Standard - Infrequent Access,Use Amazon Glacier Deep Archive,,,,회사는 재현할 수 없는 재무 문서를 저장하기 위해 Amazon S3를 사용해야 합니다. 분기별 보고의 경우 3개월 후에 파일을 검색해야 합니다. 즉시 제출해야 하는 보관된 데이터에 대한 액세스가 필요한 깜짝 감사가 열리는 경우가 있습니다.비용 효율적인 방식으로 이 요구 사항을 충족하기 위해 무엇을 하시겠습니까?,Amazon S3 사용 - 지능형 계층화,Amazon S3 표준 사용,Amazon S3 Standard 사용 - 드문 액세스,Amazon Glacier Deep Archive 사용,,,0,,
udemy,CLF-01,272,A client is hosting their company website on a cluster of web servers that are behind a public-facing load balancer. The client also uses Amazon Route 53 to manage their public DNS.   How should the client configure the DNS zone apex record to point to the load balancer?,B,B,Create an A record pointing to the IP address of the load balancer.,Create an A record aliased to the load balancer DNS name.,Create an alias for CNAME record to the load balancer DNS name.,Create a CNAME record pointing to the load balancer DNS name.,,,,클라이언트는 공용 로드 밸런서 뒤에 있는 웹 서버 클러스터에서 회사 웹 사이트를 호스팅하고 있습니다. 또한 클라이언트는 Amazon Route 53을 사용하여 퍼블릭 DNS를 관리합니다.   클라이언트는 로드 밸런서를 가리키도록 DNS 영역 apex 레코드를 어떻게 구성해야 합니까?,로드 밸런서의 IP 주소를 가리키는 A 레코드를 생성합니다.,로드 밸런서 DNS 이름에 별칭이 지정된 A 레코드를 생성합니다.,로드 밸런서 DNS 이름에 대한 CNAME 레코드의 별칭을 만듭니다.,로드 밸런서 DNS 이름을 가리키는 CNAME 레코드를 생성합니다.,,,0,,
udemy,CLF-01,273,An investment bank has a distributed batch processing application which is hosted in an Auto Scaling group of Spot EC2 instances with an SQS queue. You configured your components to use client-side buffering so that the calls made from the client will be buffered first and then sent as a batch request to SQS. What is a period of time during which the SQS queue prevents other consuming components from receiving and processing a message?,A,A,Visibility Timeout,Receiving Timeout,Processing Timeout,Component Timeout,,,,투자 은행에는 SQS 대기열이 있는 스팟 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅되는 분산 배치 처리 애플리케이션이 있습니다. 클라이언트 측 버퍼링을 사용하도록 구성 요소를 구성하여 클라이언트에서 수행된 호출이 먼저 버퍼링된 다음 일괄 요청으로 SQS에 전송되도록 했습니다. SQS 대기열이 다른 소비 구성 요소가 메시지를 수신하고 처리하지 못하도록 막는 기간은 얼마입니까?,가시성 시간 초과,수신 시간 초과,처리 시간 초과,구성 요소 시간 초과,,,0,,
udemy,CLF-01,274,A social media company needs to capture the detailed information of all HTTP requests that went through their public-facing Application Load Balancer every five minutes. The client's IP address and network latencies must also be tracked. They want to use this data for analyzing traffic patterns and for troubleshooting their Docker applications orchestrated by the Amazon ECS Anywhere service.Which of the following options meets the customer requirements with the LEAST amount of overhead?,D,D,Integrate Amazon EventBridge (Amazon CloudWatch Events) metrics on the Application Load Balancer to capture the client IP address. Use Amazon CloudWatch Container Insights to analyze traffic patterns.,Install and run the AWS X-Ray daemon on the Amazon ECS cluster. Use the Amazon CloudWatch ServiceLens to analyze the traffic that goes through the application.,Enable AWS CloudTrail for their Application Load Balancer. Use the AWS CloudTrail Lake to analyze and troubleshoot the application traffic.,Enable access logs on the Application Load Balancer. Integrate the Amazon ECS cluster with Amazon CloudWatch Application Insights to analyze traffic patterns and simplify troubleshooting.,,,,소셜 미디어 회사는 5분마다 공개 Application Load Balancer를 통과한 모든 HTTP 요청의 세부 정보를 캡처해야 합니다. 클라이언트의 IP 주소와 네트워크 대기 시간도 추적해야 합니다. 트래픽 패턴을 분석하고 Amazon ECS Anywhere 서비스로 조정되는 Docker 애플리케이션의 문제를 해결하는 데 이 데이터를 사용하려고 합니다.다음 중 최소한의 오버헤드로 고객 요구 사항을 충족하는 옵션은 무엇입니까?,Application Load Balancer에서 Amazon EventBridge(Amazon CloudWatch Events) 지표를 통합하여 클라이언트 IP 주소를 캡처합니다. Amazon CloudWatch Container Insights를 사용하여 트래픽 패턴을 분석하십시오.,Amazon ECS 클러스터에 AWS X-Ray 데몬을 설치하고 실행합니다. Amazon CloudWatch ServiceLens를 사용하여 애플리케이션을 통과하는 트래픽을 분석합니다.,Application Load Balancer에 대해 AWS CloudTrail을 활성화합니다. AWS CloudTrail Lake를 사용하여 애플리케이션 트래픽을 분석하고 문제를 해결하십시오.,Application Load Balancer에서 액세스 로그를 활성화합니다. Amazon ECS 클러스터를 Amazon CloudWatch Application Insights와 통합하여 트래픽 패턴을 분석하고 문제 해결을 간소화합니다.,,,0,,
udemy,CLF-01,275,A company is running a batch job on an EC2 instance inside a private subnet. The instance gathers input data from an S3 bucket in the same region through a NAT Gateway. The company is looking for a solution that will reduce costs without imposing risks on redundancy or availability.Which solution will accomplish this?,B,B,Replace the NAT Gateway with a NAT instance hosted on a burstable instance type.,Remove the NAT Gateway and use a Gateway VPC endpoint to access the S3 bucket from the instance.,Re-assign the NAT Gateway to a lower EC2 instance type.,Deploy a Transit Gateway to peer connection between the instance and the S3 bucket.,,,,회사는 프라이빗 서브넷 내부의 EC2 인스턴스에서 배치 작업을 실행하고 있습니다. 인스턴스는 NAT 게이트웨이를 통해 동일한 리전의 S3 버킷에서 입력 데이터를 수집합니다. 회사는 중복성이나 가용성에 위험을 초래하지 않으면서 비용을 절감할 수 있는 솔루션을 찾고 있습니다.어떤 솔루션이 이를 달성할까요?,NAT 게이트웨이를 버스트 가능한 인스턴스 유형에서 호스팅되는 NAT 인스턴스로 교체하십시오.,NAT 게이트웨이를 제거하고 게이트웨이 VPC 엔드포인트를 사용하여 인스턴스에서 S3 버킷에 액세스합니다.,NAT 게이트웨이를 더 낮은 EC2 인스턴스 유형에 다시 할당합니다.,인스턴스와 S3 버킷 사이의 피어 연결에 Transit Gateway를 배포합니다.,,,0,,
udemy,CLF-01,276,A company plans to use Route 53 instead of an ELB to load balance the incoming request to the web application. The system is deployed to two EC2 instances to which the traffic needs to be distributed. You want to set a specific percentage of traffic to go to each instance.Which routing policy would you use?,D,D,Latency,Geolocation,Failover,Weighted,,,,회사는 ELB 대신 Route 53을 사용하여 웹 애플리케이션에 대한 수신 요청을 로드 밸런싱할 계획입니다. 시스템은 트래픽을 분산해야 하는 두 개의 EC2 인스턴스에 배포됩니다. 각 인스턴스로 이동할 트래픽의 특정 비율을 설정하려고 합니다.어떤 라우팅 정책을 사용하시겠습니까?,지연 시간,지리적 위치,장애 조치,가중,,,0,,
udemy,CLF-01,277,A company needs to accelerate the development of its GraphQL APIs for its new customer service portal. The solution must be serverless to lower the monthly operating cost of the business. Their GraphQL APIs must be accessible via HTTPS and have a custom domain.What solution should the Solutions Architect implement to meet the above requirements?,D,D,Launch an AWS Elastic Beanstalk environment and use Amazon Route 53 for the custom domain. Configure Domain Name System Security Extensions (DNSSEC) in the Route 53 hosted zone to enable HTTPS communication.,Host the application in the VMware Cloud on AWS service. Associate a custom domain to the GraphSQL APIs via the AWS Directory Service for Microsoft Active Directory and provide multiple domain controllers to enable HTTPS communication.,Deploy the GraphQL APIs as Kubernetes pods to AWS Fargate and AWS Outposts using Amazon EKS Anywhere for deployment. Create a custom domain using Amazon CloudFront and enable the Origin Shield feature to allow HTTPS communication to the GraphQL APIs.,Develop the application using the AWS AppSync service and use its built-in custom domain feature. Associate an SSL certificate to the AWS AppSync API using the AWS Certificate Manager (ACM) service to enable HTTPS communication.,,,,회사는 새로운 고객 서비스 포털을 위해 GraphQL API 개발을 가속화해야 합니다. 비즈니스의 월별 운영 비용을 낮추려면 솔루션이 서버리스여야 합니다. GraphQL API는 HTTPS를 통해 액세스할 수 있어야 하며 사용자 지정 도메인이 있어야 합니다.Solutions Architect는 위의 요구 사항을 충족하기 위해 어떤 솔루션을 구현해야 합니까?,AWS Elastic Beanstalk 환경을 시작하고 사용자 지정 도메인에 Amazon Route 53을 사용합니다. HTTPS 통신을 활성화하려면 Route 53 호스팅 영역에서 DNSSEC(Domain Name System Security Extensions)를 구성합니다.,VMware Cloud on AWS 서비스에서 애플리케이션을 호스팅합니다. Microsoft Active Directory용 AWS Directory Service를 통해 사용자 지정 도메인을 GraphSQL API에 연결하고 여러 도메인 컨트롤러를 제공하여 HTTPS 통신을 활성화합니다.,배포를 위해 Amazon EKS Anywhere를 사용하여 GraphQL API를 Kubernetes 포드로 AWS Fargate 및 AWS Outposts에 배포합니다. Amazon CloudFront를 사용하여 사용자 지정 도메인을 생성하고 Origin Shield 기능을 활성화하여 GraphQL API에 대한 HTTPS 통신을 허용합니다.,AWS AppSync 서비스를 사용하여 애플리케이션을 개발하고 내장된 사용자 지정 도메인 기능을 사용합니다. HTTPS 통신을 활성화하려면 AWS Certificate Manager(ACM) 서비스를 사용하여 SSL 인증서를 AWS AppSync API에 연결합니다.,,,0,,
udemy,CLF-01,278,"A company is using AWS IAM to manage access to AWS services. The Solutions Architect of the company created the following IAM policy for AWS Lambda:{   ""Version"": ""2012-10-17"",   ""Statement"": [   {     ""Effect"": ""Allow"",     ""Action"": [      ""lambda:CreateFunction"",     ""lambda:DeleteFunction""   ],    ""Resource"": ""*""}, {   ""Effect"": ""Deny"",   ""Action"": [    ""lambda:CreateFunction"",   ""lambda:DeleteFunction"",   ""lambda:InvokeFunction"",   ""lambda:TagResource""],  ""Resource"": ""*"",  ""Condition"": {    ""IpAddress"": {     ""aws:SourceIp"": ""187.5.104.11/32""    }   }  }  ] } Which of the following options are allowed by this policy?",C,C,Delete an AWS Lambda function using the 187.5.104.11/32 address.,Create an AWS Lambda function using the 187.5.104.11/32 address.,Create an AWS Lambda function using the 100.220.0.11/32 address.,Delete an AWS Lambda function from any network address.,,,,"회사에서 AWS IAM을 사용하여 AWS 서비스에 대한 액세스를 관리하고 있습니다. 회사의 Solutions Architect는 AWS Lambda에 대해 다음과 같은 IAM 정책을 생성했습니다.{   ""버전"" : ""2012-10-17"" ,    ""진술서"" : [    {     ""효과"" : ""허용"" ,      ""액션"" : [       ""lambda:CreateFunction"" ,     ""람다:DeleteFunction""   ],    ""자원"" : ""*"" }, {   ""효과"" : ""거부"" ,    ""액션"" : [     ""lambda:CreateFunction"" ,   ""lambda:DeleteFunction"" ,   ""lambda:InvokeFunction"" ,   ""람다:태그 리소스""],  ""자원"" : ""*"" ,   ""조건"" : {     ""IP 주소"" : {      ""aws:SourceIp"" : ""187.5.104.11/32""     }   }  }  ] } 다음 중 이 정책에서 허용하는 옵션은 무엇입니까?",주소를 사용하여 AWS Lambda 함수를 삭제합니다 187.5.104.11/32.,주소를 사용하여 AWS Lambda 함수를 생성합니다 187.5.104.11/32.,주소를 사용하여 AWS Lambda 함수를 생성합니다 100.220.0.11/32.,모든 네트워크 주소에서 AWS Lambda 함수를 삭제합니다.,,,0,,
udemy,CLF-01,279,A company needs secure access to its Amazon RDS for MySQL database that is used by multiple applications. Each IAM user must use a short-lived authentication token to connect to the database.Which of the following is the most suitable solution in this scenario?,D,D,Use an MFA token to access and connect to a database.,Use AWS Secrets Manager to generate and store short-lived authentication tokens.,Use AWS SSO to access the RDS database.,Use IAM DB Authentication and create database accounts using the AWS-provided AWSAuthenticationPlugin plugin in MySQL.,,,,회사는 여러 애플리케이션에서 사용하는 MySQL용 Amazon RDS 데이터베이스에 대한 보안 액세스가 필요합니다. 각 IAM 사용자는 단기 인증 토큰을 사용하여 데이터베이스에 연결해야 합니다.다음 중 이 시나리오에서 가장 적합한 솔루션은 무엇입니까?,MFA 토큰을 사용하여 데이터베이스에 액세스하고 연결합니다.,AWS Secrets Manager를 사용하여 단기 인증 토큰을 생성하고 저장합니다.,AWS SSO를 사용하여 RDS 데이터베이스에 액세스합니다.,AWSAuthenticationPluginIAM DB 인증을 사용하고 MySQL에서 AWS 제공 플러그인을 사용하여 데이터베이스 계정을 생성합니다 .,,,0,,
udemy,CLF-01,280,"A Solutions Architect is working for a multinational telecommunications company. The IT Manager wants to consolidate their log streams including the access, application, and security logs in one single system. Once consolidated, the company will analyze these logs in real-time based on heuristics. There will be some time in the future where the company will need to validate heuristics, which requires going back to data samples extracted from the last 12 hours.What is the best approach to meet this requirement?",B,B,"First, send all the log events to Amazon SQS then set up an Auto Scaling group of EC2 servers to consume the logs and finally, apply the heuristics.","First, send all of the log events to Amazon Kinesis then afterwards, develop a client process to apply heuristics on the logs.","First, set up an Auto Scaling group of EC2 servers then store the logs on Amazon S3 then finally, use EMR to apply heuristics on the logs.","First, configure Amazon Cloud Trail to receive custom logs and then use EMR to apply heuristics on the logs.",,,,"Solutions Architect는 다국적 통신 회사에서 일하고 있습니다. IT 관리자는 액세스, 애플리케이션 및 보안 로그를 포함한 로그 스트림을 단일 시스템에 통합하려고 합니다. 통합되면 회사는 휴리스틱을 기반으로 이러한 로그를 실시간으로 분석합니다. 앞으로 회사에서 지난 12시간 동안 추출한 데이터 샘플로 돌아가 휴리스틱을 검증해야 하는 시간이 있을 것입니다.이 요구 사항을 충족하는 가장 좋은 방법은 무엇입니까?",먼저 모든 로그 이벤트를 Amazon SQS로 보낸 다음 EC2 서버의 Auto Scaling 그룹을 설정하여 로그를 사용하고 마지막으로 휴리스틱을 적용합니다.,먼저 모든 로그 이벤트를 Amazon Kinesis로 보낸 다음 나중에 로그에 휴리스틱을 적용하는 클라이언트 프로세스를 개발합니다.,먼저 EC2 서버의 Auto Scaling 그룹을 설정한 다음 Amazon S3에 로그를 저장하고 마지막으로 EMR을 사용하여 로그에 휴리스틱을 적용합니다.,먼저 사용자 지정 로그를 수신하도록 Amazon Cloud Trail을 구성한 다음 EMR을 사용하여 로그에 휴리스틱을 적용합니다.,,,0,,
udemy,CLF-01,281,"An e-commerce application is using a fanout messaging pattern for its order management system. For every order, it sends an Amazon SNS message to an SNS topic, and the message is replicated and pushed to multiple Amazon SQS queues for parallel asynchronous processing. A Spot EC2 instance retrieves the message from each SQS queue and processes the message. There was an incident that while an EC2 instance is currently processing a message, the instance was abruptly terminated, and the processing was not completed in time. In this scenario, what happens to the SQS message?",C,C,The message will automatically be assigned to the same EC2 instance when it comes back online within or after the visibility timeout.,The message is deleted and becomes duplicated in the SQS when the EC2 instance comes online.,"When the message visibility timeout expires, the message becomes available for processing by other EC2 instances",The message will be sent to a Dead Letter Queue in AWS DataSync.,,,,전자 상거래 애플리케이션은 주문 관리 시스템에 대한 팬아웃 메시징 패턴을 사용하고 있습니다. 모든 주문에 대해 Amazon SNS 메시지를 SNS 주제로 보내고 메시지는 병렬 비동기 처리를 위해 여러 Amazon SQS 대기열에 복제 및 푸시됩니다. 스팟 EC2 인스턴스는 각 SQS 대기열에서 메시지를 검색하고 메시지를 처리합니다. EC2 인스턴스가 현재 메시지를 처리하는 동안 인스턴스가 갑자기 종료되어 처리가 제 시간에 완료되지 않는 문제가 발생했습니다.이 시나리오에서 SQS 메시지는 어떻게 됩니까?,메시지는 가시성 제한 시간 내 또는 이후에 다시 온라인 상태가 되면 동일한 EC2 인스턴스에 자동으로 할당됩니다.,EC2 인스턴스가 온라인 상태가 되면 메시지가 삭제되고 SQS에 복제됩니다.,메시지 가시성 제한 시간이 만료되면 다른 EC2 인스턴스에서 메시지를 처리할 수 있게 됩니다.,메시지는 AWS DataSync의 배달 못한 편지 대기열로 전송됩니다.,,,0,,
udemy,CLF-01,282,"A company plans to design a highly available architecture in AWS. They have two target groups with three EC2 instances each, which are added to an Application Load Balancer. In the security group of the EC2 instance, you have verified that port 80 for HTTP is allowed. However, the instances are still showing out of service from the load balancer.What could be the root cause of this issue?",A,A,The health check configuration is not properly defined.,The instances are using the wrong AMI.,The wrong subnet was used in your VPC,The wrong instance type was used for the EC2 instance.,,,,회사는 AWS에서 고가용성 아키텍처를 설계할 계획입니다. 여기에는 Application Load Balancer에 추가되는 각각 3개의 EC2 인스턴스가 있는 2개의 대상 그룹이 있습니다. EC2 인스턴스의 보안 그룹에서 HTTP용 포트 80이 허용되는지 확인했습니다. 그러나 인스턴스는 로드 밸런서에서 여전히 서비스 불능으로 표시됩니다.이 문제의 근본 원인은 무엇입니까?,상태 확인 구성이 제대로 정의되지 않았습니다.,인스턴스가 잘못된 AMI를 사용하고 있습니다.,VPC에서 잘못된 서브넷이 사용되었습니다.,EC2 인스턴스에 잘못된 인스턴스 유형이 사용되었습니다.,,,0,,
udemy,CLF-01,283,"An On-Demand EC2 instance is launched into a VPC subnet with the Network ACL configured to allow all inbound traffic and deny all outbound traffic. The instance’s security group has an inbound rule to allow SSH from any IP address and does not have any outbound rules. In this scenario, what are the changes needed to allow SSH connection to the instance?",D,D,Both the outbound security group and outbound network ACL need to be modified to allow outbound traffic.,The outbound security group needs to be modified to allow outbound traffic.,No action needed. It can already be accessed from any IP address using SSH.,The network ACL needs to be modified to allow outbound traffic.,,,,온디맨드 EC2 인스턴스는 네트워크 ACL이 모든 인바운드 트래픽을 허용하고 모든 아웃바운드 트래픽을 거부하도록 구성된 VPC 서브넷으로 시작됩니다. 인스턴스의 보안 그룹에는 모든 IP 주소에서 SSH를 허용하는 인바운드 규칙이 있으며 아웃바운드 규칙은 없습니다. 이 시나리오에서 인스턴스에 대한 SSH 연결을 허용하기 위해 필요한 변경 사항은 무엇입니까?,아웃바운드 트래픽을 허용하려면 아웃바운드 보안 그룹과 아웃바운드 네트워크 ACL을 모두 수정해야 합니다.,아웃바운드 트래픽을 허용하려면 아웃바운드 보안 그룹을 수정해야 합니다.,조치가 필요하지 않습니다. 이미 SSH를 사용하여 모든 IP 주소에서 액세스할 수 있습니다.,아웃바운드 트래픽을 허용하려면 네트워크 ACL을 수정해야 합니다.,,,0,,
udemy,CLF-01,284,"A research institute has developed simulation software that requires significant computational power. Currently, the software runs on a local server with limited resources, taking several hours to complete each simulation. The server has 32 virtual CPUs (vCPUs) and 256 GiB of memory. The institute plans to migrate the software to AWS. Their objective is to speed up the simulations by running them in parallel.As a Solutions Architect, which solution will achieve this goal with the LEAST operational overhead?",D,D,Use Lambda functions to process simulation tasks in parallel.,Run the simulations using AWS Fargate.,Consider using Amazon EC2 Spot Instances to run the simulations.,Utilize AWS Batch to manage the execution of the software.,,,,한 연구 기관에서 상당한 계산 능력이 필요한 시뮬레이션 소프트웨어를 개발했습니다. 현재 이 소프트웨어는 리소스가 제한된 로컬 서버에서 실행되며 각 시뮬레이션을 완료하는 데 몇 시간이 걸립니다. 서버에는 32개의 가상 CPU(vCPU)와 256GiB의 메모리가 있습니다. 연구소는 소프트웨어를 AWS로 마이그레이션할 계획입니다. 그들의 목표는 시뮬레이션을 병렬로 실행하여 시뮬레이션 속도를 높이는 것입니다.솔루션 아키텍트로서 최소한의 운영 오버헤드로 이 목표를 달성할 수 있는 솔루션은 무엇입니까?,Lambda 함수를 사용하여 시뮬레이션 작업을 병렬로 처리합니다.,AWS Fargate를 사용하여 시뮬레이션을 실행합니다.,Amazon EC2 스팟 인스턴스를 사용하여 시뮬레이션을 실행하는 것을 고려하십시오.,AWS Batch를 활용하여 소프트웨어 실행을 관리합니다.,,,0,,
udemy,CLF-01,285,A company plans to deploy a Docker-based batch application in AWS. The application will be used to process both mission-critical data as well as non-essential batch jobs.Which of the following is the most cost-effective option to use in implementing this architecture?,D,D,Use ECS as the container management service then set up Reserved EC2 Instances for processing both mission-critical and non-essential batch jobs.,Use ECS as the container management service then set up Spot EC2 Instances for processing both mission-critical and non-essential batch jobs.,Use ECS as the container management service then set up On-Demand EC2 Instances for processing both mission-critical and non-essential batch jobs.,Use ECS as the container management service then set up a combination of Reserved and Spot EC2 Instances for processing mission-critical and non-essential batch jobs respectively.,,,,회사에서 AWS에 Docker 기반 배치 애플리케이션을 배포할 계획입니다. 이 응용 프로그램은 미션 크리티컬 데이터와 중요하지 않은 배치 작업을 모두 처리하는 데 사용됩니다.다음 중 이 아키텍처를 구현하는 데 사용할 수 있는 가장 비용 효율적인 옵션은 무엇입니까?,ECS를 컨테이너 관리 서비스로 사용한 다음 미션 크리티컬 및 비필수 배치 작업을 모두 처리하기 위해 예약된 EC2 인스턴스를 설정합니다.,ECS를 컨테이너 관리 서비스로 사용한 다음 미션 크리티컬 및 비필수 배치 작업을 모두 처리하기 위해 스팟 EC2 인스턴스를 설정합니다.,ECS를 컨테이너 관리 서비스로 사용한 다음 미션 크리티컬 및 비필수 배치 작업을 모두 처리하기 위해 온디맨드 EC2 인스턴스를 설정합니다.,ECS를 컨테이너 관리 서비스로 사용한 다음 미션 크리티컬 및 비필수 배치 작업을 각각 처리하기 위해 예약 및 스팟 EC2 인스턴스의 조합을 설정합니다.,,,0,,
udemy,CLF-01,286,"The start-up company that you are working for has a batch job application that is currently hosted on an EC2 instance. It is set to process messages from a queue created in SQS with default settings. You configured the application to process the messages once a week. After 2 weeks, you noticed that not all messages are being processed by the application. What is the root cause of this issue?",D,D,The SQS queue is set to short-polling.,The batch job application is configured to long polling.,Missing permissions in SQS.,Amazon SQS has automatically deleted the messages that have been in a queue for more than the maximum message retention period.,,,,귀하가 근무하고 있는 스타트업 회사에는 현재 EC2 인스턴스에서 호스팅되는 배치 작업 애플리케이션이 있습니다. 기본 설정으로 SQS에서 생성된 대기열의 메시지를 처리하도록 설정됩니다. 일주일에 한 번 메시지를 처리하도록 애플리케이션을 구성했습니다. 2주 후에 애플리케이션에서 일부 메시지를 처리하지 않는 것을 확인했습니다. 이 문제의 근본 원인은 무엇입니까?,SQS 대기열은 짧은 폴링으로 설정됩니다.,배치 작업 애플리케이션이 긴 폴링으로 구성되었습니다.,SQS에서 권한이 누락되었습니다.,Amazon SQS는 최대 메시지 보존 기간을 초과하여 대기열에 있는 메시지를 자동으로 삭제했습니다.,,,0,,
udemy,CLF-01,287,"A data analytics company keeps a massive volume of data that they store in their on-premises data center. To scale their storage systems, they are looking for cloud-backed storage volumes that they can mount using Internet Small Computer System Interface (iSCSI) devices from their on-premises application servers. They have an on-site data analytics application that frequently accesses the latest data subsets locally while the older data are rarely accessed. You are required to minimize the need to scale the on-premises storage infrastructure while still providing their web application with low-latency access to the data.Which type of AWS Storage Gateway service will you use to meet the above requirements?",A,A,Volume Gateway in cached mode,Tape Gateway,File Gateway,Volume Gateway in stored mode,,,,데이터 분석 회사는 온프레미스 데이터 센터에 저장하는 방대한 양의 데이터를 보관합니다. 스토리지 시스템을 확장하기 위해 온프레미스 애플리케이션 서버에서 iSCSI(Internet Small Computer System Interface) 장치를 사용하여 마운트할 수 있는 클라우드 지원 스토리지 볼륨을 찾고 있습니다. 오래된 데이터는 거의 액세스하지 않는 반면 로컬에서 최신 데이터 하위 집합에 자주 액세스하는 현장 데이터 분석 애플리케이션이 있습니다. 데이터에 대한 대기 시간이 짧은 웹 애플리케이션을 계속 제공하면서 온프레미스 스토리지 인프라를 확장할 필요성을 최소화해야 합니다.위의 요구 사항을 충족하기 위해 어떤 유형의 AWS Storage Gateway 서비스를 사용하시겠습니까?,캐시 모드의 볼륨 게이트웨이,테이프 게이트웨이,파일 게이트웨이,저장 모드의 볼륨 게이트웨이,,,0,,
udemy,CLF-01,288,"A web application is hosted on an EC2 instance that processes sensitive financial information which is launched in a private subnet. All of the data are stored in an Amazon S3 bucket. Financial information is accessed by users over the Internet. The security team of the company is concerned that the Internet connectivity to Amazon S3 is a security risk.In this scenario, what will you do to resolve this security vulnerability in the most cost-effective manner?",A,A,Change the web architecture to access the financial data through a Gateway VPC Endpoint.,Change the web architecture to access the financial data hosted in your S3 bucket by creating a custom VPC endpoint service.,Change the web architecture to access the financial data in your S3 bucket through a VPN connection.,"Change the web architecture to access the financial data in S3 through an interface VPC endpoint, which is powered by AWS PrivateLink.",,,,웹 애플리케이션은 프라이빗 서브넷에서 시작되는 민감한 금융 정보를 처리하는 EC2 인스턴스에서 호스팅됩니다. 모든 데이터는 Amazon S3 버킷에 저장됩니다. 재무 정보는 인터넷을 통해 사용자가 액세스합니다. 회사의 보안 팀은 Amazon S3에 대한 인터넷 연결이 보안 위험이라고 우려하고 있습니다.이 시나리오에서 가장 비용 효율적인 방식으로 이 보안 취약점을 해결하기 위해 무엇을 하시겠습니까?,게이트웨이 VPC 엔드포인트를 통해 금융 데이터에 액세스하도록 웹 아키텍처를 변경합니다.,사용자 지정 VPC 엔드포인트 서비스를 생성하여 S3 버킷에서 호스팅되는 금융 데이터에 액세스하도록 웹 아키텍처를 변경합니다.,VPN 연결을 통해 S3 버킷의 금융 데이터에 액세스하도록 웹 아키텍처를 변경하십시오.,AWS PrivateLink에서 제공하는 인터페이스 VPC 엔드포인트를 통해 S3의 재무 데이터에 액세스하도록 웹 아키텍처를 변경합니다.,,,0,,
udemy,CLF-01,289,"A company has an On-Demand EC2 instance with an attached EBS volume. There is a scheduled job that creates a snapshot of this EBS volume every midnight at 12 AM when the instance is not used. One night, there has been a production incident where you need to perform a change on both the instance and on the EBS volume at the same time when the snapshot is currently taking place.Which of the following scenario is true when it comes to the usage of an EBS volume while the snapshot is in progress?",D,D,The EBS volume cannot be used until the snapshot completes.,The EBS volume cannot be detached or attached to an EC2 instance until the snapshot completes,The EBS volume can be used in read-only mode while the snapshot is in progress.,The EBS volume can be used while the snapshot is in progress.,,,,"회사에 EBS 볼륨이 연결된 온디맨드 EC2 인스턴스가 있습니다. 인스턴스가 사용되지 않는 매일 자정 오전 12시에 이 EBS 볼륨의 스냅샷을 생성하는 예약된 작업이 있습니다. 어느 날 밤, 현재 스냅샷이 생성되고 있을 때 인스턴스와 EBS 볼륨 모두에서 동시에 변경을 수행해야 하는 프로덕션 사고가 발생했습니다.스냅샷이 진행되는 동안 EBS 볼륨을 사용할 때 다음 중 참인 시나리오는 무엇입니까?",스냅샷이 완료될 때까지 EBS 볼륨을 사용할 수 없습니다.,스냅샷이 완료될 때까지 EBS 볼륨을 분리하거나 EC2 인스턴스에 연결할 수 없습니다.,EBS 볼륨은 스냅샷이 진행되는 동안 읽기 전용 모드로 사용할 수 있습니다.,스냅샷이 진행되는 동안 EBS 볼륨을 사용할 수 있습니다.,,,0,,
udemy,CLF-01,290,A multimedia company needs to deploy web services to an AWS region that they have never used before. The company currently has an IAM role for its Amazon EC2 instance that permits the instance to access Amazon DynamoDB. They want their EC2 instances in the new region to have the exact same privileges.What should be done to accomplish this?,D,D,"In the new Region, create a new IAM role and associated policies then assign it to the new instance.",Duplicate the IAM role and associated policies to the new region and attach it to the instances.,Create an Amazon Machine Image (AMI) of the instance and copy it to the new region.,Assign the existing IAM role to instances in the new region.,,,,멀티미디어 회사는 이전에 사용한 적이 없는 AWS 리전에 웹 서비스를 배포해야 합니다. 회사는 현재 인스턴스가 Amazon DynamoDB에 액세스하도록 허용하는 Amazon EC2 인스턴스에 대한 IAM 역할을 가지고 있습니다. 그들은 새 지역의 EC2 인스턴스가 정확히 동일한 권한을 갖기를 원합니다.이를 달성하려면 어떻게 해야 합니까?,새 리전에서 새 IAM 역할 및 관련 정책을 생성한 다음 새 인스턴스에 할당합니다.,IAM 역할 및 관련 정책을 새 지역에 복제하고 인스턴스에 연결합니다.,인스턴스의 Amazon 머신 이미지(AMI)를 생성하고 새 리전에 복사합니다.,새 리전의 인스턴스에 기존 IAM 역할을 할당합니다.,,,0,,
udemy,CLF-01,291,"An application is hosted in an On-Demand EC2 instance and is using Amazon SDK to communicate to other AWS services such as S3, DynamoDB, and many others. As part of the upcoming IT audit, you need to ensure that all API calls to your AWS resources are logged and durably stored.  Which is the most suitable service that you should use to meet this requirement?",A,A,AWS CloudTrail,Amazon API Gateway,AWS X-Ray,Amazon CloudWatch,,,,"애플리케이션은 온디맨드 EC2 인스턴스에서 호스팅되며 Amazon SDK를 사용하여 S3, DynamoDB 등의 다른 AWS 서비스와 통신합니다. 예정된 IT 감사의 일환으로 AWS 리소스에 대한 모든 API 호출이 기록되고 지속적으로 저장되는지 확인해야 합니다.  이 요구 사항을 충족하기 위해 사용해야 하는 가장 적합한 서비스는 무엇입니까?",AWS CloudTrail,아마존 API 게이트웨이,AWS 엑스레이,아마존 클라우드워치,,,0,,
udemy,CLF-01,292,A company deployed a web application that stores static assets in an Amazon Simple Storage Service (S3) bucket. The Solutions Architect expects the S3 bucket to immediately receive over 2000 PUT requests and 3500 GET requests per second at peak hour.What should the Solutions Architect do to ensure optimal performance?,A,A,Do nothing. Amazon S3 will automatically manage performance at this scale.,Use a predictable naming scheme in the key names such as sequential numbers or date time sequences.,Use Byte-Range Fetches to retrieve multiple ranges of an object data per GET request.,Add a random prefix to the key names.,,,,한 회사에서 Amazon Simple Storage Service(S3) 버킷에 정적 자산을 저장하는 웹 애플리케이션을 배포했습니다. Solutions Architect는 피크 시간에 S3 버킷이 초당 2000개 이상의 PUT 요청과 3500개 이상의 GET 요청을 즉시 수신할 것으로 예상합니다.솔루션 설계자는 최적의 성능을 보장하기 위해 무엇을 해야 합니까?,아무것도하지 마세요. Amazon S3는 이 규모에서 자동으로 성능을 관리합니다.,일련 번호 또는 날짜 시간 순서와 같은 키 이름에 예측 가능한 명명 체계를 사용합니다.,Byte-Range Fetches를 사용하여 GET 요청당 개체 데이터의 여러 범위를 검색합니다.,키 이름에 임의의 접두사를 추가합니다.,,,0,,
udemy,CLF-01,293,"A local bank has an in-house application that handles sensitive financial data in a private subnet. After the data is processed by the EC2 worker instances, they will be delivered to S3 for ingestion by other services.How should you design this solution so that the data does not pass through the public Internet?",B,B,Configure a Transit gateway along with a corresponding route entry that directs the data to S3.,Configure a VPC Endpoint along with a corresponding route entry that directs the data to S3.,Provision a NAT gateway in the private subnet with a corresponding route entry that directs the data to S3.,Create an Internet gateway in the public subnet with a corresponding route entry that directs the data to S3.,,,,지역 은행에는 프라이빗 서브넷에서 민감한 금융 데이터를 처리하는 사내 애플리케이션이 있습니다. 데이터는 EC2 작업자 인스턴스에서 처리된 후 다른 서비스에서 수집할 수 있도록 S3로 전달됩니다.데이터가 공용 인터넷을 통과하지 않도록 이 솔루션을 어떻게 설계해야 합니까?,데이터를 S3로 보내는 해당 경로 항목과 함께 Transit gateway를 구성합니다.,데이터를 S3로 보내는 해당 경로 항목과 함께 VPC 끝점을 구성합니다.,데이터를 S3로 보내는 해당 경로 항목을 사용하여 프라이빗 서브넷에 NAT 게이트웨이를 프로비저닝합니다.,데이터를 S3로 보내는 해당 경로 항목을 사용하여 퍼블릭 서브넷에 인터넷 게이트웨이를 생성합니다.,,,0,,
udemy,CLF-01,294,"In a startup company you are working for, you are asked to design a web application that requires a NoSQL database that has no limit on the storage size for a given table. The startup is still new in the market and it has very limited human resources who can take care of the database infrastructure. Which is the most suitable service that you can implement that provides a fully managed, scalable and highly available NoSQL service?",C,C,SimpleDB,Amazon Neptune,DynamoDB,Amazon Aurora,,,,당신이 일하고 있는 스타트업 회사에서 당신은 주어진 테이블에 대한 스토리지 크기에 제한이 없는 NoSQL 데이터베이스가 필요한 웹 애플리케이션을 설계하라는 요청을 받았습니다. 이 스타트업은 아직 시장에 나온 지 얼마 되지 않았고 데이터베이스 인프라를 관리할 수 있는 인적 자원이 매우 제한적입니다.완벽하게 관리되고 확장 가능하며 가용성이 높은 NoSQL 서비스를 제공하는 구현 가능한 가장 적합한 서비스는 무엇입니까?,심플DB,아마존 해왕성,DynamoDB,Amazon Aurora,,,0,,
udemy,CLF-01,295,"A Solutions Architect joined a large tech company with an existing Amazon VPC. When reviewing the Auto Scaling events, the Architect noticed that their web application is scaling up and down multiple times within the hour.What design change could the Architect make to optimize cost while preserving elasticity?",B,B,Increase the base number of Auto Scaling instances for the Auto Scaling group,Change the cooldown period of the Auto Scaling group and set the CloudWatch metric to a higher threshold,Add provisioned IOPS to the instances,Increase the instance type in the launch configuration,,,,Solutions Architect는 기존 Amazon VPC를 사용하여 대규모 기술 회사에 합류했습니다. Auto Scaling 이벤트를 검토할 때 Architect는 웹 애플리케이션이 한 시간 내에 여러 번 확장 및 축소되고 있음을 확인했습니다.건축가는 탄력성을 유지하면서 비용을 최적화하기 위해 어떤 설계 변경을 할 수 있습니까?,Auto Scaling 그룹에 대한 Auto Scaling 인스턴스의 기본 수를 늘립니다.,Auto Scaling 그룹의 휴지 기간을 변경하고 CloudWatch 지표를 더 높은 임계값으로 설정합니다.,프로비저닝된 IOPS를 인스턴스에 추가,시작 구성에서 인스턴스 유형을 늘립니다.,,,0,,
udemy,CLF-01,296,"A company has multiple AWS Site-to-Site VPN connections placed between their VPCs and their remote network. During peak hours, many employees are experiencing slow connectivity issues, which limits their productivity. The company has asked a solutions architect to scale the throughput of the VPN connections.Which solution should the architect carry out?",B,B,Re-route some of the VPN connections to a secondary customer gateway device on the remote network’s end.,Associate the VPCs to an Equal Cost Multipath Routing (ECMR)-enabled transit gateway and attach additional VPN tunnels.,Modify the VPN configuration by increasing the number of tunnels to scale the throughput.,Add more virtual private gateways to a VPC and enable Equal Cost Multipath Routing (ECMR) to get higher VPN bandwidth.,,,,회사에는 VPC와 원격 네트워크 사이에 배치된 여러 AWS Site-to-Site VPN 연결이 있습니다. 피크 시간 동안 많은 직원들이 느린 연결 문제를 경험하여 생산성을 제한합니다. 회사는 솔루션 설계자에게 VPN 연결의 처리량을 확장하도록 요청했습니다.아키텍트는 어떤 솔루션을 수행해야 할까요?,일부 VPN 연결을 원격 네트워크 끝에 있는 보조 고객 게이트웨이 디바이스로 다시 라우팅합니다.,VPC를 ECMR(Equal Cost Multipath Routing) 지원 전송 게이트웨이에 연결하고 추가 VPN 터널을 연결합니다.,처리량을 확장하기 위해 터널 수를 늘려 VPN 구성을 수정합니다.,VPC에 더 많은 가상 프라이빗 게이트웨이를 추가하고 ECMR(Equal Cost Multipath Routing)을 활성화하여 더 높은 VPN 대역폭을 얻으십시오.,,,0,,
udemy,CLF-01,297,"A company has multiple research departments that have deployed several resources to the AWS cloud. The departments are free to provision their own resources as they are needed. To ensure normal operations, the company wants to track its AWS resource usage so that it is not reaching the AWS service quotas unexpectedly.Which combination of actions should the Solutions Architect implement to meet the company requirements? (Select TWO.)",AD,AD,Write an AWS Lambda function that refreshes the AWS Trusted Advisor Service Limits checks and set it to run every 24 hours.,Query the AWS Trusted Advisor Service Limits check every 24 hours by calling the DescribeTrustedAdvisorChecks API operation. Ensure that your AWS account has a Developer support plan.,Create an Amazon Simple Notification Service (Amazon SNS) topic and configure it as a target for notifications.,Capture the events using Amazon EventBridge (Amazon CloudWatch Events) and use an Amazon Simple Notification Service (Amazon SNS) topic as the target for notifications.,Utilize the AWS managed rule on AWS Config to monitor AWS resource service quotas. Schedule this checking using an AWS Lambda function.,,,회사에는 여러 리소스를 AWS 클라우드에 배포한 여러 연구 부서가 있습니다. 부서는 필요에 따라 자체 리소스를 자유롭게 프로비저닝할 수 있습니다. 정상적인 운영을 보장하기 위해 회사는 예기치 않게 AWS 서비스 할당량에 도달하지 않도록 AWS 리소스 사용량을 추적하려고 합니다.회사 요구 사항을 충족하기 위해 Solutions Architect가 구현해야 하는 작업 조합은 무엇입니까? (2개를 선택하세요.),AWS Trusted Advisor Service Limits 검사를 새로 고치고 24시간마다 실행되도록 설정하는 AWS Lambda 함수를 작성합니다.,API 작업을 호출하여 24시간마다 AWS Trusted Advisor 서비스 제한 검사를 쿼리합니다 DescribeTrustedAdvisorChecks. AWS 계정에 개발자 지원 계획이 있는지 확인하십시오.,Amazon Simple Notification Service(Amazon SNS) 주제를 생성하고 알림 대상으로 구성합니다.,Amazon EventBridge(Amazon CloudWatch Events)를 사용하여 이벤트를 캡처하고 Amazon Simple Notification Service(Amazon SNS) 주제를 알림 대상으로 사용합니다.,AWS Config에서 AWS 관리형 규칙을 활용하여 AWS 리소스 서비스 할당량을 모니터링합니다. AWS Lambda 함수를 사용하여 이 확인을 예약하십시오.,,0,,
udemy,CLF-01,298,"A company has a web application hosted on a fleet of EC2 instances located in two Availability Zones that are all placed behind an Application Load Balancer. As a Solutions Architect, you have to add a health check configuration to ensure your application is highly-available.Which health checks will you implement?",C,C,TCP health check,FTP health check,HTTP or HTTPS health check,ICMP health check,,,,회사에는 모두 Application Load Balancer 뒤에 배치된 두 개의 가용 영역에 있는 EC2 인스턴스 플릿에서 호스팅되는 웹 애플리케이션이 있습니다. 솔루션 설계자는 애플리케이션의 고가용성을 보장하기 위해 상태 확인 구성을 추가해야 합니다.어떤 상태 확인을 구현할 예정인가요?,TCP 상태 확인,FTP 상태 확인,HTTP 또는 HTTPS 상태 확인,ICMP 상태 확인,,,0,,
udemy,CLF-01,299,"A company has recently adopted a hybrid cloud architecture and is planning to migrate a database hosted on-premises to AWS. The database currently has over 50 TB of consumer data, handles highly transactional (OLTP) workloads, and is expected to grow. The Solutions Architect should ensure that the database is ACID-compliant and can handle complex queries of the application. Which type of database service should the Architect use?",C,C,Amazon Redshift,Amazon DynamoDB,Amazon Aurora,Amazon RDS,,,,한 회사가 최근 하이브리드 클라우드 아키텍처를 채택했으며 온프레미스에서 호스팅되는 데이터베이스를 AWS로 마이그레이션할 계획입니다. 이 데이터베이스는 현재 50TB 이상의 소비자 데이터를 보유하고 있으며 높은 트랜잭션(OLTP) 워크로드를 처리하며 증가할 것으로 예상됩니다. Solutions Architect는 데이터베이스가 ACID를 준수하고 애플리케이션의 복잡한 쿼리를 처리할 수 있는지 확인해야 합니다.Architect는 어떤 유형의 데이터베이스 서비스를 사용해야 합니까?,아마존 레드시프트,아마존 다이나모DB,Amazon Aurora,아마존 RDS,,,0,,
udemy,CLF-01,300,A startup launched a fleet of on-demand EC2 instances to host a massively multiplayer online role-playing game (MMORPG). The EC2 instances are configured with Auto Scaling and AWS Systems Manager.What can be used to configure the EC2 instances without having to establish an RDP or SSH connection to each instance?,A,A,Run Command,EC2Config,AWS CodePipeline,AWS Config,,,,한 스타트업이 MMORPG(대규모 멀티플레이어 온라인 롤 플레잉 게임)를 호스팅하기 위해 온디맨드 EC2 인스턴스 플릿을 출시했습니다. EC2 인스턴스는 Auto Scaling 및 AWS Systems Manager로 구성됩니다.각 인스턴스에 대한 RDP 또는 SSH 연결을 설정하지 않고 EC2 인스턴스를 구성하는 데 사용할 수 있는 것은 무엇입니까?,실행 명령,EC2Config,AWS CodePipeline,AWS 구성,,,0,,
udemy,CLF-01,301,"A leading e-commerce company is in need of a storage solution that can be simultaneously accessed by 1000 Linux servers in multiple availability zones. The servers are hosted in EC2 instances that use a hierarchical directory structure via the NFSv4 protocol. The service should be able to handle the rapidly changing data at scale while still maintaining high performance. It should also be highly durable and highly available whenever the servers will pull data from it, with little need for management. As the Solutions Architect, which of the following services is the most cost-effective choice that you should use to meet the above requirement?",A,A,Amazon EFS,Amazon FSx for Windows File Server,Amazon S3,Amazon EBS,,,,선도적인 전자 상거래 회사는 여러 가용 영역에서 1000개의 Linux 서버가 동시에 액세스할 수 있는 스토리지 솔루션이 필요합니다. 서버는 NFSv4 프로토콜을 통해 계층적 디렉터리 구조를 사용하는 EC2 인스턴스에서 호스팅됩니다. 서비스는 고성능을 유지하면서 빠르게 변화하는 데이터를 대규모로 처리할 수 있어야 합니다. 또한 관리가 거의 필요 없이 서버가 데이터를 가져올 때마다 내구성과 가용성이 높아야 합니다.Solutions Architect로서 다음 중 위의 요구 사항을 충족하기 위해 사용해야 하는 가장 비용 효율적인 선택은 무엇입니까?,아마존 EFS,Windows 파일 서버용 Amazon FSx,아마존 S3,아마존 EBS,,,0,,
udemy,CLF-01,302,"A healthcare company stores sensitive patient health records in their on-premises storage systems. These records must be kept indefinitely and protected from any type of modifications once they are stored. Compliance regulations mandate that the records must have granular access control and each data access must be audited at all levels. Currently, there are millions of obsolete records that are not accessed by their web application, and their on-premises storage is quickly running out of space. The Solutions Architect must design a solution to immediately move existing records to AWS and support the ever-growing number of new health records.Which of the following is the most suitable solution that the Solutions Architect should implement to meet the above requirements?",B,B,Set up AWS DataSync to move the existing health records from the on-premises network to the AWS Cloud. Launch a new Amazon S3 bucket to store existing and new records. Enable AWS CloudTrail with Management Events and Amazon S3 Object Lock in the bucket.,Set up AWS DataSync to move the existing health records from the on-premises network to the AWS Cloud. Launch a new Amazon S3 bucket to store existing and new records. Enable AWS CloudTrail with Data Events and Amazon S3 Object Lock in the bucket.,Set up AWS Storage Gateway to move the existing health records from the on-premises network to the AWS Cloud. Launch a new Amazon S3 bucket to store existing and new records. Enable AWS CloudTrail with Management Events and Amazon S3 Object Lock in the bucket.,Set up AWS Storage Gateway to move the existing health records from the on-premises network to the AWS Cloud. Launch an Amazon EBS-backed EC2 instance to store both the existing and new records. Enable Amazon S3 server access logging and S3 Object Lock in the bucket.,,,,의료 회사는 온프레미스 스토리지 시스템에 민감한 환자 건강 기록을 저장합니다. 이러한 기록은 무기한으로 보관되어야 하며 일단 저장되면 모든 유형의 수정으로부터 보호되어야 합니다. 규정 준수 규정에 따르면 레코드에는 세분화된 액세스 제어가 있어야 하며 각 데이터 액세스는 모든 수준에서 감사되어야 합니다. 현재 웹 애플리케이션에서 액세스할 수 없는 사용되지 않는 수백만 개의 레코드가 있으며 온프레미스 스토리지 공간이 빠르게 부족합니다. Solutions Architect는 기존 기록을 즉시 AWS로 옮기고 계속 증가하는 새로운 건강 기록을 지원하는 솔루션을 설계해야 합니다.솔루션 아키텍트가 위의 요구 사항을 충족하기 위해 구현해야 하는 가장 적합한 솔루션은 다음 중 무엇입니까?,온프레미스 네트워크에서 AWS 클라우드로 기존 상태 레코드를 이동하도록 AWS DataSync를 설정합니다. 새 Amazon S3 버킷을 시작하여 기존 레코드와 새 레코드를 저장합니다. 버킷에서 관리 이벤트 및 Amazon S3 객체 잠금으로 AWS CloudTrail을 활성화합니다.,온프레미스 네트워크에서 AWS 클라우드로 기존 상태 레코드를 이동하도록 AWS DataSync를 설정합니다. 새 Amazon S3 버킷을 시작하여 기존 레코드와 새 레코드를 저장합니다. 버킷에서 데이터 이벤트 및 Amazon S3 객체 잠금으로 AWS CloudTrail을 활성화합니다.,온프레미스 네트워크에서 AWS 클라우드로 기존 상태 레코드를 이동하도록 AWS Storage Gateway를 설정합니다. 새 Amazon S3 버킷을 시작하여 기존 레코드와 새 레코드를 저장합니다. 버킷에서 관리 이벤트 및 Amazon S3 객체 잠금으로 AWS CloudTrail을 활성화합니다.,온프레미스 네트워크에서 AWS 클라우드로 기존 상태 레코드를 이동하도록 AWS Storage Gateway를 설정합니다. Amazon EBS 지원 EC2 인스턴스를 시작하여 기존 레코드와 새 레코드를 모두 저장합니다. 버킷에서 Amazon S3 서버 액세스 로깅 및 S3 객체 잠금을 활성화합니다.,,,0,,
udemy,CLF-01,303,"A wellness company is currently working on a wearable device that monitors key health metrics such as heart rate, sleep, and steps per day. The device is designed to send data to an Amazon S3 bucket for storage and analysis. On a daily basis, the device produces 1 MB of data. In order to quickly process and summarize this data, the company requires 512 MB of memory and must complete the task within a maximum of 10 seconds.Which solution can fulfill these requirements in the MOST cost-effective manner?",B,B,Use Amazon Kinesis Data Firehose to send the data from the device to Amazon S3. Process the data on an EC2 instance with at least 512 MB of memory.,Use AWS Lambda with a Python library for processing.,Store the data in Amazon Redshift and process it with AWS Lambda.,Create an AWS Glue PySpark job to process the data.,,,,"웰빙 회사는 현재 심박수, 수면 및 하루 걸음 수와 같은 주요 건강 메트릭을 모니터링하는 웨어러블 장치를 개발하고 있습니다. 이 장치는 저장 및 분석을 위해 데이터를 Amazon S3 버킷으로 보내도록 설계되었습니다. 장치는 매일 1MB의 데이터를 생성합니다. 이 데이터를 빠르게 처리하고 요약하기 위해 회사는 512MB의 메모리가 필요하고 최대 10초 이내에 작업을 완료해야 합니다.가장 비용 효율적인 방식으로 이러한 요구 사항을 충족할 수 있는 솔루션은 무엇입니까?",Amazon Kinesis Data Firehose를 사용하여 디바이스에서 Amazon S3로 데이터를 보냅니다. 메모리가 512MB 이상인 EC2 인스턴스에서 데이터를 처리합니다.,처리를 위해 Python 라이브러리와 함께 AWS Lambda를 사용합니다.,데이터를 Amazon Redshift에 저장하고 AWS Lambda로 처리합니다.,데이터를 처리할 AWS Glue PySpark 작업을 생성합니다.,,,0,,
udemy,CLF-01,304,A startup needs to use a shared file system for its .NET web application running on an Amazon EC2 Windows instance. The file system must provide a high level of throughput and IOPS that can also be integrated with Microsoft Active Directory.Which is the MOST suitable service that you should use to achieve this requirement?,C,C,Amazon EBS Provisioned IOPS SSD volumes,Amazon Elastic File System,Amazon FSx for Windows File Server,AWS Storage Gateway - File Gateway,,,,스타트업은 Amazon EC2 Windows 인스턴스에서 실행되는 .NET 웹 애플리케이션에 공유 파일 시스템을 사용해야 합니다. 파일 시스템은 Microsoft Active Directory와도 통합될 수 있는 높은 수준의 처리량과 IOPS를 제공해야 합니다.이 요구 사항을 달성하기 위해 사용해야 하는 가장 적합한 서비스는 무엇입니까?,Amazon EBS 프로비저닝된 IOPS SSD 볼륨,아마존 탄력적 파일 시스템,Windows 파일 서버용 Amazon FSx,AWS Storage Gateway - 파일 게이트웨이,,,0,,
udemy,CLF-01,305,"A company has an application hosted in an Amazon ECS Cluster behind an Application Load Balancer. The Solutions Architect is building a sophisticated web filtering solution that allows or blocks web requests based on the country that the requests originate from. However, the solution should still allow specific IP addresses from that country. Which combination of steps should the Architect implement to satisfy this requirement? (Select TWO.)",DE,DE,Set up a geo match condition in the Application Load Balancer that blocks requests from a specific country.,Place a Transit Gateway in front of the VPC where the application is hosted and set up Network ACLs that block requests that originate from a specific country.,"In the Application Load Balancer, create a listener rule that explicitly allows requests from approved IP addresses.",Add another rule in the AWS WAF web ACL with a geo match condition that blocks requests that originate from a specific country.,"Using AWS WAF, create a web ACL with a rule that explicitly allows requests from approved IP addresses declared in an IP Set.",,,회사에는 Application Load Balancer 뒤의 Amazon ECS 클러스터에서 호스팅되는 애플리케이션이 있습니다. Solutions Architect는 요청이 발생한 국가를 기반으로 웹 요청을 허용하거나 차단하는 정교한 웹 필터링 솔루션을 구축하고 있습니다. 그러나 솔루션은 여전히 ​​해당 국가의 특정 IP 주소를 허용해야 합니다.Architect는 이 요구 사항을 충족하기 위해 어떤 단계 조합을 구현해야 합니까? (2개를 선택하세요.),특정 국가의 요청을 차단하는 Application Load Balancer에서 지역 일치 조건을 설정합니다.,애플리케이션이 호스팅되는 VPC 앞에 Transit Gateway를 배치하고 특정 국가에서 시작되는 요청을 차단하는 네트워크 ACL을 설정합니다.,Application Load Balancer에서 승인된 IP 주소의 요청을 명시적으로 허용하는 리스너 규칙을 생성합니다.,특정 국가에서 시작되는 요청을 차단하는 지리적 일치 조건을 사용하여 AWS WAF 웹 ACL에 다른 규칙을 추가합니다.,AWS WAF를 사용하여 IP 집합에 선언된 승인된 IP 주소의 요청을 명시적으로 허용하는 규칙으로 웹 ACL을 생성합니다.,,0,,
udemy,CLF-01,306,A leading media company has recently adopted a hybrid cloud architecture which requires them to migrate their application servers and databases in AWS. One of their applications requires a heterogeneous database migration in which you need to transform your on-premises Oracle database to PostgreSQL in AWS. This entails a schema and code transformation before the proper data migration starts.   Which of the following options is the most suitable approach to migrate the database in AWS?,D,D,"Configure a Launch Template that automatically converts the source schema and code to match that of the target database. Then, use the AWS Database Migration Service to migrate data from the source database to the target database.",Heterogeneous database migration is not supported in AWS. You have to transform your database first to PostgreSQL and then migrate it to RDS.,Use Amazon Neptune to convert the source schema and code to match that of the target database in RDS. Use the AWS Batch to effectively migrate the data from the source database to the target database in a batch process.,"First, use the AWS Schema Conversion Tool to convert the source schema and application code to match that of the target database, and then use the AWS Database Migration Service to migrate data from the source database to the target database.",,,,선도적인 미디어 회사는 최근 애플리케이션 서버와 데이터베이스를 AWS로 마이그레이션해야 하는 하이브리드 클라우드 아키텍처를 채택했습니다. 그들의 애플리케이션 중 하나는 온프레미스 Oracle 데이터베이스를 AWS의 PostgreSQL로 변환해야 하는 이기종 데이터베이스 마이그레이션이 필요합니다. 여기에는 적절한 데이터 마이그레이션이 시작되기 전에 스키마 및 코드 변환이 수반됩니다.   다음 중 AWS에서 데이터베이스를 마이그레이션하는 가장 적합한 방법은 무엇입니까?,대상 데이터베이스와 일치하도록 소스 스키마 및 코드를 자동으로 변환하는 시작 템플릿을 구성합니다. 그런 다음 AWS Database Migration Service를 사용하여 원본 데이터베이스에서 대상 데이터베이스로 데이터를 마이그레이션합니다.,이기종 데이터베이스 마이그레이션은 AWS에서 지원되지 않습니다. 먼저 데이터베이스를 PostgreSQL로 변환한 다음 RDS로 마이그레이션해야 합니다.,Amazon Neptune을 사용하여 RDS의 대상 데이터베이스와 일치하도록 소스 스키마 및 코드를 변환합니다. AWS Batch를 사용하여 배치 프로세스에서 원본 데이터베이스에서 대상 데이터베이스로 데이터를 효과적으로 마이그레이션합니다.,먼저 AWS Schema Conversion Tool을 사용하여 원본 스키마와 애플리케이션 코드를 대상 데이터베이스와 일치하도록 변환한 다음 AWS Database Migration Service를 사용하여 원본 데이터베이스에서 대상 데이터베이스로 데이터를 마이그레이션합니다.,,,0,,
udemy,CLF-01,307,A company has an infrastructure that allows EC2 instances from a private subnet to fetch objects from Amazon S3 via a NAT Instance. The company’s Solutions Architect was instructed to lower down the cost incurred by the current solution.How should the Solutions Architect redesign the architecture in the most cost-efficient manner?,A,A,Remove the NAT instance and create an S3 gateway endpoint to access S3 objects.,Remove the NAT instance and create an S3 interface endpoint to access S3 objects.,Use a smaller instance type for the NAT instance.,Replace the NAT instance with NAT Gateway to access S3 objects.,,,,회사에는 프라이빗 서브넷의 EC2 인스턴스가 NAT 인스턴스를 통해 Amazon S3에서 개체를 가져올 수 있도록 하는 인프라가 있습니다. 회사의 솔루션 설계자는 현재 솔루션에서 발생하는 비용을 낮추라는 지시를 받았습니다.Solutions Architect는 어떻게 가장 비용 효율적인 방식으로 아키텍처를 재설계해야 합니까?,NAT 인스턴스를 제거하고 S3 객체에 액세스할 S3 게이트웨이 엔드포인트를 생성합니다.,NAT 인스턴스를 제거하고 S3 객체에 액세스하기 위한 S3 인터페이스 엔드포인트를 생성합니다.,NAT 인스턴스에는 더 작은 인스턴스 유형을 사용하십시오.,NAT 인스턴스를 NAT 게이트웨이로 교체하여 S3 객체에 액세스합니다.,,,0,,
udemy,CLF-01,308,A company has a running m5ad.large EC2 instance with a default attached 75 GB SSD instance-store backed volume. You shut it down and then start the instance. You noticed that the data which you have saved earlier on the attached volume is no longer available.What might be the cause of this?,B,B,"The EC2 instance was using EBS backed root volumes, which are ephemeral and only live for the life of the instance.","The EC2 instance was using instance store volumes, which are ephemeral and only live for the life of the instance.",The volume of the instance was not big enough to handle all of the processing data.,The instance was hit by a virus that wipes out all data.,,,,회사에는 기본 연결된 75GB SSD 인스턴스 스토어 백업 볼륨이 있는 실행 중인 m5ad.large EC2 인스턴스가 있습니다. 종료한 다음 인스턴스를 시작합니다. 이전에 연결된 볼륨에 저장한 데이터를 더 이상 사용할 수 없음을 확인했습니다.그 원인은 무엇입니까?,EC2 인스턴스는 EBS 지원 루트 볼륨을 사용하고 있었습니다. 이 볼륨은 임시이며 인스턴스 수명 동안만 유지됩니다.,EC2 인스턴스는 일시적이고 인스턴스 수명 동안만 유지되는 인스턴스 스토어 볼륨을 사용하고 있었습니다.,인스턴스의 볼륨이 모든 처리 데이터를 처리할 만큼 크지 않았습니다.,인스턴스가 모든 데이터를 지우는 바이러스에 감염되었습니다.,,,0,,
udemy,CLF-01,309,A company has a UAT and production EC2 instances running on AWS. They want to ensure that employees who are responsible for the UAT instances don't have access to work on the production instances to minimize security risks.Which of the following would be the best way to achieve this?,C,C,Launch the UAT and production EC2 instances in separate VPC's connected by VPC peering.,Provide permissions to the users via the AWS Resource Access Manager (RAM) service to only access EC2 instances that are used for production or development.,Define the tags on the UAT and production servers and add a condition to the IAM policy which allows access to specific tags.,Launch the UAT and production instances in different Availability Zones and use Multi Factor Authentication.,,,,회사에는 AWS에서 실행되는 UAT 및 프로덕션 EC2 인스턴스가 있습니다. 그들은 UAT 인스턴스를 담당하는 직원이 보안 위험을 최소화하기 위해 프로덕션 인스턴스에서 작업할 수 없도록 하려고 합니다.다음 중 이를 달성하는 가장 좋은 방법은 무엇입니까?,VPC 피어링으로 연결된 별도의 VPC에서 UAT 및 프로덕션 EC2 인스턴스를 시작합니다.,프로덕션 또는 개발에 사용되는 EC2 인스턴스에만 액세스할 수 있도록 AWS RAM(Resource Access Manager) 서비스를 통해 사용자에게 권한을 제공합니다.,UAT 및 프로덕션 서버에서 태그를 정의하고 특정 태그에 대한 액세스를 허용하는 IAM 정책에 조건을 추가합니다.,다른 가용 영역에서 UAT 및 프로덕션 인스턴스를 시작하고 다단계 인증을 사용합니다.,,,0,,
udemy,CLF-01,310,"A Solutions Architect is working for a fast-growing startup that just started operations during the past 3 months. They currently have an on-premises Active Directory and 10 computers. To save costs in procuring physical workstations, they decided to deploy virtual desktops for their new employees in a virtual private cloud in AWS. The new cloud infrastructure should leverage the existing security controls in AWS but can still communicate with their on-premises network.Which set of AWS services will the Architect use to meet these requirements?",D,D,"AWS Directory Services, VPN connection, and AWS Identity and Access Management","AWS Directory Services, VPN connection, and ClassicLink","AWS Directory Services, VPN connection, and Amazon S3","AWS Directory Services, VPN connection, and Amazon Workspaces",,,,Solutions Architect는 지난 3개월 동안 운영을 시작한 빠르게 성장하는 스타트업에서 일하고 있습니다. 현재 온프레미스 Active Directory와 10대의 컴퓨터가 있습니다. 물리적 워크스테이션 조달 비용을 절감하기 위해 그들은 AWS의 가상 사설 클라우드에 신입 직원을 위한 가상 데스크톱을 배포하기로 결정했습니다. 새로운 클라우드 인프라는 AWS의 기존 보안 제어를 활용해야 하지만 여전히 온프레미스 네트워크와 통신할 수 있습니다.Architect는 이러한 요구 사항을 충족하기 위해 어떤 AWS 서비스 세트를 사용합니까?,"AWS Directory Services, VPN 연결 및 AWS Identity and Access Management","AWS Directory Services, VPN 연결 및 ClassicLink","AWS Directory Services, VPN 연결 및 Amazon S3","AWS Directory Services, VPN 연결 및 Amazon Workspaces",,,0,,
udemy,CLF-01,311,"A web application hosted in an Auto Scaling group of EC2 instances in AWS. The application receives a burst of traffic every morning, and a lot of users are complaining about request timeouts. The EC2 instance takes 1 minute to boot up before it can respond to user requests. The cloud architecture must be redesigned to better respond to the changing traffic of the application.How should the Solutions Architect redesign the architecture?",D,D,Create a new launch template and upgrade the size of the instance.,Create a Network Load Balancer with slow-start mode.,Create a CloudFront distribution and set the EC2 instance as the origin.,Create a step scaling policy and configure an instance warm-up time condition.,,,,AWS에 있는 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅되는 웹 애플리케이션입니다. 이 애플리케이션은 매일 아침 폭주하는 트래픽을 수신하며 많은 사용자가 요청 시간 초과에 대해 불평하고 있습니다. EC2 인스턴스는 사용자 요청에 응답하기 전에 부팅하는 데 1분이 걸립니다. 애플리케이션의 변화하는 트래픽에 더 잘 대응할 수 있도록 클라우드 아키텍처를 재설계해야 합니다.Solutions Architect는 아키텍처를 어떻게 재설계해야 합니까?,새 시작 템플릿을 생성하고 인스턴스 크기를 업그레이드합니다.,느린 시작 모드로 Network Load Balancer를 생성합니다.,CloudFront 배포를 생성하고 EC2 인스턴스를 오리진으로 설정합니다.,단계 조정 정책을 생성하고 인스턴스 워밍업 시간 조건을 구성합니다.,,,0,,
udemy,CLF-01,312,"A company troubleshoots the operational issues of their cloud architecture by logging the AWS API call history of all AWS resources. The Solutions Architect must implement a solution to quickly identify the most recent changes made to resources in their environment, including creation, modification, and deletion of AWS resources. One of the requirements is that the generated log files should be encrypted to avoid any security issues.Which of the following is the most suitable approach to implement the encryption?",B,B,Use CloudTrail and configure the destination S3 bucket to use Server-Side Encryption (SSE).,Use CloudTrail with its default settings,Use CloudTrail and configure the destination S3 bucket to use Server Side Encryption (SSE) with AES-128 encryption algorithm.,Use CloudTrail and configure the destination Amazon Glacier archive to use Server-Side Encryption (SSE).,,,,"회사는 모든 AWS 리소스의 AWS API 호출 기록을 기록하여 클라우드 아키텍처의 운영 문제를 해결합니다. Solutions Architect는 AWS 리소스의 생성, 수정 및 삭제를 포함하여 환경에서 리소스에 대한 가장 최근 변경 사항을 신속하게 식별하는 솔루션을 구현해야 합니다. 요구 사항 중 하나는 보안 문제를 방지하기 위해 생성된 로그 파일을 암호화해야 한다는 것입니다.다음 중 암호화를 구현하는 데 가장 적합한 접근 방식은 무엇입니까?",CloudTrail을 사용하고 SSE(서버 측 암호화)를 사용하도록 대상 S3 버킷을 구성합니다.,기본 설정으로 CloudTrail 사용,CloudTrail을 사용하고 AES-128 암호화 알고리즘과 함께 서버 측 암호화(SSE)를 사용하도록 대상 S3 버킷을 구성합니다.,CloudTrail을 사용하고 SSE(서버 측 암호화)를 사용하도록 대상 Amazon Glacier 아카이브를 구성합니다.,,,0,,
udemy,CLF-01,313,"A company has several microservices that send messages to an Amazon SQS queue and a backend application that poll the queue to process the messages. The company also has a Service Level Agreement (SLA) which defines the acceptable amount of time that can elapse from the point when the messages are received until a response is sent. The backend operations are I/O-intensive as the number of messages is constantly growing, causing the company to miss its SLA. The Solutions Architect must implement a new architecture that improves the application's processing time and load management.Which of the following is the MOST effective solution that can satisfy the given requirement?",C,C,Create an AMI of the backend application's EC2 instance. Use the image to set up an Auto Scaling group and configure a target tracking scaling policy based on the CPUUtilization metric with a target value of 80%.,Create an AMI of the backend application's EC2 instance and replace it with a larger instance size.,Create an AMI of the backend application's EC2 instance. Use the image to set up an Auto Scaling group and configure a target tracking scaling policy based on the ApproximateAgeOfOldestMessage metric.,Create an AMI of the backend application's EC2 instance and launch it to a cluster placement group.,,,,회사에는 메시지를 Amazon SQS 대기열로 보내는 여러 마이크로 서비스와 대기열을 폴링하여 메시지를 처리하는 백엔드 애플리케이션이 있습니다. 회사는 또한 메시지를 받은 시점부터 응답을 보낼 때까지 경과할 수 있는 허용 가능한 시간을 정의하는 서비스 수준 계약(SLA)을 가지고 있습니다. 백엔드 작업은 메시지 수가 지속적으로 증가하여 회사가 SLA를 놓치게 되므로 I/O 집약적입니다. Solutions Architect는 애플리케이션의 처리 시간과 부하 관리를 개선하는 새로운 아키텍처를 구현해야 합니다.다음 중 주어진 요구 사항을 충족할 수 있는 가장 효과적인 솔루션은 무엇입니까?,백엔드 애플리케이션의 EC2 인스턴스의 AMI를 생성합니다. 이미지를 사용하여 Auto Scaling 그룹을 설정하고 CPUUtilization대상 값이 80%인 메트릭을 기반으로 대상 추적 조정 정책을 구성합니다.,백엔드 애플리케이션의 EC2 인스턴스의 AMI를 생성하고 더 큰 인스턴스 크기로 바꿉니다.,백엔드 애플리케이션의 EC2 인스턴스의 AMI를 생성합니다. 이미지를 사용하여 Auto Scaling 그룹을 설정하고 메트릭을 기반으로 대상 추적 조정 정책을 구성합니다 ApproximateAgeOfOldestMessage.,백엔드 애플리케이션의 EC2 인스턴스의 AMI를 생성하고 클러스터 배치 그룹으로 시작합니다.,,,0,,
udemy,CLF-01,314,A company plans to implement a hybrid architecture. They need to create a dedicated connection from their Amazon Virtual Private Cloud (VPC) to their on-premises network. The connection must provide high bandwidth throughput and a more consistent network experience than Internet-based solutions.Which of the following can be used to create a private connection between the VPC and the company's on-premises network?,B,B,Transit Gateway with equal-cost multipath routing (ECMP),AWS Direct Connect,Transit VPC,AWS Site-to-Site VPN,,,,회사에서 하이브리드 아키텍처를 구현할 계획입니다. Amazon Virtual Private Cloud(VPC)에서 온프레미스 네트워크로의 전용 연결을 생성해야 합니다. 연결은 높은 대역폭 처리량과 인터넷 기반 솔루션보다 더 일관된 네트워크 환경을 제공해야 합니다.다음 중 VPC와 회사의 온프레미스 네트워크 간에 비공개 연결을 생성하는 데 사용할 수 있는 것은 무엇입니까?,등가 다중 경로 라우팅(ECMP)을 사용하는 Transit Gateway,AWS 다이렉트 커넥트,전송 VPC,AWS 사이트 간 VPN,,,0,,
udemy,CLF-01,315,"An organization plans to run an application in a dedicated physical server that doesn’t use virtualization. The application data will be stored in a storage solution that uses an NFS protocol. To prevent data loss, you need to use a durable cloud storage service to store a copy of your data.Which of the following is the most suitable solution to meet the requirement?",D,D,Use an AWS Storage Gateway hardware appliance for your compute resources. Configure Volume Gateway to store the application data and create an Amazon S3 bucket to store a backup of your data.,Use an AWS Storage Gateway hardware appliance for your compute resources. Configure Volume Gateway to store the application data and backup data.,Use AWS Storage Gateway with a gateway VM appliance for your compute resources. Configure File Gateway to store the application data and backup data.,Use an AWS Storage Gateway hardware appliance for your compute resources. Configure File Gateway to store the application data and create an Amazon S3 bucket to store a backup of your data.,,,,조직에서 가상화를 사용하지 않는 전용 물리적 서버에서 애플리케이션을 실행할 계획입니다. 애플리케이션 데이터는 NFS 프로토콜을 사용하는 스토리지 솔루션에 저장됩니다. 데이터 손실을 방지하려면 내구성 있는 클라우드 스토리지 서비스를 사용하여 데이터 사본을 저장해야 합니다.다음 중 요구 사항을 충족하는 가장 적합한 솔루션은 무엇입니까?,컴퓨팅 리소스에 AWS Storage Gateway 하드웨어 어플라이언스를 사용하십시오. 애플리케이션 데이터를 저장하도록 볼륨 게이트웨이를 구성하고 데이터 백업을 저장할 Amazon S3 버킷을 생성합니다.,컴퓨팅 리소스에 AWS Storage Gateway 하드웨어 어플라이언스를 사용하십시오. 애플리케이션 데이터 및 백업 데이터를 저장하도록 볼륨 게이트웨이를 구성합니다.,컴퓨팅 리소스용 게이트웨이 VM 어플라이언스와 함께 AWS Storage Gateway를 사용합니다. 애플리케이션 데이터 및 백업 데이터를 저장하도록 파일 게이트웨이를 구성합니다.,컴퓨팅 리소스에 AWS Storage Gateway 하드웨어 어플라이언스를 사용하십시오. 애플리케이션 데이터를 저장하도록 파일 게이트웨이를 구성하고 데이터 백업을 저장할 Amazon S3 버킷을 생성합니다.,,,0,,
udemy,CLF-01,316,"A financial analytics application that collects, processes and analyzes stock data in real-time is using Kinesis Data Streams. The producers continually push data to Kinesis Data Streams while the consumers process the data in real time. In Amazon Kinesis, where can the consumers store their results? (Select TWO.)",CD,CD,Glacier Select,Amazon Athena,Amazon Redshift,Amazon S3,AWS Glue,,,"주식 데이터를 실시간으로 수집, 처리 및 분석하는 재무 분석 애플리케이션은 Kinesis Data Streams를 사용하고 있습니다. 생산자는 지속적으로 데이터를 Kinesis Data Streams로 푸시하고 소비자는 데이터를 실시간으로 처리합니다. Amazon Kinesis에서 소비자는 결과를 어디에 저장할 수 있습니까? (2개를 선택하세요.)",빙하 선택,아마존 아테나,아마존 레드시프트,아마존 S3,AWS 글루,,0,,
udemy,CLF-01,317,"A top investment bank is in the process of building a new Forex trading platform. To ensure high availability and scalability, you designed the trading platform to use an Elastic Load Balancer in front of an Auto Scaling group of On-Demand EC2 instances across multiple Availability Zones. For its database tier, you chose to use a single Amazon Aurora instance to take advantage of its distributed, fault-tolerant, and self-healing storage system.In the event of system failure on the primary database instance, what happens to Amazon Aurora during the failover?",C,C,"Aurora will first attempt to create a new DB Instance in a different Availability Zone of the original instance. If unable to do so, Aurora will attempt to create a new DB Instance in the original Availability Zone in which the instance was first launched.","Amazon Aurora flips the canonical name record (CNAME) for your DB Instance to point at the healthy replica, which in turn is promoted to become the new primary.",Aurora will attempt to create a new DB Instance in the same Availability Zone as the original instance and is done on a best-effort basis.,"Amazon Aurora flips the A record of your DB Instance to point at the healthy replica, which in turn is promoted to become the new primary.",,,,최고의 투자 은행이 새로운 Forex 거래 플랫폼을 구축하는 과정에 있습니다. 고가용성과 확장성을 보장하기 위해 여러 가용 영역에서 온디맨드 EC2 인스턴스의 Auto Scaling 그룹 앞에 Elastic Load Balancer를 사용하도록 거래 플랫폼을 설계했습니다. 데이터베이스 계층의 경우 단일 Amazon Aurora 인스턴스를 사용하여 분산된 내결함성 및 자가 치유 스토리지 시스템을 활용하도록 선택했습니다.기본 데이터베이스 인스턴스에서 시스템 장애가 발생하는 경우 장애 조치 중에 Amazon Aurora는 어떻게 됩니까?,Aurora는 먼저 원래 인스턴스의 다른 가용 영역에서 새 DB 인스턴스 생성을 시도합니다. 그렇게 할 수 없는 경우 Aurora는 인스턴스가 처음 시작된 원래 가용 영역에서 새 DB 인스턴스를 생성하려고 시도합니다.,Amazon Aurora는 정상 복제본을 가리키도록 DB 인스턴스의 정식 이름 레코드(CNAME)를 뒤집습니다. 그러면 복제본이 새로운 기본 레코드로 승격됩니다.,Aurora는 원래 인스턴스와 동일한 가용 영역에서 새 DB 인스턴스를 생성하려고 시도하며 최선의 노력을 다합니다.,Amazon Aurora는 건강한 복제본을 가리키도록 DB 인스턴스의 A 레코드를 뒤집습니다. 그러면 새로운 기본 인스턴스로 승격됩니다.,,,0,,
udemy,CLF-01,318,"A company has an application hosted in an Auto Scaling group of Amazon EC2 instances across multiple Availability Zones behind an Application Load Balancer. There are several occasions where some instances are automatically terminated after failing the HTTPS health checks in the ALB and then purges all the ephemeral logs stored in the instance. A Solutions Architect must implement a solution that collects all of the application and server logs effectively. She should be able to perform a root cause analysis based on the logs, even if the Auto Scaling group immediately terminated the instance. What is the EASIEST way for the Architect to automate the log collection from the Amazon EC2 instances?",A,A,Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state to delay the termination of unhealthy Amazon EC2 instances. Configure a CloudWatch Events rule for the EC2 Instance-terminate Lifecycle Action Auto Scaling Event with an associated Lambda function. Trigger the CloudWatch agent to push the application logs and then resume the instance termination once all the logs are sent to CloudWatch Logs.,Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state to delay the termination of the unhealthy Amazon EC2 instances. Configure a CloudWatch Events rule for the EC2 Instance Terminate Successful Auto Scaling Event with an associated Lambda function. Set up the AWS Systems Manager Run Command service to run a script that collects and uploads the application logs from the instance to a CloudWatch Logs group. Resume the instance termination once all the logs are sent.,Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Pending:Wait state to delay the termination of the unhealthy Amazon EC2 instances. Configure a CloudWatch Events rule for the EC2 Instance-terminate Lifecycle Action Auto Scaling Event with an associated Lambda function. Set up an AWS Systems Manager Automation script that collects and uploads the application logs from the instance to a CloudWatch Logs group. Configure the solution to only resume the instance termination once all the logs were successfully sent.,Add a lifecycle hook to your Auto Scaling group to move instances in the Terminating state to the Terminating:Wait state to delay the termination of the unhealthy Amazon EC2 instances. Set up AWS Step Functions to collect the application logs and send them to a CloudWatch Log group. Configure the solution to resume the instance termination as soon as all the logs were successfully sent to CloudWatch Logs.,,,,회사에는 Application Load Balancer 뒤의 여러 가용 영역에 걸쳐 Amazon EC2 인스턴스의 Auto Scaling 그룹에서 호스팅되는 애플리케이션이 있습니다. ALB에서 HTTPS 상태 확인에 실패한 후 일부 인스턴스가 자동으로 종료된 다음 인스턴스에 저장된 모든 임시 로그를 제거하는 경우가 여러 번 있습니다. Solutions Architect는 모든 애플리케이션 및 서버 로그를 효과적으로 수집하는 솔루션을 구현해야 합니다. Auto Scaling 그룹이 즉시 인스턴스를 종료하더라도 로그를 기반으로 근본 원인 분석을 수행할 수 있어야 합니다.Architect가 Amazon EC2 인스턴스에서 로그 수집을 자동화하는 가장 쉬운 방법은 무엇입니까?,Auto Scaling 그룹에 수명 주기 후크를 추가하여 비정상 Amazon EC2 인스턴스의 종료를 지연시키기 위해 Terminating상태의 인스턴스를 상태로 이동합니다. 연결된 Lambda 함수를 사용하여 Auto Scaling 이벤트 Terminating:Wait에 대한 CloudWatch 이벤트 규칙을 구성합니다 . EC2 Instance-terminate Lifecycle ActionCloudWatch 에이전트를 트리거하여 애플리케이션 로그를 푸시한 다음 모든 로그가 CloudWatch Logs로 전송되면 인스턴스 종료를 재개하십시오.,Auto Scaling 그룹에 수명 주기 후크를 추가하여 비정상 Amazon EC2 인스턴스의 종료를 지연시키기 위해 상태 의 인스턴스를 Terminating상태로 이동합니다. 연결된 Lambda 함수를 사용하여 Auto Scaling 이벤트 Terminating:Wait에 대한 CloudWatch 이벤트 규칙을 구성합니다 . EC2 Instance Terminate Successful인스턴스에서 CloudWatch Logs 그룹으로 애플리케이션 로그를 수집하고 업로드하는 스크립트를 실행하도록 AWS Systems Manager Run Command 서비스를 설정합니다. 모든 로그가 전송되면 인스턴스 종료를 재개하십시오.,Auto Scaling 그룹에 수명 주기 후크를 추가하여 비정상 Amazon EC2 인스턴스의 종료를 지연시키기 위해 상태 의 인스턴스를 Terminating상태로 이동합니다. 연결된 Lambda 함수를 사용하여 Auto Scaling 이벤트 Pending:Wait에 대한 CloudWatch 이벤트 규칙을 구성합니다 . EC2 Instance-terminate Lifecycle Action인스턴스에서 CloudWatch Logs 그룹으로 애플리케이션 로그를 수집하고 업로드하는 AWS Systems Manager 자동화 스크립트를 설정합니다. 모든 로그가 성공적으로 전송된 후에만 인스턴스 종료를 재개하도록 솔루션을 구성하십시오.,Auto Scaling 그룹에 수명 주기 후크를 추가하여 비정상 Amazon EC2 인스턴스의 종료를 지연시키기 위해 상태 의 인스턴스를 Terminating상태로 이동합니다. Terminating:Wait애플리케이션 로그를 수집하고 CloudWatch Log 그룹으로 보내도록 AWS Step Functions를 설정합니다. 모든 로그가 CloudWatch Logs로 성공적으로 전송되는 즉시 인스턴스 종료를 재개하도록 솔루션을 구성하십시오.,,,0,,
udemy,CLF-01,319,"A top IT Consultancy has a VPC with two On-Demand EC2 instances with Elastic IP addresses. You were notified that the EC2 instances are currently under SSH brute force attacks over the Internet. The IT Security team has identified the IP addresses where these attacks originated. You have to immediately implement a temporary fix to stop these attacks while the team is setting up AWS WAF, GuardDuty, and AWS Shield Advanced to permanently fix the security vulnerability.Which of the following provides the quickest way to stop the attacks to the instances?",A,A,Block the IP addresses in the Network Access Control List,Remove the Internet Gateway from the VPC,Place the EC2 instances into private subnets,Assign a static Anycast IP address to each EC2 instance,,,,"최고의 IT 컨설팅 회사에는 탄력적 IP 주소가 있는 2개의 온디맨드 EC2 인스턴스가 있는 VPC가 있습니다. EC2 인스턴스가 현재 인터넷을 통해 SSH 무차별 암호 대입 공격을 받고 있다는 알림을 받았습니다. IT 보안 팀은 이러한 공격이 시작된 IP 주소를 식별했습니다. 팀이 보안 취약성을 영구적으로 수정하기 위해 AWS WAF, GuardDuty 및 AWS Shield Advanced를 설정하는 동안 이러한 공격을 중지하기 위한 임시 수정을 즉시 구현해야 합니다.다음 중 인스턴스에 대한 공격을 중지하는 가장 빠른 방법은 무엇입니까?",네트워크 액세스 제어 목록에서 IP 주소 차단,VPC에서 인터넷 게이트웨이 제거,프라이빗 서브넷에 EC2 인스턴스 배치,각 EC2 인스턴스에 정적 Anycast IP 주소 할당,,,0,,
udemy,CLF-01,320,A logistics company based in the USA runs its web application on a fleet of Amazon EC2 instances in an Auto Scaling group. It runs the same application in multiple AWS regions to cater to clients across several countries. A recent government policy has been enacted that prohibits the company from servicing a specific country.Which of the following options is the recommended action to comply with the government requirement?,C,C,Update the route tables to forward all outbound traffic to AWS Network Firewall and configure a stateful domain list rule group to block the specified country,Update the Network Access Control Lists of all subnets used by the Amazon EC2 instances to “deny” all IP addresses from the specific country.,Create a Web ACL rule in AWS WAF to block the specified country. Associate the rule to the Application Load Balancers.,Update the Network Access Control Lists of all subnets used by the Application Load Balancers to “deny” all IP addresses from the specific country.,,,,미국에 기반을 둔 물류 회사는 Auto Scaling 그룹의 Amazon EC2 인스턴스 플릿에서 웹 애플리케이션을 실행합니다. 여러 국가의 클라이언트를 수용하기 위해 여러 AWS 리전에서 동일한 애플리케이션을 실행합니다. 회사가 특정 국가에 서비스를 제공하는 것을 금지하는 최근 정부 정책이 제정되었습니다.다음 중 정부 요구 사항을 준수하기 위해 권장되는 조치는 무엇입니까?,모든 아웃바운드 트래픽을 AWS Network Firewall로 전달하도록 라우팅 테이블을 업데이트하고 지정된 국가를 차단하도록 상태 저장 도메인 목록 규칙 그룹을 구성합니다.,"Amazon EC2 인스턴스가 사용하는 모든 서브넷의 네트워크 액세스 제어 목록을 업데이트하여 특정 국가의 모든 IP 주소를 ""거부""합니다.",지정된 국가를 차단하려면 AWS WAF에서 웹 ACL 규칙을 생성합니다. 규칙을 Application Load Balancer에 연결합니다.,"Application Load Balancer가 사용하는 모든 서브넷의 네트워크 액세스 제어 목록을 업데이트하여 특정 국가의 모든 IP 주소를 ""거부""합니다.",,,0,,
udemy,CLF-01,321,"A company has a web-based ticketing service that utilizes Amazon SQS and a fleet of EC2 instances. The EC2 instances that consume messages from the SQS queue are configured to poll the queue as often as possible to keep end-to-end throughput as high as possible. The Solutions Architect noticed that polling the queue in tight loops is using unnecessary CPU cycles, resulting in increased operational costs due to empty responses.In this scenario, what should the Solutions Architect do to make the system more cost-effective?",B,B,Configure Amazon SQS to use long polling by setting the ReceiveMessageWaitTimeSeconds to zero.,Configure Amazon SQS to use long polling by setting the ReceiveMessageWaitTimeSeconds to a number greater than zero.,Configure Amazon SQS to use short polling by setting the ReceiveMessageWaitTimeSeconds to a number greater than zero.,Configure Amazon SQS to use short polling by setting the ReceiveMessageWaitTimeSeconds to zero.,,,,한 회사에 Amazon SQS와 여러 EC2 인스턴스를 활용하는 웹 기반 발권 서비스가 있습니다. SQS 대기열의 메시지를 소비하는 EC2 인스턴스는 종단간 처리량을 최대한 높게 유지하기 위해 가능한 한 자주 대기열을 폴링하도록 구성됩니다. Solutions Architect는 빡빡한 루프에서 대기열을 폴링하는 것이 불필요한 CPU 주기를 사용하여 빈 응답으로 인해 운영 비용이 증가한다는 사실을 알게 되었습니다.이 시나리오에서 Solutions Architect는 시스템을 보다 비용 효율적으로 만들기 위해 무엇을 해야 합니까?,ReceiveMessageWaitTimeSeconds를 0으로 설정하여 긴 폴링을 사용하도록 Amazon SQS를 구성합니다.,ReceiveMessageWaitTimeSeconds를 0보다 큰 숫자로 설정하여 긴 폴링을 사용하도록 Amazon SQS를 구성합니다.,ReceiveMessageWaitTimeSeconds를 0보다 큰 숫자로 설정하여 짧은 폴링을 사용하도록 Amazon SQS를 구성합니다.,ReceiveMessageWaitTimeSeconds를 0으로 설정하여 짧은 폴링을 사용하도록 Amazon SQS를 구성합니다.,,,0,,
udemy,CLF-01,322,"An application is hosted in an Auto Scaling group of EC2 instances. To improve the monitoring process, you have to configure the current capacity to increase or decrease based on a set of scaling adjustments. This should be done by specifying the scaling metrics and threshold values for the CloudWatch alarms that trigger the scaling process. Which of the following is the most suitable type of scaling policy that you should use?",D,D,Scheduled Scaling,Target tracking scaling,Simple scaling,Step scaling,,,,애플리케이션은 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅됩니다. 모니터링 프로세스를 개선하려면 일련의 조정 조정에 따라 현재 용량을 늘리거나 줄이도록 구성해야 합니다. 이는 조정 프로세스를 트리거하는 CloudWatch 경보에 대한 조정 지표 및 임계값을 지정하여 수행해야 합니다.다음 중 사용해야 하는 가장 적합한 조정 정책 유형은 무엇입니까?,예정된 조정,대상 추적 스케일링,단순 스케일링,단계 스케일링,,,0,,
udemy,CLF-01,323,"A company has both on-premises data center as well as AWS cloud infrastructure. They store their graphics, audios, videos, and other multimedia assets primarily in their on-premises storage server and use an S3 Standard storage class bucket as a backup. Their data is heavily used for only a week (7 days) but after that period, it will only be infrequently used by their customers. The Solutions Architect is instructed to save storage costs in AWS yet maintain the ability to fetch a subset of their media assets in a matter of minutes for a surprise annual data audit, which will be conducted on their cloud storage.Which of the following are valid options that the Solutions Architect can implement to meet the above requirement? (Select TWO.)",BC,BC,Set a lifecycle policy in the bucket to transition the data to S3 Glacier Deep Archive storage class after one week (7 days).,Set a lifecycle policy in the bucket to transition to S3 - Standard IA after 30 days,Set a lifecycle policy in the bucket to transition the data to Glacier after one week (7 days).,Set a lifecycle policy in the bucket to transition the data to S3 - Standard IA storage class after one week (7 days).,Set a lifecycle policy in the bucket to transition the data to S3 - One Zone-Infrequent Access storage class after one week (7 days).,,,"회사는 온프레미스 데이터 센터와 AWS 클라우드 인프라를 모두 보유하고 있습니다. 그래픽, 오디오, 비디오 및 기타 멀티미디어 자산을 주로 온프레미스 스토리지 서버에 저장하고 S3 Standard 스토리지 클래스 버킷을 백업으로 사용합니다. 그들의 데이터는 일주일(7일) 동안만 많이 사용되지만 그 기간이 지나면 고객이 거의 사용하지 않습니다. Solutions Architect는 AWS에서 스토리지 비용을 절감하면서도 클라우드 스토리지에서 수행될 깜짝 연간 데이터 감사를 위해 몇 분 만에 미디어 자산의 하위 집합을 가져올 수 있는 기능을 유지하도록 지시 받았습니다.솔루션 아키텍트가 위의 요구 사항을 충족하기 위해 구현할 수 있는 유효한 옵션은 다음 중 무엇입니까? (2개를 선택하세요.)",1주일(7일) 후에 데이터를 S3 Glacier Deep Archive 스토리지 클래스로 전환하도록 버킷에 수명 주기 정책을 설정합니다.,버킷에서 수명 주기 정책을 설정하여 30일 후 S3 - Standard IA로 전환,일주일(7일) 후에 데이터를 Glacier로 전환하도록 버킷에 수명 주기 정책을 설정합니다.,1주일(7일) 후에 데이터를 S3 - Standard IA 스토리지 클래스로 전환하도록 버킷에 수명 주기 정책을 설정합니다.,1주일(7일) 후에 데이터를 S3 - One Zone-Infrequent Access 스토리지 클래스로 전환하도록 버킷에 수명 주기 정책을 설정합니다.,,0,,
udemy,CLF-01,324,"An application is hosted on an EC2 instance with multiple EBS Volumes attached and uses Amazon Neptune as its database. To improve data security, you encrypted all of the EBS volumes attached to the instance to protect the confidential data stored in the volumes.  Which of the following statements are true about encrypted Amazon Elastic Block Store volumes? (Select TWO.)",AE,AE,All data moving between the volume and the instance are encrypted.,The volumes created from the encrypted snapshot are not encrypted.,Only the data in the volume is encrypted and not all the data moving between the volume and the instance.,Snapshots are not automatically encrypted.,Snapshots are automatically encrypted.,,,애플리케이션은 여러 EBS 볼륨이 연결된 EC2 인스턴스에서 호스팅되며 Amazon Neptune을 데이터베이스로 사용합니다. 데이터 보안을 강화하기 위해 인스턴스에 연결된 모든 EBS 볼륨을 암호화하여 볼륨에 저장된 기밀 데이터를 보호했습니다.  다음 중 암호화된 Amazon Elastic Block Store 볼륨에 대한 설명으로 옳은 것은 무엇입니까? (2개를 선택하세요.),볼륨과 인스턴스 간에 이동하는 모든 데이터는 암호화됩니다.,암호화된 스냅샷에서 생성된 볼륨은 암호화되지 않습니다.,볼륨의 데이터만 암호화되고 볼륨과 인스턴스 간에 이동하는 모든 데이터는 암호화되지 않습니다.,스냅샷은 자동으로 암호화되지 않습니다.,스냅샷은 자동으로 암호화됩니다.,,0,,
udemy,CLF-01,325,"A company has 10 TB of infrequently accessed financial data files that would need to be stored in AWS. These data would be accessed infrequently during specific weeks when they are retrieved for auditing purposes. The retrieval time is not strict as long as it does not exceed 24 hours. Which of the following would be a secure, durable, and cost-effective solution for this scenario?",B,B,Upload the data to Amazon FSx for Windows File Server using the Server Message Block (SMB) protocol.,Upload the data to S3 and set a lifecycle policy to transition data to Glacier after 0 days.,Upload the data to S3 then use a lifecycle policy to transfer data to S3-IA.,Upload the data to S3 then use a lifecycle policy to transfer data to S3 One Zone-IA.,,,,회사에는 AWS에 저장해야 하는 10TB의 자주 액세스하지 않는 재무 데이터 파일이 있습니다. 이러한 데이터는 감사 목적으로 검색되는 특정 주 동안 드물게 액세스됩니다. 검색 시간은 24시간을 초과하지 않는 한 엄격하지 않습니다.다음 중 이 시나리오에 대한 안전하고 내구성이 있으며 비용 효율적인 솔루션은 무엇입니까?,SMB(서버 메시지 블록) 프로토콜을 사용하여 Windows 파일 서버용 Amazon FSx에 데이터를 업로드합니다.,데이터를 S3에 업로드하고 0며칠 후 데이터를 Glacier로 전환하도록 수명 주기 정책을 설정합니다.,데이터를 S3에 업로드한 다음 수명 주기 정책을 사용하여 데이터를 S3-IA로 전송합니다.,데이터를 S3에 업로드한 다음 수명 주기 정책을 사용하여 데이터를 S3 One Zone-IA로 전송합니다.,,,0,,
udemy,CLF-01,326,"A fast food company is using AWS to host their online ordering system which uses an Auto Scaling group of EC2 instances deployed across multiple Availability Zones with an Application Load Balancer in front. To better handle the incoming traffic from various digital devices, you are planning to implement a new routing system where requests which have a URL of <server>/api/android are forwarded to one specific target group named ""Android-Target-Group"". Conversely, requests which have a URL of <server>/api/ios are forwarded to another separate target group named ""iOS-Target-Group"".   How can you implement this change in AWS?",A,A,Use path conditions to define rules that forward requests to different target groups based on the URL in the request.,Replace your ALB with a Gateway Load Balancer then use path conditions to define rules that forward requests to different target groups based on the URL in the request.,Replace your ALB with a Network Load Balancer then use host conditions to define rules that forward requests to different target groups based on the URL in the request.,Use host conditions to define rules that forward requests to different target groups based on the hostname in the host header. This enables you to support multiple domains using a single load balancer.,,,,"한 패스트푸드 회사는 AWS를 사용하여 Application Load Balancer가 앞에 있는 여러 가용 영역에 배포된 EC2 인스턴스의 Auto Scaling 그룹을 사용하는 온라인 주문 시스템을 호스팅하고 있습니다. 다양한 디지털 장치에서 들어오는 트래픽을 더 잘 처리하기 위해 URL이 <server>/api/android인 요청이 ""Android-Target-Group""이라는 하나의 특정 대상 그룹으로 전달되는 새로운 라우팅 시스템을 구현할 계획입니다. 반대로 URL이 <server>/api/ios인 요청은 ""iOS-Target-Group""이라는 별도의 대상 그룹으로 전달됩니다.   AWS에서 이 변경 사항을 어떻게 구현할 수 있습니까?",경로 조건을 사용하여 요청의 URL을 기반으로 다른 대상 그룹에 요청을 전달하는 규칙을 정의합니다.,ALB를 게이트웨이 로드 밸런서로 교체한 다음 경로 조건을 사용하여 요청의 URL을 기반으로 다른 대상 그룹에 요청을 전달하는 규칙을 정의합니다.,ALB를 Network Load Balancer로 교체한 다음 호스트 조건을 사용하여 요청의 URL을 기반으로 다른 대상 그룹에 요청을 전달하는 규칙을 정의합니다.,호스트 조건을 사용하여 호스트 헤더의 호스트 이름을 기반으로 다른 대상 그룹에 요청을 전달하는 규칙을 정의합니다. 이를 통해 단일 로드 밸런서를 사용하여 여러 도메인을 지원할 수 있습니다.,,,0,,
udemy,CLF-01,327,A Solutions Architect is developing a three-tier cryptocurrency web application for a FinTech startup. The Architect has been instructed to restrict access to the database tier to only accept traffic from the application-tier and deny traffic from other sources. The application-tier is composed of application servers hosted in an Auto Scaling group of EC2 instances. Which of the following options is the MOST suitable solution to implement in this scenario?,D,D,Set up the Network ACL of the database subnet to deny all inbound non-database traffic from the subnet of the application-tier.,Set up the security group of the database tier to allow database traffic from a specified list of application server IP addresses.,Set up the Network ACL of the database subnet to allow inbound database traffic from the subnet of the application-tier.,Set up the security group of the database tier to allow database traffic from the security group of the application servers.,,,,Solutions Architect는 핀테크 스타트업을 위한 3계층 암호화폐 웹 애플리케이션을 개발하고 있습니다. Architect는 애플리케이션 계층의 트래픽만 수락하고 다른 소스의 트래픽은 거부하도록 데이터베이스 계층에 대한 액세스를 제한하라는 지시를 받았습니다. 애플리케이션 계층은 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅되는 애플리케이션 서버로 구성됩니다.다음 옵션 중 이 시나리오에서 구현하기에 가장 적합한 솔루션은 무엇입니까?,애플리케이션 계층의 서브넷에서 오는 모든 인바운드 비데이터베이스 트래픽을 거부하도록 데이터베이스 서브넷의 네트워크 ACL을 설정합니다.,지정된 애플리케이션 서버 IP 주소 목록에서 데이터베이스 트래픽을 허용하도록 데이터베이스 계층의 보안 그룹을 설정합니다.,애플리케이션 계층의 서브넷에서 인바운드 데이터베이스 트래픽을 허용하도록 데이터베이스 서브넷의 네트워크 ACL을 설정합니다.,응용 프로그램 서버의 보안 그룹에서 데이터베이스 트래픽을 허용하도록 데이터베이스 계층의 보안 그룹을 설정합니다.,,,0,,
udemy,CLF-01,328,"A multinational manufacturing company has multiple accounts in AWS to separate their various departments such as finance, human resources, engineering and many others. There is a requirement to ensure that certain access to services and actions are properly controlled to comply with the security policy of the company. As the Solutions Architect, which is the most suitable way to set up the multi-account AWS environment of the company?",C,C,Connect all departments by setting up a cross-account access to each of the AWS accounts of the company. Create and attach IAM policies to your resources based on their respective departments to control access.,Set up a common IAM policy that can be applied across all AWS accounts.,Use AWS Organizations and Service Control Policies to control services on each account.,Provide access to externally authenticated users via Identity Federation. Set up an IAM role to specify permissions for users from each department whose identity is federated from your organization or a third-party identity provider.,,,,"다국적 제조 회사는 재무, 인사, 엔지니어링 및 기타 여러 부서를 구분하기 위해 AWS에 여러 계정을 가지고 있습니다. 회사의 보안 정책을 준수하기 위해 서비스 및 작업에 대한 특정 액세스 및 작업이 적절하게 제어되도록 요구 사항이 있습니다.Solutions Architect로서 회사의 다중 계정 AWS 환경을 설정하는 가장 적합한 방법은 무엇입니까?",회사의 각 AWS 계정에 대한 교차 계정 액세스를 설정하여 모든 부서를 연결합니다. 각 부서에 따라 리소스에 IAM 정책을 생성하고 연결하여 액세스를 제어합니다.,모든 AWS 계정에 적용할 수 있는 공통 IAM 정책을 설정합니다.,AWS Organizations 및 서비스 제어 정책을 사용하여 각 계정의 서비스를 제어합니다.,ID 페더레이션을 통해 외부에서 인증된 사용자에게 액세스를 제공합니다. IAM 역할을 설정하여 조직 또는 타사 ID 공급자로부터 자격 증명이 연합된 각 부서의 사용자에 대한 권한을 지정합니다.,,,0,,
udemy,CLF-01,329,A company is using Amazon S3 to store frequently accessed data. The S3 bucket is shared with external users that will upload files regularly. A Solutions Architect needs to implement a solution that will grant the bucket owner full access to all uploaded objects in the S3 bucket.What action should be done to achieve this task?,A,A,Create a bucket policy that will require the users to set the object's ACL to bucket-owner-full-control.,Enable server access logging and set up an IAM policy that will require the users to set the object's ACL to bucket-owner-full-control.,Enable the Requester Pays feature in the Amazon S3 bucket.,Create a CORS configuration in the S3 bucket.,,,,한 회사에서 Amazon S3를 사용하여 자주 액세스하는 데이터를 저장하고 있습니다. S3 버킷은 정기적으로 파일을 업로드하는 외부 사용자와 공유됩니다. Solutions Architect는 버킷 소유자에게 S3 버킷에 업로드된 모든 객체에 대한 전체 액세스 권한을 부여하는 솔루션을 구현해야 합니다.이 작업을 수행하려면 어떤 작업을 수행해야 합니까?,사용자가 객체의 ACL을 로 설정하도록 요구하는 버킷 정책을 생성합니다 bucket-owner-full-control.,서버 액세스 로깅을 활성화하고 사용자가 객체의 ACL을 bucket-owner-full-control.,Amazon S3 버킷에서 요청자 지불 기능을 활성화합니다.,S3 버킷에 CORS 구성을 생성합니다.,,,0,,
udemy,CLF-01,330,"A commercial bank has designed its next-generation online banking platform to use a distributed system architecture. As their Software Architect, you have to ensure that their architecture is highly scalable, yet still cost-effective. Which of the following will provide the most suitable solution for this scenario?",B,B,"Launch multiple EC2 instances behind an Application Load Balancer to host your application services, and SWF which will act as a highly-scalable buffer that stores messages as they travel between distributed applications.",Launch an Auto-Scaling group of EC2 instances to host your application services and an SQS queue. Include an Auto Scaling trigger to watch the SQS queue size which will either scale in or scale out the number of EC2 instances based on the queue.,Launch multiple On-Demand EC2 instances to host your application services and an SQS queue which will act as a highly-scalable buffer that stores messages as they travel between distributed applications.,Launch multiple EC2 instances behind an Application Load Balancer to host your application services and SNS which will act as a highly-scalable buffer that stores messages as they travel between distributed applications.,,,,한 상업 은행은 분산 시스템 아키텍처를 사용하도록 차세대 온라인 뱅킹 플랫폼을 설계했습니다. 그들의 소프트웨어 아키텍트로서 당신은 그들의 아키텍처가 고도로 확장 가능하면서도 여전히 비용 효율적인지 확인해야 합니다. 다음 중 이 시나리오에 가장 적합한 솔루션은 무엇입니까?,Application Load Balancer 뒤에서 여러 EC2 인스턴스를 시작하여 애플리케이션 서비스를 호스팅하고 SWF는 분산된 애플리케이션 간에 이동할 때 메시지를 저장하는 확장성이 뛰어난 버퍼 역할을 합니다.,EC2 인스턴스의 Auto-Scaling 그룹을 시작하여 애플리케이션 서비스 및 SQS 대기열을 호스팅합니다. Auto Scaling 트리거를 포함하여 대기열을 기반으로 EC2 인스턴스 수를 축소하거나 축소하는 SQS 대기열 크기를 감시합니다.,여러 온디맨드 EC2 인스턴스를 시작하여 애플리케이션 서비스와 SQS 대기열을 호스팅하여 분산 애플리케이션 간에 이동하는 메시지를 저장하는 확장성이 뛰어난 버퍼 역할을 합니다.,Application Load Balancer 뒤에서 여러 EC2 인스턴스를 시작하여 애플리케이션 서비스와 분산된 애플리케이션 사이를 이동할 때 메시지를 저장하는 확장성이 뛰어난 버퍼 역할을 하는 SNS를 호스팅합니다.,,,0,,
udemy,CLF-01,331,"A Solutions Architect is working for a weather station in Asia with a weather monitoring system that needs to be migrated to AWS. Since the monitoring system requires a low network latency and high network throughput, the Architect decided to launch the EC2 instances to a new cluster placement group. The system was working fine for a couple of weeks, however, when they try to add new instances to the placement group that already has running EC2 instances, they receive an 'insufficient capacity error'.How will the Architect fix this issue?",C,C,Create another Placement Group and launch the new instances in the new group.,Submit a capacity increase request to AWS as you are initially limited to only 12 instances per Placement Group.,Stop and restart the instances in the Placement Group and then try the launch again.,Verify all running instances are of the same size and type and then try the launch again.,,,,Solutions Architect는 AWS로 마이그레이션해야 하는 기상 모니터링 시스템을 갖춘 아시아의 기상 관측소에서 일하고 있습니다. 모니터링 시스템에는 낮은 네트워크 대기 시간과 높은 네트워크 처리량이 필요하므로 Architect는 새 클러스터 배치 그룹에 EC2 인스턴스를 시작하기로 결정했습니다. 시스템은 몇 주 동안 제대로 작동했지만 이미 실행 중인 EC2 인스턴스가 있는 배치 그룹에 새 인스턴스를 추가하려고 하면 '용량 부족 오류'가 표시됩니다.Architect는 이 문제를 어떻게 해결할 것입니까?,다른 배치 그룹을 생성하고 새 그룹에서 새 인스턴스를 시작합니다.,처음에는 배치 그룹당 인스턴스가 12개로 제한되므로 AWS에 용량 증가 요청을 제출하십시오.,배치 그룹에서 인스턴스를 중지했다가 다시 시작한 다음 다시 시작하십시오.,실행 중인 모든 인스턴스의 크기와 유형이 같은지 확인한 다음 다시 시작하십시오.,,,0,,
udemy,CLF-01,332,"A company is running an on-premises application backed by a 1TB MySQL 8.0 database. A couple of times each month, the production data is fully copied to a staging database at the request of the analytics team. The team can't work on the staging database until the copy is finished, which takes hours.Throughout this period, the application experiences intermittent downtimes as well. To expedite the process for the analytics team, a solutions architect must redesign the application's architecture in AWS. The application must also be highly resilient to disruptions.Which combination of actions best satisfies the given set of requirements while being the most cost-effective? (Select TWO)",BD,BD,Take a manual snapshot and restore it to a database in the staging environment.,Use an Amazon Aurora database with Multi-AZ Replicas.,Use an Amazon RDS database in a Multi-AZ Deployments configuration,Clone the production database in the staging environment using Aurora cloning.,Replicate the production database to a staging database using the mysqldump client utility,,,회사에서 1TB MySQL 8.0 데이터베이스로 지원되는 온프레미스 애플리케이션을 실행하고 있습니다. 분석 팀의 요청에 따라 매월 두어 번 프로덕션 데이터가 스테이징 데이터베이스에 완전히 복사됩니다. 팀은 몇 시간이 걸리는 복사가 완료될 때까지 스테이징 데이터베이스에서 작업할 수 없습니다.이 기간 동안 애플리케이션도 간헐적으로 중단됩니다. 분석 팀의 프로세스를 신속하게 처리하려면 솔루션 설계자가 AWS에서 애플리케이션의 아키텍처를 재설계해야 합니다. 또한 애플리케이션은 중단에 대한 복원력이 매우 높아야 합니다.가장 비용 효율적이면서 주어진 요구 사항을 가장 잘 충족하는 작업 조합은 무엇입니까? (2개 선택),수동 스냅샷을 만들고 스테이징 환경의 데이터베이스로 복원합니다.,다중 AZ 복제본과 함께 Amazon Aurora 데이터베이스를 사용합니다.,다중 AZ 배포 구성에서 Amazon RDS 데이터베이스 사용,Aurora 복제를 사용하여 스테이징 환경에서 프로덕션 데이터베이스를 복제합니다.,mysqldump클라이언트 유틸리티를 사용하여 프로덕션 데이터베이스를 스테이징 데이터베이스로 복제,,0,,
udemy,CLF-01,333,"A company has an application that uses multiple EC2 instances located in various AWS regions such as US East (Ohio), US West (N. California), and EU (Ireland). The manager instructed the Solutions Architect to set up a latency-based routing to route incoming traffic for www.tutorialsdojo.com to all the EC2 instances across all AWS regions.Which of the following options can satisfy the given requirement?",D,D,Use an Application Load Balancer to distribute the load to the multiple EC2 instances across all AWS Regions.,Use AWS DataSync to distribute the load to the multiple EC2 instances across all AWS Regions.,Use a Network Load Balancer to distribute the load to the multiple EC2 instances across all AWS Regions.,Use Route 53 to distribute the load to the multiple EC2 instances across all AWS Regions.,,,,"회사에는 미국 동부(오하이오), 미국 서부(캘리포니아 북부) 및 EU(아일랜드)와 같은 다양한 AWS 지역에 있는 여러 EC2 인스턴스를 사용하는 애플리케이션이 있습니다. 관리자는 솔루션 아키텍트에게 www.tutorialsdojo.com에 대한 수신 트래픽을 모든 AWS 지역의 모든 EC2 인스턴스로 라우팅하도록 지연 시간 기반 라우팅을 설정하도록 지시했습니다.다음 옵션 중 주어진 요구 사항을 충족할 수 있는 옵션은 무엇입니까?",Application Load Balancer를 사용하여 모든 AWS 리전에 걸쳐 여러 EC2 인스턴스에 로드를 분산합니다.,AWS DataSync를 사용하여 모든 AWS 리전에 걸쳐 여러 EC2 인스턴스에 로드를 분산합니다.,Network Load Balancer를 사용하여 모든 AWS 리전에 걸쳐 여러 EC2 인스턴스에 로드를 분산합니다.,Route 53을 사용하여 모든 AWS 리전에 걸쳐 여러 EC2 인스턴스에 로드를 분산합니다.,,,0,,
udemy,CLF-01,334,"A Solutions Architect needs to set up the required compute resources for the application which have workloads that require high, sequential read and write access to very large data sets on local storage.Which of the following instance type is the most suitable one to use in this scenario?",B,B,Compute Optimized Instances,Storage Optimized Instances,Memory Optimized Instances,General Purpose Instances,,,,Solutions Architect는 로컬 스토리지의 매우 큰 데이터 세트에 대한 높은 순차적 읽기 및 쓰기 액세스가 필요한 워크로드가 있는 애플리케이션에 필요한 컴퓨팅 리소스를 설정해야 합니다.다음 중 이 시나리오에서 사용하기에 가장 적합한 인스턴스 유형은 무엇입니까?,컴퓨팅 최적화 인스턴스,스토리지 최적화 인스턴스,메모리 최적화 인스턴스,범용 인스턴스,,,0,,
udemy,CLF-01,335,"A global medical research company has a molecular imaging system that provides each client with frequently updated images of what is happening inside the human body at the molecular and cellular levels. The system is hosted in AWS and the images are hosted in an S3 bucket behind a CloudFront web distribution. When a fresh batch of images is uploaded to S3, it is required to keep the previous ones in order to prevent them from being overwritten.Which of the following is the most suitable solution to solve this issue?",C,C,"Add Cache-Control no-cache, no-store, or private directives in the S3 bucket",Invalidate the files in your CloudFront web distribution,Use versioned objects,Add a separate cache behavior path for the content and configure a custom object caching with a Minimum TTL of 0,,,,한 글로벌 의료 연구 회사는 분자 및 세포 수준에서 인체 내부에서 일어나는 일에 대한 자주 업데이트되는 이미지를 각 고객에게 제공하는 분자 이미징 시스템을 보유하고 있습니다. 시스템은 AWS에서 호스팅되고 이미지는 CloudFront 웹 배포 뒤의 S3 버킷에서 호스팅됩니다. 이미지의 새로운 배치가 S3에 업로드되면 덮어쓰지 않도록 이전 이미지를 유지해야 합니다.다음 중 이 문제를 해결하는 데 가장 적합한 솔루션은 무엇입니까?,"S3 버킷에 Cache-Control no-cache, no-store 또는 개인 지시문 추가",CloudFront 웹 배포의 파일 무효화,버전이 지정된 개체 사용,콘텐츠에 대한 별도의 캐시 동작 경로를 추가하고 최소 TTL이 0인 사용자 지정 개체 캐싱을 구성합니다.,,,0,,
udemy,CLF-01,336,"A multinational corporate and investment bank is regularly processing steady workloads of accruals, loan interests, and other critical financial calculations every night from 10 PM to 3 AM on their on-premises data center for their corporate clients. Once the process is done, the results are then uploaded to the Oracle General Ledger which means that the processing should not be delayed or interrupted. The CTO has decided to move its IT infrastructure to AWS to save costs. The company needs to reserve compute capacity in a specific Availability Zone to properly run their workloads.As the Senior Solutions Architect, how can you implement a cost-effective architecture in AWS for their financial system?",B,B,Use On-Demand EC2 instances which allows you to pay for the instances that you launch and use by the second. Reserve compute capacity in a specific Availability Zone to avoid any interruption.,"Use On-Demand Capacity Reservations, which provide compute capacity that is always available on the specified recurring schedule.",Use Regional Reserved Instances to reserve capacity on a specific Availability Zone and lower down the operating cost through its billing discounts.,"Use Dedicated Hosts which provide a physical host that is fully dedicated to running your instances, and bring your existing per-socket, per-core, or per-VM software licenses to reduce costs.",,,,"다국적 기업 및 투자 은행은 기업 고객을 위해 온프레미스 데이터 센터에서 매일 밤 오후 10시부터 오전 3시까지 발생액, 대출 이자 및 기타 중요한 재무 계산의 꾸준한 워크로드를 정기적으로 처리하고 있습니다. 프로세스가 완료되면 결과가 Oracle General Ledger에 업로드되므로 처리가 지연되거나 중단되어서는 안 됩니다. CTO는 비용을 절감하기 위해 IT 인프라를 AWS로 이전하기로 결정했습니다. 회사는 워크로드를 적절하게 실행하기 위해 특정 가용 영역에 컴퓨팅 용량을 예약해야 합니다.선임 솔루션 아키텍트로서 재무 시스템을 위해 AWS에서 어떻게 비용 효율적인 아키텍처를 구현할 수 있습니까?",두 번째로 시작하고 사용하는 인스턴스에 대한 비용을 지불할 수 있는 온디맨드 EC2 인스턴스를 사용하십시오. 중단을 방지하기 위해 특정 가용 영역에서 컴퓨팅 용량을 예약합니다.,지정된 반복 일정에 따라 항상 사용 가능한 컴퓨팅 용량을 제공하는 온디맨드 용량 예약을 사용합니다.,지역 예약 인스턴스를 사용하여 특정 가용 영역의 용량을 예약하고 청구 할인을 통해 운영 비용을 낮춥니다.,"인스턴스 실행에 완전히 전용되는 물리적 호스트를 제공하는 전용 호스트를 사용하고 기존의 소켓당, 코어당 또는 VM당 소프트웨어 라이선스를 가져와 비용을 절감하십시오.",,,0,,
udemy,CLF-01,337,"A company launched a cryptocurrency mining server on a Reserved EC2 instance in the us-east-1 region's private subnet that uses IPv6. Due to the financial data that the server contains, the system should be secured to prevent any unauthorized access and to meet regulatory compliance requirements.In this scenario, which VPC feature will allow the EC2 instance to communicate to the Internet but prevents inbound IPv6 traffic?",A,A,Egress-only Internet gateway,NAT instances,NAT Gateway,Internet Gateway,,,,한 회사가 IPv6를 사용하는 us-east-1 지역의 프라이빗 서브넷에 있는 Reserved EC2 인스턴스에서 암호화폐 채굴 서버를 시작했습니다. 서버에 포함된 재무 데이터로 인해 무단 액세스를 방지하고 규정 준수 요구 사항을 충족하도록 시스템을 보호해야 합니다.이 시나리오에서 EC2 인스턴스가 인터넷과 통신할 수 있도록 허용하지만 인바운드 IPv6 트래픽을 방지하는 VPC 기능은 무엇입니까?,외부 전용 인터넷 게이트웨이,NAT 인스턴스,NAT 게이트웨이,인터넷 게이트웨이,,,0,,
udemy,CLF-01,338,"An online stock trading system is hosted in AWS and uses an Auto Scaling group of EC2 instances, an RDS database, and an Amazon ElastiCache for Redis. You need to improve the data security of your in-memory data store by requiring the user to enter a password before they are granted permission to execute Redis commands.   Which of the following should you do to meet the above requirement?",C,C,None of the above.,Enable the in-transit encryption for Redis replication groups.,Authenticate the users using Redis AUTH by creating a new Redis Cluster with both the --transit-encryption-enabled and --auth-token parameters enabled.,Create a new Redis replication group and set the AtRestEncryptionEnabled parameter to true.,Do nothing. This feature is already enabled by default.,,,"온라인 주식 거래 시스템은 AWS에서 호스팅되며 EC2 인스턴스의 Auto Scaling 그룹, RDS 데이터베이스 및 Redis용 Amazon ElastiCache를 사용합니다. Redis 명령을 실행할 수 있는 권한을 부여받기 전에 사용자에게 암호를 입력하도록 요구하여 메모리 내 데이터 저장소의 데이터 보안을 개선해야 합니다.   위의 요구 사항을 충족하려면 다음 중 무엇을 해야 합니까?",위의 어느 것도 아닙니다.,Redis 복제 그룹에 대해 전송 중 암호화를 활성화합니다.,--transit-encryption-enabled및 매개변수가 활성화된 새 Redis 클러스터를 생성하여 Redis AUTH를 사용하여 사용자를 인증합니다 --auth-token.,새 Redis 복제 그룹을 생성하고 AtRestEncryptionEnabled매개변수를 로 설정합니다 true.,아무것도하지 마세요. 이 기능은 기본적으로 이미 활성화되어 있습니다.,,0,,
udemy,CLF-01,339,A Solutions Architect is managing a three-tier web application that processes credit card payments and online transactions. Static web pages are used on the front-end tier while the application tier contains a single Amazon EC2 instance that handles long-running processes. The data is stored in a MySQL database. The Solutions Architect is instructed to decouple the tiers to create a highly available application.Which of the following options can satisfy the given requirement?,B,B,"Move all the static assets, web pages, and the backend application to a larger instance. Use Auto Scaling in Amazon EC2 instance. Migrate the database to Amazon Aurora.",Move all the static assets and web pages to Amazon S3. Re-host the application to Amazon Elastic Container Service (Amazon ECS) containers and enable Service Auto Scaling. Migrate the database to Amazon RDS with Multi-AZ deployments configuration.,Move all the static assets to Amazon S3. Set concurrency limit in AWS Lambda to move the application to a serverless architecture. Migrate the database to Amazon DynamoDB.,Move all the static assets and web pages to Amazon CloudFront. Use Auto Scaling in Amazon EC2 instance. Migrate the database to Amazon RDS with Multi-AZ deployments configuration.,,,,Solutions Architect는 신용 카드 결제 및 온라인 거래를 처리하는 3계층 웹 애플리케이션을 관리하고 있습니다. 정적 웹 페이지는 프런트 엔드 계층에서 사용되는 반면 애플리케이션 계층에는 장기 실행 프로세스를 처리하는 단일 Amazon EC2 인스턴스가 포함됩니다. 데이터는 MySQL 데이터베이스에 저장됩니다. Solutions Architect는 계층을 분리하여 고가용성 애플리케이션을 생성하도록 지시받습니다.다음 옵션 중 주어진 요구 사항을 충족할 수 있는 옵션은 무엇입니까?,"모든 정적 자산, 웹 페이지 및 백엔드 애플리케이션을 더 큰 인스턴스로 이동합니다. Amazon EC2 인스턴스에서 Auto Scaling을 사용합니다. 데이터베이스를 Amazon Aurora로 마이그레이션합니다.",모든 정적 자산과 웹 페이지를 Amazon S3로 이동합니다. 애플리케이션을 Amazon Elastic Container Service(Amazon ECS) 컨테이너에 다시 호스팅하고 서비스 자동 조정을 활성화합니다. 다중 AZ 배포 구성을 사용하여 데이터베이스를 Amazon RDS로 마이그레이션합니다.,모든 정적 자산을 Amazon S3로 이동합니다. 애플리케이션을 서버리스 아키텍처로 이동하려면 AWS Lambda에서 동시성 제한을 설정하십시오. 데이터베이스를 Amazon DynamoDB로 마이그레이션합니다.,모든 정적 자산과 웹 페이지를 Amazon CloudFront로 이동합니다. Amazon EC2 인스턴스에서 Auto Scaling을 사용합니다. 다중 AZ 배포 구성을 사용하여 데이터베이스를 Amazon RDS로 마이그레이션합니다.,,,0,,
udemy,CLF-01,340,"A company developed a financial analytics web application hosted in a Docker container using MEAN (MongoDB, Express.js, AngularJS, and Node.js) stack. You want to easily port that web application to AWS Cloud which can automatically handle all the tasks such as balancing load, auto-scaling, monitoring, and placing your containers across your cluster.Which of the following services can be used to fulfill this requirement?",B,B,Amazon Elastic Container Service (Amazon ECS),AWS Elastic Beanstalk,AWS Compute Optimizer,AWS CloudFormation,,,,"한 회사에서 MEAN(MongoDB, Express.js, AngularJS 및 Node.js) 스택을 사용하여 Docker 컨테이너에서 호스팅되는 재무 분석 웹 애플리케이션을 개발했습니다. 로드 밸런싱, Auto-Scaling, 모니터링 및 클러스터 전체에 컨테이너 배치와 같은 모든 작업을 자동으로 처리할 수 있는 AWS 클라우드로 해당 웹 애플리케이션을 쉽게 포팅하려고 합니다.다음 중 이 요구 사항을 충족하는 데 사용할 수 있는 서비스는 무엇입니까?",Amazon Elastic Container Service(Amazon ECS),AWS Elastic Beanstalk,AWS 컴퓨팅 옵티마이저,AWS 클라우드포메이션,,,0,,
udemy,CLF-01,341,There is a technical requirement by a financial firm that does online credit card processing to have a secure application environment on AWS. They are trying to decide on whether to use KMS or CloudHSM. Which of the following statements is right when it comes to CloudHSM and KMS?,A,A,"You should consider using AWS CloudHSM over AWS KMS if you require your keys stored in dedicated, third-party validated hardware security modules under your exclusive control.",No major difference. They both do the same thing.,AWS CloudHSM should always be used for any payment transactions.,"If you want a managed service for creating and controlling your encryption keys but don't want or need to operate your own HSM, consider using AWS CloudHSM.",,,,AWS에서 안전한 애플리케이션 환경을 갖추기 위해 온라인 신용 카드 처리를 수행하는 금융 회사의 기술적 요구 사항이 있습니다. KMS를 사용할지 CloudHSM을 사용할지 결정하려고 합니다. 다음 중 CloudHSM 및 KMS에 대한 설명으로 옳은 것은 무엇입니까?,독점 제어 하에 검증된 전용 타사 하드웨어 보안 모듈에 저장된 키가 필요한 경우 AWS KMS를 통해 AWS CloudHSM을 사용하는 것을 고려해야 합니다.,큰 차이는 없습니다. 둘 다 같은 일을합니다.,AWS CloudHSM은 모든 결제 거래에 항상 사용해야 합니다.,암호화 키 생성 및 제어를 위한 관리형 서비스를 원하지만 자체 HSM을 운영하고 싶지 않거나 운영할 필요가 없는 경우 AWS CloudHSM 사용을 고려하십시오.,,,0,,
udemy,CLF-01,342,A top university has recently launched its online learning portal where the students can take e-learning courses from the comforts of their homes. The portal is on a large On-Demand EC2 instance with a single Amazon Aurora database.   How can you improve the availability of your Aurora database to prevent any unnecessary downtime of the online portal?,B,B,Enable Hash Joins to improve the database query performance.,Create Amazon Aurora Replicas.,Use an Asynchronous Key Prefetch in Amazon Aurora to improve the performance of queries that join tables across indexes.,Deploy Aurora to two Auto-Scaling groups of EC2 instances across two Availability Zones with an elastic load balancer which handles load balancing.,,,,일류 대학은 최근 학생들이 집에서 편안하게 e-러닝 과정을 수강할 수 있는 온라인 학습 포털을 개설했습니다. 포털은 단일 Amazon Aurora 데이터베이스가 있는 대규모 온디맨드 EC2 인스턴스에 있습니다.   온라인 포털의 불필요한 다운타임을 방지하기 위해 Aurora 데이터베이스의 가용성을 어떻게 향상시킬 수 있습니까?,데이터베이스 쿼리 성능을 향상시키려면 해시 조인을 활성화하십시오.,Amazon Aurora 복제본을 생성합니다.,Amazon Aurora에서 비동기식 키 미리 가져오기를 사용하여 여러 인덱스에서 테이블을 조인하는 쿼리의 성능을 개선합니다.,로드 밸런싱을 처리하는 탄력적 로드 밸런서를 사용하여 2개의 가용 영역에 걸쳐 EC2 인스턴스의 2개 Auto-Scaling 그룹에 Aurora를 배포합니다.,,,0,,
udemy,CLF-01,343,"A company has a VPC for its Human Resource department and another VPC located in different AWS regions for its Finance department. The Solutions Architect must redesign the architecture to allow the finance department to access all resources that are in the human resource department, and vice versa. An Intrusion Prevention System (IPS) must also be integrated for active traffic flow inspection and to block any vulnerability exploits.Which network architecture design in AWS should the Solutions Architect set up to satisfy the above requirement?",B,B,Create a Traffic Policy in Amazon Route 53 to connect the two VPCs. Configure the Route 53 Resolver DNS Firewall to do active traffic flow inspection and block any vulnerability exploits.,Launch an AWS Transit Gateway and add VPC attachments to connect all departments. Set up AWS Network Firewall to secure the application traffic travelling between the VPCs.,Create a Direct Connect Gateway and add VPC attachments to connect all departments. Configure AWS Security Hub to secure the application traffic travelling between the VPCs.,Establish a secure connection between the two VPCs using a NAT Gateway. Manage user sessions via the AWS Systems Manager Session Manager service.,,,,회사에는 인적 자원 부서를 위한 VPC가 있고 재무 부서를 위한 다른 AWS 리전에 또 다른 VPC가 있습니다. Solutions Architect는 재무 부서가 인사 부서에 있는 모든 리소스에 액세스할 수 있도록 아키텍처를 재설계해야 하며 그 반대의 경우도 마찬가지입니다. IPS(침입 방지 시스템)도 활성 트래픽 흐름 검사를 위해 통합하고 모든 취약성 악용을 차단해야 합니다.솔루션 아키텍트가 위의 요구 사항을 충족하기 위해 설정해야 하는 AWS의 네트워크 아키텍처 디자인은 무엇입니까?,Amazon Route 53에서 트래픽 정책을 생성하여 두 VPC를 연결합니다. 활성 트래픽 흐름 검사를 수행하고 취약성 악용을 차단하도록 Route 53 Resolver DNS 방화벽을 구성합니다.,AWS Transit Gateway를 시작하고 VPC 연결을 추가하여 모든 부서를 연결합니다. VPC 간에 이동하는 애플리케이션 트래픽을 보호하도록 AWS Network Firewall을 설정합니다.,Direct Connect 게이트웨이를 생성하고 VPC 연결을 추가하여 모든 부서를 연결합니다. VPC 간에 이동하는 애플리케이션 트래픽을 보호하도록 AWS Security Hub를 구성합니다.,NAT 게이트웨이를 사용하여 두 VPC 간에 보안 연결을 설정합니다. AWS Systems Manager Session Manager 서비스를 통해 사용자 세션을 관리합니다.,,,0,,
udemy,CLF-01,344,A manufacturing company launched a new type of IoT sensor. The sensor will be used to collect large streams of data records. You need to create a solution that can ingest and analyze the data in real-time with millisecond response times.Which of the following is the best option that you should implement in this scenario?,A,A,Ingest the data using Amazon Kinesis Data Streams and create an AWS Lambda function to store the data in Amazon DynamoDB.,Ingest the data using Amazon Simple Queue Service and create an AWS Lambda function to store the data in Amazon Redshift.,Ingest the data using Amazon Kinesis Data Streams and create an AWS Lambda function to store the data in Amazon Redshift.,Ingest the data using Amazon Kinesis Data Firehose and create an AWS Lambda function to store the data in Amazon DynamoDB.,,,,한 제조업체가 새로운 유형의 IoT 센서를 출시했습니다. 센서는 대량의 데이터 기록을 수집하는 데 사용됩니다. 밀리초 응답 시간으로 실시간으로 데이터를 수집하고 분석할 수 있는 솔루션을 만들어야 합니다.다음 중 이 시나리오에서 구현해야 하는 가장 좋은 옵션은 무엇입니까?,Amazon Kinesis Data Streams를 사용하여 데이터를 수집하고 AWS Lambda 함수를 생성하여 Amazon DynamoDB에 데이터를 저장합니다.,Amazon Simple Queue Service를 사용하여 데이터를 수집하고 AWS Lambda 함수를 생성하여 Amazon Redshift에 데이터를 저장합니다.,Amazon Kinesis Data Streams를 사용하여 데이터를 수집하고 AWS Lambda 함수를 생성하여 Amazon Redshift에 데이터를 저장합니다.,Amazon Kinesis Data Firehose를 사용하여 데이터를 수집하고 AWS Lambda 함수를 생성하여 Amazon DynamoDB에 데이터를 저장합니다.,,,0,,
udemy,CLF-01,345,"A game development company operates several virtual reality (VR) and augmented reality (AR) games which use various RESTful web APIs hosted on their on-premises data center. Due to the unprecedented growth of their company, they decided to migrate their system to AWS Cloud to scale out their resources as well to minimize costs.  Which of the following should you recommend as the most cost-effective and scalable solution to meet the above requirement?",C,C,Host the APIs in a static S3 web hosting bucket behind a CloudFront web distribution.,"Use a Spot Fleet of Amazon EC2 instances, each with an Elastic Fabric Adapter (EFA) for more consistent latency and higher network throughput. Set up an Application Load Balancer to distribute traffic to the instances.",Use AWS Lambda and Amazon API Gateway.,"Set up a micro-service architecture with ECS, ECR, and Fargate.",,,,게임 개발 회사는 온프레미스 데이터 센터에서 호스팅되는 다양한 RESTful 웹 API를 사용하는 여러 가상 현실(VR) 및 증강 현실(AR) 게임을 운영합니다. 회사의 전례 없는 성장으로 인해 시스템을 AWS 클라우드로 마이그레이션하여 리소스를 확장하고 비용을 최소화하기로 결정했습니다.  다음 중 위의 요구 사항을 충족하기 위해 가장 비용 효율적이고 확장 가능한 솔루션으로 권장해야 하는 것은 무엇입니까?,CloudFront 웹 배포 뒤의 정적 S3 웹 호스팅 버킷에서 API를 호스팅합니다.,더 일관된 지연 시간과 더 높은 네트워크 처리량을 위해 각각 Elastic Fabric Adapter(EFA)가 있는 Amazon EC2 인스턴스의 스팟 플릿을 사용합니다. 인스턴스에 트래픽을 분산하도록 Application Load Balancer를 설정합니다.,AWS Lambda 및 Amazon API Gateway를 사용합니다.,"ECS, ECR 및 Fargate로 마이크로 서비스 아키텍처를 설정하십시오.",,,0,,
udemy,CLF-01,346,"A company is designing a customized text messaging service that targets its mobile app users. As part of its multi-engagement marketing campaign, a company needs to send a one-time confirmation message to all of its subscribers using Short Message Service (SMS). The solutions architect must design the system to allow a subscriber to reply to the SMS messages.The customer responses must be kept for an entire year for analysis and targeted sale promotions. In addition, the SMS responses must also be collected, processed, and analyzed in near-real-time.Which solution will meet these requirements with the LEAST operational overhead?",B,B,"Launch a new Amazon Simple Queue Service (Amazon SQS) queue to send out SMS messages. Use AWS Step Functions and AWS Lambda to collect, process, and analyze responses. Store the data to Amazon S3 Glacier Instant Retrieval.","Create an Amazon Pinpoint journey for the multi-engagement SMS marketing campaign and an Amazon Kinesis Data Stream for analysis. Configure Amazon Pinpoint to send events to the Kinesis data stream for collection, processing, and analysis. Set the retention period of the Kinesis data stream to 365 days.",Set up an Amazon Connect contact flow to send the confirmation SMS messages to the mobile app users. Deploy an AWS Lambda function to process and analyze the responses. Store the data to Amazon S3 Glacier Flexible Retrieval,"Create a new topic in Amazon Simple Notification Service (Amazon SNS) and an Amazon Kinesis data stream configured with all its default settings. Send SMS messages using Amazon SNS. Integrate the Kinesis data stream to the SNS topic for data collection, archiving, and analysis.",,,,"회사에서 모바일 앱 사용자를 대상으로 하는 맞춤형 문자 메시지 서비스를 설계하고 있습니다. 다중 참여 마케팅 캠페인의 일환으로 회사는 SMS(Short Message Service)를 사용하여 모든 가입자에게 일회성 확인 메시지를 보내야 합니다. 솔루션 설계자는 가입자가 SMS 메시지에 회신할 수 있도록 시스템을 설계해야 합니다.고객 응답은 분석 및 대상 판매 프로모션을 위해 1년 동안 보관해야 합니다. 또한 SMS 응답도 거의 실시간으로 수집, 처리 및 분석해야 합니다.최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까?","새로운 Amazon Simple Queue Service(Amazon SQS) 대기열을 시작하여 SMS 메시지를 보냅니다. AWS Step Functions 및 AWS Lambda를 사용하여 응답을 수집, 처리 및 분석합니다. Amazon S3 Glacier Instant Retrieval에 데이터를 저장합니다.","다중 참여 SMS 마케팅 캠페인을 위한 Amazon Pinpoint 여정과 분석을 위한 Amazon Kinesis Data Stream을 생성합니다. 수집, 처리 및 분석을 위해 이벤트를 Kinesis 데이터 스트림으로 보내도록 Amazon Pinpoint를 구성합니다. Kinesis 데이터 스트림의 보존 기간을 365일로 설정합니다.",모바일 앱 사용자에게 확인 SMS 메시지를 보내도록 Amazon Connect 통화 흐름을 설정합니다. 응답을 처리하고 분석하기 위해 AWS Lambda 함수를 배포합니다. 데이터를 Amazon S3 Glacier Flexible Retrieval에 저장,"Amazon Simple Notification Service(Amazon SNS)에서 새 주제를 생성하고 모든 기본 설정으로 구성된 Amazon Kinesis 데이터 스트림을 생성합니다. Amazon SNS를 사용하여 SMS 메시지를 보냅니다. 데이터 수집, 보관 및 분석을 위해 Kinesis 데이터 스트림을 SNS 주제에 통합합니다.",,,0,,
udemy,CLF-01,347,A Solutions Architect is implementing a new High-Performance Computing (HPC) system in AWS that involves orchestrating several Amazon Elastic Container Service (Amazon ECS) tasks with an EC2 launch type that is part of an Amazon ECS cluster. The system will be frequently accessed by users around the globe and it is expected that there would be hundreds of ECS tasks running most of the time. The Architect must ensure that its storage system is optimized for high-frequency read and write operations. The output data of each ECS task is around 10 MB but the obsolete data will eventually be archived and deleted so the total storage size won’t exceed 10 TB.Which of the following is the MOST suitable solution that the Architect should recommend?,C,C,Launch an Amazon DynamoDB table with Amazon DynamoDB Accelerator (DAX) and DynamoDB Streams enabled. Configure the table to be accessible by all Amazon ECS cluster instances. Set the DynamoDB table as the container mount point in the ECS task definition of the Amazon ECS cluster.,Set up an SMB file share by creating an Amazon FSx File Gateway in Storage Gateway. Set the file share as the container mount point in the ECS task definition of the Amazon ECS cluster.,Launch an Amazon Elastic File System (Amazon EFS) with Provisioned Throughput mode and set the performance mode to  Max I/O. Configure the EFS file system as the container mount point in the ECS task definition of the Amazon ECS cluster.,Launch an Amazon Elastic File System (Amazon EFS) file system with Bursting Throughput mode and set the performance mode to General Purpose. Configure the EFS file system as the container mount point in the ECS task definition of the Amazon ECS cluster.,,,,Solutions Architect는 Amazon ECS 클러스터의 일부인 EC2 시작 유형으로 여러 Amazon Elastic Container Service(Amazon ECS) 작업을 오케스트레이션하는 것과 관련된 새로운 고성능 컴퓨팅(HPC) 시스템을 AWS에 구현하고 있습니다. 이 시스템은 전 세계 사용자가 자주 액세스할 것이며 대부분의 시간 동안 수백 개의 ECS 작업이 실행될 것으로 예상됩니다. Architect는 스토리지 시스템이 빈도가 높은 읽기 및 쓰기 작업에 최적화되어 있는지 확인해야 합니다. 각 ECS 작업의 출력 데이터는 약 10MB이지만 사용되지 않는 데이터는 결국 보관 및 삭제되므로 총 스토리지 크기는 10TB를 초과하지 않습니다.다음 중 Architect가 추천해야 하는 가장 적합한 솔루션은 무엇입니까?,Amazon DynamoDB Accelerator(DAX) 및 DynamoDB 스트림이 활성화된 Amazon DynamoDB 테이블을 시작합니다. 모든 Amazon ECS 클러스터 인스턴스에서 액세스할 수 있도록 테이블을 구성합니다. Amazon ECS 클러스터의 ECS 작업 정의에서 DynamoDB 테이블을 컨테이너 탑재 지점으로 설정합니다.,Storage Gateway에서 Amazon FSx 파일 게이트웨이를 생성하여 SMB 파일 공유를 설정합니다. Amazon ECS 클러스터의 ECS 작업 정의에서 파일 공유를 컨테이너 탑재 지점으로 설정합니다.,프로비저닝된 처리량 모드로 Amazon Elastic File System(Amazon EFS)을 시작하고 성능 모드를   Max I/O. Amazon ECS 클러스터의 ECS 작업 정의에서 EFS 파일 시스템을 컨테이너 탑재 지점으로 구성합니다.,버스팅 처리량 모드로 Amazon Elastic File System(Amazon EFS) 파일 시스템을 시작하고 성능 모드를 General Purpose. Amazon ECS 클러스터의 ECS 작업 정의에서 EFS 파일 시스템을 컨테이너 탑재 지점으로 구성합니다.,,,0,,
udemy,CLF-01,348,"A Solutions Architect designed a real-time data analytics system based on Kinesis Data Stream and Lambda. A week after the system has been deployed, the users noticed that it performed slowly as the data rate increases. The Architect identified that the performance of the Kinesis Data Streams is causing this problem. Which of the following should the Architect do to improve performance?",A,A,Increase the number of shards of the Kinesis stream by using the UpdateShardCount command.,Improve the performance of the stream by decreasing the number of its shards using the MergeShard command.,Replace the data stream with Amazon Kinesis Data Firehose instead.,Implement Step Scaling to the Kinesis Data Stream.,,,,Solutions Architect는 Kinesis Data Stream 및 Lambda를 기반으로 실시간 데이터 분석 시스템을 설계했습니다. 시스템이 배포된 지 일주일 후 사용자는 데이터 속도가 증가함에 따라 성능이 느려지는 것을 알게 되었습니다. 설계자는 Kinesis Data Streams의 성능이 이 문제의 원인임을 확인했습니다.아키텍트가 성능을 향상시키기 위해 다음 중 무엇을 해야 합니까?,명령을 사용하여 Kinesis 스트림의 샤드 수를 늘립니다 UpdateShardCount.,명령 을 사용하여 샤드 수를 줄여 스트림의 성능을 개선합니다 MergeShard.,대신 데이터 스트림을 Amazon Kinesis Data Firehose로 교체하십시오.,Kinesis Data Stream에 대한 단계 조정을 구현합니다.,,,0,,
udemy,CLF-01,349,An application is hosted in an Auto Scaling group of EC2 instances and a Microsoft SQL Server on Amazon RDS. There is a requirement that all in-flight data between your web servers and RDS should be secured. Which of the following options is the MOST suitable solution that you should implement? (Select TWO.),AB,AB,Download the Amazon RDS Root CA certificate. Import the certificate to your servers and configure your application to use SSL to encrypt the connection to RDS.,"Force all connections to your DB instance to use SSL by setting the rds.force_ssl parameter to true. Once done, reboot your DB instance.",Enable the IAM DB authentication in RDS using the AWS Management Console.,Configure the security groups of your EC2 instances and RDS to only allow traffic to and from port 443.,Specify the TDE option in an RDS option group that is associated with that DB instance to enable transparent data encryption (TDE).,,,애플리케이션은 EC2 인스턴스의 Auto Scaling 그룹과 Amazon RDS의 Microsoft SQL Server에서 호스팅됩니다. 웹 서버와 RDS 간의 전송 중인 모든 데이터를 보호해야 한다는 요구 사항이 있습니다.다음 옵션 중 구현해야 하는 가장 적합한 솔루션은 무엇입니까? (2개를 선택하세요.),Amazon RDS 루트 CA 인증서를 다운로드합니다. 인증서를 서버로 가져오고 SSL을 사용하여 RDS에 대한 연결을 암호화하도록 애플리케이션을 구성합니다.,파라미터를 true로 설정하여 SSL을 사용하도록 DB 인스턴스에 대한 모든 연결을 강제 적용합니다 rds.force_ssl. 완료되면 DB 인스턴스를 재부팅합니다.,AWS Management Console을 사용하여 RDS에서 IAM DB 인증을 활성화합니다.,포트 443에서 들어오고 나가는 트래픽만 허용하도록 EC2 인스턴스 및 RDS의 보안 그룹을 구성합니다.,TDE(투명한 데이터 암호화)를 활성화하려면 해당 DB 인스턴스와 연결된 RDS 옵션 그룹에서 TDE 옵션을 지정합니다.,,0,,
udemy,CLF-01,350,"A computer animation film studio has a web application running on an Amazon EC2 instance. It uploads 5 GB video objects to an Amazon S3 bucket. Video uploads are taking longer than expected, which impacts the performance of your application.Which method will help improve the performance of the application?",A,A,Use Amazon S3 Multipart Upload API.,Use Amazon Elastic Block Store Provisioned IOPS and an Amazon EBS-optimized instance.,Leverage on Amazon CloudFront and use HTTP POST method to reduce latency.,Enable Enhanced Networking with the Elastic Network Adapter (ENA) on your EC2 Instances.,,,,컴퓨터 애니메이션 영화 스튜디오에는 Amazon EC2 인스턴스에서 실행되는 웹 애플리케이션이 있습니다. Amazon S3 버킷에 5GB 비디오 객체를 업로드합니다. 비디오 업로드가 예상보다 오래 걸리고 있어 애플리케이션 성능에 영향을 미칩니다.어떤 방법이 응용 프로그램의 성능을 향상시키는 데 도움이 됩니까?,Amazon S3 멀티파트 업로드 API를 사용합니다.,Amazon Elastic Block Store 프로비저닝된 IOPS 및 Amazon EBS 최적화 인스턴스를 사용합니다.,Amazon CloudFront를 활용하고 HTTP POST 방법을 사용하여 지연 시간을 줄입니다.,EC2 인스턴스에서 Elastic Network Adapter(ENA)로 향상된 네트워킹을 활성화하십시오.,,,0,,
udemy,CLF-01,351,A company has several websites and hosts its infrastructure on the AWS Cloud. The mission-critical web applications are hosted on fleets of Amazon EC2 instances behind Application Load Balancers. The company uses AWS Certificate Manager (ACM) provided certificate on the ALBs to enable HTTPS access on its websites. The security team wants to get notified 30 days before the expiration of the SSL certificates.Which of the following can the Solutions Architect implement to meet this request? (Select TWO.),BD,BD,Use AWS Config to manually create a rule that checks for certificate expiry on ACM. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to send an alert to an Amazon Simple Notification Service (Amazon SNS) topic when AWS Config flags a resource.,Create an Amazon EventBridge (Amazon CloudWatch Events) rule and schedule it to run every day to identify the expiring ACM certificates. Configure to rule to check the DaysToExpiry metric of all ACM certificates in Amazon CloudWatch. Send an alert notification to an Amazon Simple Notification Service (Amazon SNS) topic when a certificate is going to expire in 30 days.,Modify all certificates to use the AWS Certificate Manager Private Certificate Authority. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that will check for ACM events that shows certificates expiring within 30 days. Set the target to invoke an AWS Lambda function to send a message to an Amazon SNS topic.,Create an Amazon EventBridge (Amazon CloudWatch Events) rule that will check AWS Health or ACM expiration events related to ACM certificates. Send an alert notification to an Amazon Simple Notification Service (Amazon SNS) topic when a certificate is going to expire in 30 days.,"Utilize AWS Trusted Advisor to check for the ACM certificates that will expire in 30 days. Using this metric, create an Amazon CloudWatch alarm that will send an alert to an AWS Systems Manager OpsItem.",,,회사는 여러 웹 사이트를 보유하고 있으며 AWS 클라우드에서 인프라를 호스팅합니다. 미션 크리티컬 웹 애플리케이션은 Application Load Balancer 뒤에 있는 Amazon EC2 인스턴스 플릿에서 호스팅됩니다. 회사는 ALB에서 AWS Certificate Manager(ACM) 제공 인증서를 사용하여 웹 사이트에서 HTTPS 액세스를 활성화합니다. 보안 팀은 SSL 인증서가 만료되기 30일 전에 알림을 받기를 원합니다.이 요청을 충족하기 위해 Solutions Architect가 구현할 수 있는 것은 다음 중 무엇입니까? (2개를 선택하세요.),AWS Config를 사용하여 ACM에서 인증서 만료를 확인하는 규칙을 수동으로 생성합니다. AWS Config가 리소스에 플래그를 지정할 때 Amazon Simple Notification Service(Amazon SNS) 주제에 알림을 보내도록 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다.,Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성하고 매일 실행되도록 예약하여 만료되는 ACM 인증서를 식별합니다. Amazon CloudWatch에서 모든 ACM 인증서의 지표를 확인하도록 규칙을 구성합니다 DaysToExpiry. 인증서가 30일 후에 만료될 예정이면 Amazon Simple Notification Service(Amazon SNS) 주제로 알림을 보냅니다.,AWS Certificate Manager Private Certificate Authority를 ​​사용하도록 모든 인증서를 수정합니다. 30일 이내에 만료되는 인증서를 표시하는 ACM 이벤트를 확인하는 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다. AWS Lambda 함수를 호출하여 Amazon SNS 주제로 메시지를 보내도록 대상을 설정합니다.,ACM 인증서와 관련된 AWS 상태 또는 ACM 만료 이벤트를 확인하는 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다. 인증서가 30일 후에 만료될 예정이면 Amazon Simple Notification Service(Amazon SNS) 주제로 알림을 보냅니다.,AWS Trusted Advisor를 활용하여 30일 후에 만료되는 ACM 인증서를 확인하십시오. 이 지표를 사용하여 AWS Systems Manager OpsItem에 알림을 보낼 Amazon CloudWatch 경보를 생성합니다.,,0,,
udemy,CLF-01,352,"A startup is building IoT devices and monitoring applications. They are using IoT sensors to monitor the traffic in real-time by using an Amazon Kinesis Stream that is configured with default settings. It then sends the data to an Amazon S3 bucket every 3 days. When you checked the data in S3 on the 3rd day, only the data for the last day is present and no data is present from 2 days ago.Which of the following is the MOST likely cause of this issue?",B,B,The access of the Kinesis stream to the S3 bucket is insufficient.,"By default, data records in Kinesis are only accessible for 24 hours from the time they are added to a stream.",Someone has manually deleted the record in Amazon S3.,Amazon S3 bucket has encountered a data loss.,,,,신생 기업은 IoT 장치를 구축하고 응용 프로그램을 모니터링합니다. 기본 설정으로 구성된 Amazon Kinesis Streams를 사용하여 실시간으로 트래픽을 모니터링하기 위해 IoT 센서를 사용하고 있습니다. 그런 다음 3일마다 데이터를 Amazon S3 버킷으로 보냅니다. 3일째에 S3에서 데이터를 확인해보니 마지막 날의 데이터만 있고 2일전 데이터는 없습니다.다음 중 이 문제의 원인으로 가장 가능성이 높은 것은 무엇입니까?,S3 버킷에 대한 Kinesis 스트림의 액세스가 충분하지 않습니다.,기본적으로 Kinesis의 데이터 레코드는 스트림에 추가된 후 24시간 동안만 액세스할 수 있습니다.,누군가 Amazon S3에서 레코드를 수동으로 삭제했습니다.,Amazon S3 버킷에서 데이터 손실이 발생했습니다.,,,0,,
udemy,CLF-01,353,"A company is looking for a way to analyze the calls between customers and service agents. Each conversation is transcribed, JSON-formatted, and saved to an Amazon S3 bucket. The company’s solutions architect is tasked to design a solution for extracting and visualizing sentiments from the transcribed files.Which solution meets the requirements while minimizing the amount of operational overhead?",A,A,Create an Amazon Comprehend analysis job. Index the sentiment along with the transcript to an Amazon OpenSearch cluster. Visualize the results using the OpenSearch Dashboard.,Train a custom Natural Language Processing (NLP) model using Amazon SageMaker. Index the sentiment along with the transcript to an Amazon OpenSearch cluster. Visualize the results using the OpenSearch Dashboard.,Analyze the JSON files with Amazon Textract. Index the sentiment along with the transcript to an Amazon OpenSearch cluster. Visualize the results using Amazon Managed Grafana.,Create an Amazon Comprehend analysis job. Index the sentiment along with the transcript to an Amazon OpenSearch cluster. Visualize the results using Amazon Managed Grafana.,,,,회사에서 고객과 서비스 에이전트 간의 통화를 분석하는 방법을 찾고 있습니다. 각 대화는 기록되고 JSON 형식으로 Amazon S3 버킷에 저장됩니다. 이 회사의 솔루션 설계자는 기록된 파일에서 감정을 추출하고 시각화하기 위한 솔루션을 설계해야 합니다.운영 오버헤드를 최소화하면서 요구 사항을 충족하는 솔루션은 무엇입니까?,Amazon Comprehend 분석 작업을 생성합니다. Amazon OpenSearch 클러스터에 대한 트랜스크립트와 함께 감정을 인덱싱합니다. OpenSearch 대시보드를 사용하여 결과를 시각화합니다.,Amazon SageMaker를 사용하여 사용자 지정 자연어 처리(NLP) 모델을 교육합니다. Amazon OpenSearch 클러스터에 대한 트랜스크립트와 함께 감정을 인덱싱합니다. OpenSearch 대시보드를 사용하여 결과를 시각화합니다.,Amazon Textract로 JSON 파일을 분석합니다. Amazon OpenSearch 클러스터에 대한 트랜스크립트와 함께 감정을 인덱싱합니다. Amazon Managed Grafana를 사용하여 결과를 시각화합니다.,Amazon Comprehend 분석 작업을 생성합니다. Amazon OpenSearch 클러스터에 대한 트랜스크립트와 함께 감정을 인덱싱합니다. Amazon Managed Grafana를 사용하여 결과를 시각화합니다.,,,0,,
udemy,CLF-01,354,"A company created a VPC with a single subnet then launched an On-Demand EC2 instance in that subnet. You have attached an Internet gateway (IGW) to the VPC and verified that the EC2 instance has a public IP. The main route table of the VPC is as shown below:However, the instance still cannot be reached from the Internet when you tried to connect to it from your computer. Which of the following should be made to the route table to fix this issue?",A,A,Add this new entry to the route table: 0.0.0.0/0 -> Your Internet Gateway,Modify the above route table: 10.0.0.0/27 -> Your Internet Gateway,Add the following entry to the route table: 10.0.0.0/27 -> Your Internet Gateway,Add this new entry to the route table: 0.0.0.0/27 -> Your Internet Gateway,,,,회사에서 단일 서브넷이 있는 VPC를 만든 다음 해당 서브넷에서 온디맨드 EC2 인스턴스를 시작했습니다. 인터넷 게이트웨이(IGW)를 VPC에 연결했고 EC2 인스턴스에 퍼블릭 IP가 있는지 확인했습니다. VPC의 기본 라우팅 테이블은 다음과 같습니다.그러나 컴퓨터에서 인스턴스에 연결하려고 하면 여전히 인터넷에서 인스턴스에 연결할 수 없습니다. 다음 중 이 문제를 해결하기 위해 경로 테이블에 만들어야 하는 것은 무엇입니까?,경로 테이블에 이 새 항목을 추가합니다. 0.0.0.0/0 -> Your Internet Gateway,위의 라우팅 테이블 수정: 10.0.0.0/27 -> Your Internet Gateway,경로 테이블에 다음 항목을 추가합니다. 10.0.0.0/27 -> Your Internet Gateway,경로 테이블에 이 새 항목을 추가합니다. 0.0.0.0/27 -> Your Internet Gateway,,,0,,
udemy,CLF-01,355,"A disaster recovery team is planning to back up on-premises records to a local file server share through SMB protocol. To meet the company’s business continuity plan, the team must ensure that a copy of data from 48 hours ago is available for immediate access. Accessing older records with delay is tolerable.Which should the DR team implement to meet the objective with the LEAST amount of configuration effort?",B,B,Create an AWS Backup plan to copy data backups to a local SMB share every 48 hours.,Use an AWS Storage File gateway with enough storage to keep data from the last 48 hours. Send the backups to an SMB share mounted as a local disk.,Create an SMB file share in Amazon FSx for Windows File Server that has enough storage to store all backups. Access the file share from on-premises.,Mount an Amazon EFS file system on the on-premises client and copy all backups to an NFS share.,,,,재해 복구 팀은 SMB 프로토콜을 통해 온-프레미스 레코드를 로컬 파일 서버 공유에 백업할 계획입니다. 회사의 비즈니스 연속성 계획을 충족하기 위해 팀은 48시간 전의 데이터 사본을 즉시 액세스할 수 있도록 해야 합니다. 지연된 오래된 레코드에 액세스하는 것은 허용됩니다.최소한의 구성 노력으로 목표를 달성하기 위해 DR 팀이 구현해야 하는 것은 무엇입니까?,48시간마다 데이터 백업을 로컬 SMB 공유에 복사하는 AWS Backup 계획을 생성합니다.,지난 48시간 동안의 데이터를 보관하기에 충분한 스토리지가 있는 AWS 스토리지 파일 게이트웨이를 사용하십시오. 로컬 디스크로 마운트된 SMB 공유로 백업을 보냅니다.,모든 백업을 저장하기에 충분한 스토리지가 있는 Amazon FSx for Windows File Server에서 SMB 파일 공유를 생성합니다. 온프레미스에서 파일 공유에 액세스합니다.,온프레미스 클라이언트에 Amazon EFS 파일 시스템을 탑재하고 모든 백업을 NFS 공유에 복사합니다.,,,0,,
udemy,CLF-01,356,"A multinational company has been building its new data analytics platform with high-performance computing workloads (HPC) which requires a scalable, POSIX-compliant storage service. The data need to be stored redundantly across multiple AZs and allows concurrent connections from thousands of EC2 instances hosted on multiple Availability Zones. Which of the following AWS storage service is the most suitable one to use in this scenario?",D,D,Amazon S3,Amazon EBS Volumes,Amazon ElastiCache,Amazon Elastic File System,,,,다국적 기업은 확장 가능한 POSIX 호환 스토리지 서비스가 필요한 고성능 컴퓨팅 워크로드(HPC)로 새로운 데이터 분석 플랫폼을 구축하고 있습니다. 데이터는 여러 AZ에 중복 저장되어야 하며 여러 가용 영역에서 호스팅되는 수천 개의 EC2 인스턴스에서 동시 연결을 허용해야 합니다.다음 중 이 시나리오에서 사용하기에 가장 적합한 AWS 스토리지 서비스는 무엇입니까?,아마존 S3,Amazon EBS 볼륨,아마존 엘라스티캐시,아마존 탄력적 파일 시스템,,,0,,
udemy,CLF-01,357,"A company has several EC2 Reserved Instances in their account that need to be decommissioned and shut down since they are no longer used by the development team. However, the data is still required by the audit team for compliance purposes.Which of the following steps can be taken in this scenario? (Select TWO.)",AE,AE,Take snapshots of the EBS volumes and terminate the EC2 instances.,Convert the EC2 instances to Spot instances with a persistent Spot request type.,Convert the EC2 instance to On-Demand instances,Stop all the running EC2 instances.,You can opt to sell these EC2 instances on the AWS Reserved Instance Marketplace,,,회사 계정에는 개발 팀에서 더 이상 사용하지 않기 때문에 폐기하고 종료해야 하는 여러 EC2 예약 인스턴스가 있습니다. 그러나 데이터는 규정 준수를 위해 감사 팀에서 여전히 필요합니다.다음 중 이 시나리오에서 수행할 수 있는 단계는 무엇입니까? (2개를 선택하세요.),EBS 볼륨의 스냅샷을 만들고 EC2 인스턴스를 종료합니다.,영구 스팟 요청 유형을 사용하여 EC2 인스턴스를 스팟 인스턴스로 변환합니다.,EC2 인스턴스를 온디맨드 인스턴스로 변환,실행 중인 모든 EC2 인스턴스를 중지합니다.,AWS 예약 인스턴스 마켓플레이스에서 이러한 EC2 인스턴스를 판매하도록 선택할 수 있습니다.,,0,,
udemy,CLF-01,358,"A startup is building a microservices architecture in which the software is composed of small independent services that communicate over well-defined APIs. In building large-scale systems, fine-grained decoupling of microservices is a recommended practice to implement. The decoupled services should scale horizontally from each other to improve scalability.What is the difference between Horizontal scaling and Vertical scaling?",C,C,Vertical scaling means running the same software on a fully serverless architecture using Lambda. Horizontal scaling means adding more servers to the existing pool and it doesn’t run into limitations of individual servers.,Horizontal scaling means running the same software on bigger machines which is limited by the capacity of individual servers. Vertical scaling is adding more servers to the existing pool and doesn’t run into limitations of individual servers.,Vertical scaling means running the same software on bigger machines which is limited by the capacity of the individual server. Horizontal scaling is adding more servers to the existing pool and doesn’t run into limitations of individual servers.,Horizontal scaling means running the same software on smaller containers such as Docker and Kubernetes using ECS or EKS. Vertical scaling is adding more servers to the existing pool and doesn’t run into limitations of individual servers.,,,,스타트업은 소프트웨어가 잘 정의된 API를 통해 통신하는 작은 독립 서비스로 구성된 마이크로서비스 아키텍처를 구축하고 있습니다. 대규모 시스템을 구축할 때 마이크로 서비스의 세분화된 분리를 구현하는 것이 권장됩니다. 분리된 서비스는 확장성을 개선하기 위해 서로 수평으로 확장되어야 합니다.수평 스케일링과 수직 스케일링의 차이점은 무엇입니까?,수직 확장이란 Lambda를 사용하여 완전한 서버리스 아키텍처에서 동일한 소프트웨어를 실행하는 것을 의미합니다. 수평적 확장은 기존 풀에 더 많은 서버를 추가하는 것을 의미하며 개별 서버의 제한에 부딪히지 않습니다.,수평적 확장은 개별 서버의 용량에 의해 제한되는 더 큰 시스템에서 동일한 소프트웨어를 실행하는 것을 의미합니다. 수직 확장은 기존 풀에 더 많은 서버를 추가하고 개별 서버의 제한에 부딪히지 않습니다.,수직적 확장은 개별 서버의 용량에 의해 제한되는 더 큰 시스템에서 동일한 소프트웨어를 실행하는 것을 의미합니다. 수평적 확장은 기존 풀에 더 많은 서버를 추가하고 개별 서버의 제한에 부딪히지 않습니다.,수평 확장은 ECS 또는 EKS를 사용하여 Docker 및 Kubernetes와 같은 더 작은 컨테이너에서 동일한 소프트웨어를 실행하는 것을 의미합니다. 수직 확장은 기존 풀에 더 많은 서버를 추가하고 개별 서버의 제한에 부딪히지 않습니다.,,,0,,
udemy,CLF-01,359,"A company has an application architecture that stores both the access key ID and the secret access key in a plain text file on a custom Amazon Machine Image (AMI). The EC2 instances, which are created by using this AMI, are using the stored access keys to connect to a DynamoDB table.What should the Solutions Architect do to make the current architecture more secure?",D,D,Put the access keys in Amazon Glacier instead.,Do nothing. The architecture is already secure because the access keys are already in the Amazon Machine Image.,Put the access keys in an Amazon S3 bucket instead.,Remove the stored access keys in the AMI. Create a new IAM role with permissions to access the DynamoDB table and assign it to the EC2 instances.,,,,회사에는 사용자 지정 Amazon Machine Image(AMI)의 일반 텍스트 파일에 액세스 키 ID와 보안 액세스 키를 모두 저장하는 애플리케이션 아키텍처가 있습니다. 이 AMI를 사용하여 생성된 EC2 인스턴스는 저장된 액세스 키를 사용하여 DynamoDB 테이블에 연결합니다.현재 아키텍처를 보다 안전하게 만들기 위해 Solutions Architect는 무엇을 해야 합니까?,대신 Amazon Glacier에 액세스 키를 넣습니다.,아무것도하지 마세요. 액세스 키가 이미 Amazon 머신 이미지에 있기 때문에 아키텍처는 이미 안전합니다.,대신 Amazon S3 버킷에 액세스 키를 넣습니다.,AMI에 저장된 액세스 키를 제거합니다. DynamoDB 테이블에 액세스할 수 있는 권한이 있는 새 IAM 역할을 생성하고 EC2 인스턴스에 할당합니다.,,,0,,
udemy,CLF-01,360,"An online shopping platform has been deployed to AWS using Elastic Beanstalk. They simply uploaded their Node.js application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. Since the entire deployment process is automated, the DevOps team is not sure where to get the application log files of their shopping platform.  In Elastic Beanstalk, where does it store the application files and server log files?",B,B,Application files are stored in S3. The server log files can be optionally stored in CloudTrail or in CloudWatch Logs.,Application files are stored in S3. The server log files can also optionally be stored in S3 or in CloudWatch Logs.,"Application files are stored in S3. The server log files can only be stored in the attached EBS volumes of the EC2 instances, which were launched by AWS Elastic Beanstalk.",Application files are stored in S3. The server log files can be stored directly in Glacier or in CloudWatch Logs.,,,,"Elastic Beanstalk를 사용하여 온라인 쇼핑 플랫폼이 AWS에 배포되었습니다. Node.js 애플리케이션을 업로드하기만 하면 Elastic Beanstalk가 용량 프로비저닝, 로드 밸런싱, 조정 및 애플리케이션 상태 모니터링의 세부 정보를 자동으로 처리합니다. 전체 배포 프로세스가 자동화되기 때문에 DevOps 팀은 쇼핑 플랫폼의 애플리케이션 로그 파일을 어디서 얻을 수 있는지 확신할 수 없습니다.  Elastic Beanstalk에서 애플리케이션 파일과 서버 로그 파일은 어디에 저장됩니까?",애플리케이션 파일은 S3에 저장됩니다. 서버 로그 파일은 선택적으로 CloudTrail 또는 CloudWatch Logs에 저장할 수 있습니다.,애플리케이션 파일은 S3에 저장됩니다. 서버 로그 파일은 선택적으로 S3 또는 CloudWatch Logs에 저장할 수도 있습니다.,애플리케이션 파일은 S3에 저장됩니다. 서버 로그 파일은 AWS Elastic Beanstalk에서 시작한 EC2 인스턴스의 연결된 EBS 볼륨에만 저장할 수 있습니다.,애플리케이션 파일은 S3에 저장됩니다. 서버 로그 파일은 Glacier 또는 CloudWatch Logs에 직접 저장할 수 있습니다.,,,0,,
udemy,CLF-01,361,"A game company has a requirement of load balancing the incoming TCP traffic at the transport level (Layer 4) to their containerized gaming servers hosted in AWS Fargate. To maintain performance, it should handle millions of requests per second sent by gamers around the globe while maintaining ultra-low latencies. Which of the following must be implemented in the current architecture to satisfy the new requirement?",C,C,Create a new record in Amazon Route 53 with Weighted Routing policy to load balance the incoming traffic.,Launch a new microservice in AWS Fargate that acts as a load balancer since using an ALB or NLB with Fargate is not possible.,Launch a new Network Load Balancer.,Launch a new Application Load Balancer.,,,,게임 회사는 AWS Fargate에서 호스팅되는 컨테이너화된 게임 서버에 대한 전송 수준(계층 4)의 수신 TCP 트래픽을 로드 밸런싱해야 한다는 요구 사항이 있습니다. 성능을 유지하려면 초저 지연 시간을 유지하면서 전 세계 게이머가 보내는 초당 수백만 건의 요청을 처리해야 합니다.다음 중 새로운 요구 사항을 충족하기 위해 현재 아키텍처에서 구현해야 하는 것은 무엇입니까?,수신 트래픽의 로드 밸런싱을 위해 가중 라우팅 정책을 사용하여 Amazon Route 53에서 새 레코드를 생성합니다.,Fargate와 함께 ALB 또는 NLB를 사용할 수 없으므로 로드 밸런서 역할을 하는 AWS Fargate에서 새 마이크로서비스를 시작합니다.,새 Network Load Balancer를 시작합니다.,새 Application Load Balancer를 시작합니다.,,,0,,
udemy,CLF-01,362,A company has several unencrypted EBS snapshots in their VPC. The Solutions Architect must ensure that all of the new EBS volumes restored from the unencrypted snapshots are automatically encrypted.What should be done to accomplish this requirement?,D,D,Launch new EBS volumes and specify the symmetric customer master key (CMK) for encryption.,Enable the EBS Encryption By Default feature for specific EBS volumes.,Launch new EBS volumes and encrypt them using an asymmetric customer master key (CMK).,Enable the EBS Encryption By Default feature for the AWS Region.,,,,회사의 VPC에는 암호화되지 않은 EBS 스냅샷이 여러 개 있습니다. Solutions Architect는 암호화되지 않은 스냅샷에서 복원된 모든 새 EBS 볼륨이 자동으로 암호화되도록 해야 합니다.이 요구 사항을 달성하려면 어떻게 해야 합니까?,새 EBS 볼륨을 시작하고 암호화를 위한 대칭 고객 마스터 키(CMK)를 지정합니다.,특정 EBS 볼륨에 대해 기본적으로 EBS 암호화 기능을 활성화합니다.,새 EBS 볼륨을 시작하고 비대칭 고객 마스터 키(CMK)를 사용하여 암호화합니다.,AWS 리전에 대해 기본적으로 EBS 암호화 기능을 활성화합니다.,,,0,,
udemy,CLF-01,363,A Solutions Architect is trying to enable Cross-Region Replication to an S3 bucket but this option is disabled. Which of the following options is a valid reason for this?,D,D,The Cross-Region Replication feature is only available for Amazon S3 - Infrequent Access.,The Cross-Region Replication feature is only available for Amazon S3 - One Zone-IA,This is a premium feature which is only for AWS Enterprise accounts.,"In order to use the Cross-Region Replication feature in S3, you need to first enable versioning on the bucket.",,,,Solutions Architect가 S3 버킷에 대한 교차 리전 복제를 활성화하려고 하지만 이 옵션이 비활성화되어 있습니다. 다음 중 이에 대한 타당한 이유는 무엇입니까?,교차 리전 복제 기능은 Amazon S3 - Infrequent Access에서만 사용할 수 있습니다.,교차 리전 복제 기능은 Amazon S3 - One Zone-IA에서만 사용할 수 있습니다.,이것은 AWS Enterprise 계정에만 있는 프리미엄 기능입니다.,S3에서 교차 리전 복제 기능을 사용하려면 먼저 버킷에서 버전 관리를 활성화해야 합니다.,,,0,,
udemy,CLF-01,364,"A tech company is having an issue whenever they try to connect to the newly created EC2 instance using a Remote Desktop connection from a computer. Upon checking, the Solutions Architect has verified that the instance has a public IP and the Internet gateway and route tables are in place.What else should he do to resolve this issue?",B,B,Adjust the security group to allow inbound traffic on port 22 from the company’s IP address.,Adjust the security group to allow inbound traffic on port 3389 from the company’s IP address.,You should restart the EC2 instance since there might be some issue with the instance,You should create a new instance since there might be some issue with the instance,,,,기술 회사가 컴퓨터에서 원격 데스크톱 연결을 사용하여 새로 생성된 EC2 인스턴스에 연결하려고 할 때마다 문제가 발생합니다. 확인 시 Solutions Architect는 인스턴스에 퍼블릭 IP가 있고 인터넷 게이트웨이와 라우팅 테이블이 있는지 확인했습니다.이 문제를 해결하기 위해 그는 또 무엇을 해야 합니까?,회사 IP 주소에서 포트 22의 인바운드 트래픽을 허용하도록 보안 그룹을 조정합니다.,회사 IP 주소에서 포트 3389의 인바운드 트래픽을 허용하도록 보안 그룹을 조정합니다.,인스턴스에 문제가 있을 수 있으므로 EC2 인스턴스를 다시 시작해야 합니다.,인스턴스에 문제가 있을 수 있으므로 새 인스턴스를 만들어야 합니다.,,,0,,
udemy,CLF-01,365,"A website hosted on Amazon ECS container instances loads slowly during peak traffic, affecting its availability. Currently, the container instances are run behind an Application Load Balancer, and CloudWatch alarms are configured to send notifications to the operations team if there is a problem in availability so they can scale out if needed. A solutions architect needs to create an automatic scaling solution when such problems occur.Which solution could satisfy the requirement? (Select TWO.)",AB,AB,Create an AWS Auto Scaling policy that scales out the ECS cluster when the service’s CPU utilization is too high.,Create an AWS Auto Scaling policy that scales out the ECS service when the service’s memory utilization is too high.,Create an AWS Auto Scaling policy that scales out the ECS service when the ALB hits a high CPU utilization.,Create an AWS Auto Scaling policy that scales out an ECS service when the ALB endpoint becomes unreachable.,Create an AWS Auto Scaling policy that scales out the ECS cluster when the ALB target group’s CPU utilization is too high.,,,Amazon ECS 컨테이너 인스턴스에서 호스팅되는 웹 사이트는 피크 트래픽 동안 느리게 로드되어 가용성에 영향을 미칩니다. 현재 컨테이너 인스턴스는 Application Load Balancer 뒤에서 실행되며 CloudWatch 경보는 가용성에 문제가 있는 경우 운영 팀에 알림을 보내 필요한 경우 확장할 수 있도록 구성됩니다. 솔루션 설계자는 이러한 문제가 발생할 때 자동 확장 솔루션을 만들어야 합니다.어떤 솔루션이 요구 사항을 충족할 수 있습니까? (2개를 선택하세요.),서비스의 CPU 사용률이 너무 높을 때 ECS 클러스터를 확장하는 AWS Auto Scaling 정책을 생성합니다.,서비스의 메모리 사용률이 너무 높을 때 ECS 서비스를 확장하는 AWS Auto Scaling 정책을 생성합니다.,ALB가 높은 CPU 사용률에 도달하면 ECS 서비스를 확장하는 AWS Auto Scaling 정책을 생성합니다.,ALB 엔드포인트에 연결할 수 없게 되면 ECS 서비스를 확장하는 AWS Auto Scaling 정책을 생성합니다.,ALB 대상 그룹의 CPU 사용률이 너무 높을 때 ECS 클러스터를 확장하는 AWS Auto Scaling 정책을 생성합니다.,,0,,
udemy,CLF-01,366,"A company has multiple AWS sandbox accounts that are used by its development team. All developers must be given access to the contents of one of the main account’s S3 buckets. For security purposes, any personally identifiable information (PII) or financial data uploaded in the bucket must be continuously monitored and removed.How can this be done at the lowest possible cost and with the least amount of configuration effort?",A,A,Create an S3 bucket policy that grants access from the sandbox accounts. Use Amazon Macie to discover personally identifiable information (PII) or financial data.,Generate a pre-signed URL for the objects on the S3 bucket. Use the Amazon S3 Storage Lens to discover personally identifiable information (PII) or financial data.,Add S3 read permission to the IAM policy of each IAM user from the sandbox accounts. Use Amazon Detective to discover personally identifiable information (PII) or financial data.,Configure cross-account replication on the S3 bucket. Integrate AWS Audit Manager with the S3 bucket to discover any personally identifiable information (PII) or financial data.,,,,회사에는 개발 팀에서 사용하는 여러 AWS 샌드박스 계정이 있습니다. 모든 개발자는 기본 계정의 S3 버킷 중 하나에 있는 콘텐츠에 대한 액세스 권한을 부여받아야 합니다. 보안을 위해 버킷에 업로드된 개인 식별 정보(PII) 또는 금융 데이터는 지속적으로 모니터링하고 제거해야 합니다.가능한 최저 비용과 최소한의 구성 노력으로 어떻게 이 작업을 수행할 수 있습니까?,샌드박스 계정에서 액세스 권한을 부여하는 S3 버킷 정책을 생성합니다. Amazon Macie를 사용하여 개인 식별 정보(PII) 또는 재무 데이터를 검색합니다.,S3 버킷의 객체에 대해 사전 서명된 URL을 생성합니다. Amazon S3 Storage Lens를 사용하여 개인 식별 정보(PII) 또는 금융 데이터를 검색하십시오.,샌드박스 계정에서 각 IAM 사용자의 IAM 정책에 S3 읽기 권한을 추가합니다. Amazon Detective를 사용하여 PII(개인 식별 정보) 또는 금융 데이터를 검색합니다.,S3 버킷에서 교차 계정 복제를 구성합니다. AWS Audit Manager를 S3 버킷과 통합하여 PII(개인 식별 정보) 또는 금융 데이터를 검색합니다.,,,0,,
udemy,CLF-01,367,A cryptocurrency company wants to go global with its international money transfer app. Your project is to make sure that the database of the app is highly available in multiple regions.What are the benefits of adding Multi-AZ deployments in Amazon RDS? (Select TWO.),CE,CE,Significantly increases the database performance.,Creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ) in a different region.,Provides enhanced database durability in the event of a DB instance component failure or an Availability Zone outage.,Provides SQL optimization.,Increased database availability in the case of system upgrades like OS patching or DB Instance scaling.,,,cryptocurrency 회사는 국제 송금 앱으로 세계화를 원합니다. 귀하의 프로젝트는 앱의 데이터베이스가 여러 지역에서 가용성이 높은지 확인하는 것입니다.Amazon RDS에서 다중 AZ 배포를 추가하면 어떤 이점이 있습니까? (2개를 선택하세요.),데이터베이스 성능을 크게 향상시킵니다.,기본 DB 인스턴스를 생성하고 다른 지역의 다른 가용 영역(AZ)에 있는 대기 인스턴스에 데이터를 동기식으로 복제합니다.,DB 인스턴스 구성 요소 장애 또는 가용 영역 중단 시 향상된 데이터베이스 내구성을 제공합니다.,SQL 최적화를 제공합니다.,OS 패치 또는 DB 인스턴스 확장과 같은 시스템 업그레이드의 경우 데이터베이스 가용성이 향상됩니다.,,0,,
udemy,CLF-01,368,"A new DevOps engineer has created a CloudFormation template for a web application and she raised a <code>pull request</code> in GIT for you to check and review. After checking the template, you immediately told her that the template will not work. Which of the following is the reason why this CloudFormation template will fail to deploy the stack?{ ""AWSTemplateFormatVersion"":""2010-09-09"", ""Parameters"":{   ""VPCId"":{     ""Type"":""String"",     ""Description"":""manila"" }, ""SubnetId"":{   ""Type"":""String"",   ""Description"":""subnet-b46032ec"" }},""Outputs"":{ ""InstanceId"":{  ""Value"":{   ""Ref"":""manilaInstance""  },  ""Description"":""Instance Id""  } }}",D,D,The Conditions section is missing.,The value of the AWSTemplateFormatVersion is incorrect. It should be 2017-06-06.,An invalid section named Parameters is present. This will cause the CloudFormation stack to fail.,The Resources section is missing.,,,,"새로운 DevOps 엔지니어가 웹 애플리케이션용 CloudFormation 템플릿을 생성하고 GIT에서 확인 및 검토할 수 있도록 <code>풀 요청</code>을 제기했습니다. 템플릿을 확인한 후 템플릿이 작동하지 않을 것이라고 즉시 말했습니다. 다음 중 이 CloudFormation 템플릿이 스택 배포에 실패하는 이유는 무엇입니까?{ ""AWSTemplateFormatVersion"" : ""2010-09-09"" , ""매개변수"" :{   ""VPCID"" :{     ""유형"" : ""문자열"" ,     ""설명"" : ""마닐라"" }, ""서브넷 아이디"" :{   ""유형"" : ""문자열"" ,   ""설명"" : ""서브넷-b46032ec"" }},""출력"" :{ ""인스턴스 아이디"" :{  ""가치"" :{   ""Ref"" : ""마닐라 인스턴스""  },  ""설명"" : ""인스턴스 ID""  } }}",섹션 이 Conditions없습니다.,의 값이 AWSTemplateFormatVersion올바르지 않습니다. 2017-06-06이어야 합니다.,이름이 잘못된 섹션이 Parameters있습니다. 이로 인해 CloudFormation 스택이 실패합니다.,섹션 이 Resources없습니다.,,,0,,
udemy,CLF-01,369,"A company plans to use a cloud storage service to temporarily store its log files. The number of files to be stored is still unknown, but it only needs to be kept for 12 hours.Which of the following is the most cost-effective storage class to use in this scenario?",B,B,Amazon S3 Glacier Deep Archive,Amazon S3 Standard,Amazon S3 Standard-IA,Amazon S3 One Zone-IA,,,,회사에서 로그 파일을 임시로 저장하기 위해 클라우드 스토리지 서비스를 사용할 계획입니다. 저장할 파일의 수는 아직 알 수 없지만 12시간 동안만 보관하면 됩니다.다음 중 이 시나리오에서 사용할 가장 비용 효율적인 스토리지 클래스는 무엇입니까?,Amazon S3 Glacier 딥 아카이브,아마존 S3 스탠다드,아마존 S3 스탠다드-IA,Amazon S3 One Zone-IA,,,0,,
udemy,CLF-01,370,"A Python application running on VMware Cloud on AWS must connect to an Amazon DynamoDB table called tutorialsdojo. Considering the principle of least privilege access, a solutions architect must form an IAM policy that allows the application to read, write, update and delete items from the tutorialsdojo table only.Which IAM policy would satisfy the requirements?",B,B,"{""Version"": ""2012-10-17"",""Statement"": [{""Effect"": ""Allow"",""Action"": [""dynamodb:Put*"",""dynamodb:Delete*"",""dynamodb:Get*"",""dynamodb:Update*""],""Resource"": ""arn:aws:dynamodb:us-east-2:123456789012:table/tutorialsdojo""}]}","{""Version"": ""2012-10-17"",""Statement"": [{""Effect"": ""Allow"",""Action"": [""dynamodb:PutItem"",""dynamodb:DeleteItem"",""dynamodb:GetItem"",""dynamodb:UpdateItem""],""Resource"": ""arn:aws:dynamodb:us-east-2:123456789012:table/tutorialsdojo""}]}","{""Version"": ""2012-10-17"",""Statement"": [{""Effect"": ""Allow"",""Action"": ""*"",""Resource"": ""arn:aws:dynamodb:us-east-2:123456789012:table/tutorialsdojo""}]}","{""Version"": ""2012-10-17"",""Statement"": [{""Effect"": ""Allow"",""Action"": [""dynamodb:PutItem"",""dynamodb:DeleteItem"",""dynamodb:GetItem"",""dynamodb:UpdateItem""],""Resource"": ""arn:aws:dynamodb:us-east-2:123456789012:table/*""}]}",,,,"VMware Cloud on AWS에서 실행되는 Python 애플리케이션은 라는 Amazon DynamoDB 테이블에 연결해야 합니다 tutorialsdojo. 최소 권한 액세스 원칙을 고려하여 솔루션 설계자는 애플리케이션이 테이블에서만 항목을 읽고, 쓰고, 업데이트하고, 삭제할 수 있도록 허용하는 IAM 정책을 구성해야 합니다 tutorialsdojo.어떤 IAM 정책이 요구 사항을 충족합니까?","{""버전"" : ""2012-10-17"" , ""진술서"" : [ {""효과"" : ""허용"" , ""액션"" : [ ""dynamodb:Put*"" ,""dynamodb:삭제*"" ,""dynamodb:Get*"" ,""dynamodb:업데이트*""],""리소스"" : ""arn:aws:dynamodb:us-east-2:123456789012:table/tutorialsdojo"" }]}","{""버전"" : ""2012-10-17"" , ""진술서"" : [ {""효과"" : ""허용"" , ""액션"" : [ ""dynamodb:PutItem"" ,""dynamodb:항목 삭제"" ,""dynamodb:GetItem"" ,""dynamodb:업데이트 항목""],""리소스"" : ""arn:aws:dynamodb:us-east-2:123456789012:table/tutorialsdojo"" }]}","{""버전"" : ""2012-10-17"" , ""진술서"" : [ {""효과"" : ""허용"" , ""액션"" : ""*"" , ""리소스"" : ""arn:aws:dynamodb:us-east-2:123456789012:table/tutorialsdojo"" }]}","{""버전"" : ""2012-10-17"" , ""진술서"" : [ {""효과"" : ""허용"" , ""액션"" : [ ""dynamodb:PutItem"" ,""dynamodb:항목 삭제"" ,""dynamodb:GetItem"" ,""dynamodb:업데이트 항목""],""리소스"" : ""arn:aws:dynamodb:us-east-2:123456789012:테이블/*"" }]}",,,0,,
udemy,CLF-01,371,A Solutions Architect needs to launch a web application that will be served globally using Amazon CloudFront. The application is hosted in an Amazon EC2 instance which will be configured as the origin server to process and serve dynamic content to its customers.Which of the following options provides high availability for the application?,C,C,Use Lambda@Edge to improve the performance of your web application and ensure high availability. Set the Lambda@Edge functions to be part of an origin group.,Launch an Auto Scaling group of EC2 instances and configure it to be part of an origin group.,Provision two EC2 instances deployed in different Availability Zones and configure them to be part of an origin group.,Use Amazon S3 to serve the dynamic content of your web application and configure the S3 bucket to be part of an origin group.,,,,Solutions Architect는 Amazon CloudFront를 사용하여 전 세계적으로 제공될 웹 애플리케이션을 시작해야 합니다. 애플리케이션은 고객에게 동적 콘텐츠를 처리하고 제공하기 위해 원본 서버로 구성되는 Amazon EC2 인스턴스에서 호스팅됩니다.다음 중 애플리케이션에 고가용성을 제공하는 옵션은 무엇입니까?,Lambda@Edge를 사용하여 웹 애플리케이션의 성능을 개선하고 고가용성을 보장하십시오. Lambda@Edge 함수를 오리진 그룹의 일부로 설정합니다.,EC2 인스턴스의 Auto Scaling 그룹을 시작하고 오리진 그룹의 일부가 되도록 구성합니다.,서로 다른 가용 영역에 배포된 두 개의 EC2 인스턴스를 프로비저닝하고 오리진 그룹의 일부가 되도록 구성합니다.,Amazon S3를 사용하여 웹 애플리케이션의 동적 콘텐츠를 제공하고 S3 버킷을 오리진 그룹의 일부로 구성합니다.,,,0,,
udemy,CLF-01,372,An intelligence agency is currently hosting a learning and training portal in AWS. Your manager instructed you to launch a large EC2 instance with an attached EBS Volume and enable Enhanced Networking. What are the valid case scenarios in using Enhanced Networking? (Select TWO.),BD,BD,When you need a low packet-per-second performance,When you need a consistently lower inter-instance latencies,When you need a dedicated connection to your on-premises data center,When you need a higher packet per second (PPS) performance,When you need high latency networking,,,정보 기관은 현재 AWS에서 학습 및 교육 포털을 호스팅하고 있습니다. 관리자가 EBS 볼륨이 연결된 대형 EC2 인스턴스를 시작하고 향상된 네트워킹을 활성화하라고 지시했습니다. 향상된 네트워킹을 사용하는 유효한 사례 시나리오는 무엇입니까? (2개를 선택하세요.),낮은 초당 패킷 성능이 필요한 경우,지속적으로 더 낮은 인스턴스 간 대기 시간이 필요한 경우,온프레미스 데이터 센터에 대한 전용 연결이 필요한 경우,더 높은 PPS(Packet Per Second) 성능이 필요한 경우,대기 시간이 긴 네트워킹이 필요한 경우,,0,,
udemy,CLF-01,373,"A global news network created a CloudFront distribution for their web application. However, you noticed that the application's origin server is being hit for each request instead of the AWS Edge locations, which serve the cached objects. The issue occurs even for the commonly requested objects.What could be a possible cause of this issue?",D,D,There are two primary origins configured in your Amazon CloudFront Origin Group.,"An object is only cached by Cloudfront once a successful request has been made hence, the objects were not requested before, which is why the request is still directed to the origin server.",The file sizes of the cached objects are too large for CloudFront to handle.,The Cache-Control max-age directive is set to zero.,,,,글로벌 뉴스 네트워크는 웹 애플리케이션용 CloudFront 배포를 생성했습니다. 그러나 캐시된 개체를 제공하는 AWS Edge 위치 대신 각 요청에 대해 애플리케이션의 원본 서버가 적중되고 있음을 확인했습니다. 이 문제는 일반적으로 요청된 개체에 대해서도 발생합니다.이 문제의 가능한 원인은 무엇입니까?,Amazon CloudFront 오리진 그룹에는 두 개의 기본 오리진이 구성되어 있습니다.,개체는 성공적인 요청이 이루어진 후에만 Cloudfront에 의해 캐시되므로 개체는 이전에 요청되지 않았으므로 요청이 여전히 원본 서버로 전달됩니다.,캐싱된 객체의 파일 크기가 너무 커서 CloudFront에서 처리할 수 없습니다.,Cache-Control max-age 지시어는 0으로 설정됩니다.,,,0,,
udemy,CLF-01,374,A company plans to host a movie streaming app in AWS. The chief information officer (CIO) wants to ensure that the application is highly available and scalable. The application is deployed to an Auto Scaling group of EC2 instances on multiple AZs. A load balancer must be configured to distribute incoming requests evenly to all EC2 instances across multiple Availability Zones.Which of the following features should the Solutions Architect use to satisfy these criteria?,B,B,AWS Direct Connect SiteLink,Cross-zone load balancing,Amazon VPC IP Address Manager (IPAM),Path-based Routing,,,,한 회사가 AWS에서 영화 스트리밍 앱을 호스팅할 계획입니다. CIO(Chief Information Officer)는 애플리케이션의 가용성과 확장성이 높기를 원합니다. 애플리케이션은 여러 AZ에 있는 EC2 인스턴스의 Auto Scaling 그룹에 배포됩니다. 들어오는 요청을 여러 가용 영역의 모든 EC2 인스턴스에 균등하게 분배하도록 로드 밸런서를 구성해야 합니다.다음 중 Solutions Architect가 이러한 기준을 충족하기 위해 사용해야 하는 기능은 무엇입니까?,AWS Direct Connect 사이트링크,교차 영역 로드 밸런싱,Amazon VPC IP 주소 관리자(IPAM),경로 기반 라우팅,,,0,,
udemy,CLF-01,375,A company deployed a web application to an EC2 instance that adds a variety of photo effects to a picture uploaded by the users. The application will put the generated photos to an S3 bucket by sending PUT requests to the S3 API.What is the best option for this scenario considering that you need to have API credentials to be able to send a request to the S3 API?,D,D,Encrypt the API credentials and store in any directory of the EC2 instance.,Store your API credentials in Amazon Glacier.,Store the API credentials in the root web application directory of the EC2 instance.,"Create a role in IAM. Afterwards, assign this role to a new EC2 instance.",,,,한 회사에서 사용자가 업로드한 사진에 다양한 사진 효과를 추가하는 웹 애플리케이션을 EC2 인스턴스에 배포했습니다. 애플리케이션은 S3 API에 PUT 요청을 전송하여 생성된 사진을 S3 버킷에 넣습니다.S3 API에 요청을 보내려면 API 자격 증명이 필요하다는 점을 고려할 때 이 시나리오에 가장 적합한 옵션은 무엇입니까?,API 자격 증명을 암호화하고 EC2 인스턴스의 디렉터리에 저장합니다.,API 자격 증명을 Amazon Glacier에 저장합니다.,EC2 인스턴스의 루트 웹 애플리케이션 디렉터리에 API 자격 증명을 저장합니다.,IAM에서 역할을 생성합니다. 그런 다음 이 역할을 새 EC2 인스턴스에 할당합니다.,,,0,,
udemy,CLF-01,376,A company is using an Amazon RDS for MySQL 5.6 with Multi-AZ deployment enabled and several web servers across two AWS Regions. The database is currently experiencing highly dynamic reads due to the growth of the company’s website. The Solutions Architect tried to test the read performance from the secondary AWS Region and noticed a notable slowdown on the SQL queries.Which of the following options would provide a read replication latency of less than 1 second?,C,C,Upgrade the MySQL database engine.,Create an Amazon RDS for MySQL read replica in the secondary AWS Region.,Migrate the existing database to Amazon Aurora and create a cross-region read replica.,Use Amazon ElastiCache to improve database performance.,,,,한 회사에서 다중 AZ 배포가 활성화된 MySQL 5.6용 Amazon RDS와 두 AWS 리전에서 여러 웹 서버를 사용하고 있습니다. 데이터베이스는 현재 회사 웹 사이트의 성장으로 인해 매우 동적인 읽기를 경험하고 있습니다. Solutions Architect는 보조 AWS 리전에서 읽기 성능을 테스트하려고 했고 SQL 쿼리에서 현저한 속도 저하를 발견했습니다.다음 중 읽기 복제 대기 시간이 1초 미만인 옵션은 무엇입니까?,MySQL 데이터베이스 엔진을 업그레이드합니다.,보조 AWS 리전에서 Amazon RDS for MySQL 읽기 전용 복제본을 생성합니다.,기존 데이터베이스를 Amazon Aurora로 마이그레이션하고 지역 간 읽기 전용 복제본을 생성합니다.,Amazon ElastiCache를 사용하여 데이터베이스 성능을 개선하십시오.,,,0,,
udemy,CLF-01,377,"A healthcare company has developed an AWS Lambda function to handle requests from a third-party analytics service. When new patient data is available, the service sends an HTTP POST request to a webhook intended to trigger the Lambda function.What would be the MOST operationally efficient solution to ensure that the service can call the Lambda function?",D,D,Attach an Amazon SQS queue to the Lambda function. Use the SQS queue URL as the webhook.,Launch an EC2 instance that acts as a proxy for the Lambda function. Give the instance’s public IP as a webhook to the third-party analytics service.,Create an API Gateway endpoint for the Lambda function. Provide the endpoint as a webhook to the third-party analytics service.,Generate a Lambda Function URL and use it as the webhook for the third-party analytics service.,,,,한 의료 회사가 타사 분석 서비스의 요청을 처리하기 위해 AWS Lambda 함수를 개발했습니다. 새 환자 데이터를 사용할 수 있는 경우 서비스는 HTTP POSTLambda 함수를 트리거하기 위한 웹후크에 요청을 보냅니다.서비스가 Lambda 함수를 호출할 수 있도록 보장하는 가장 효율적인 운영 솔루션은 무엇입니까?,Amazon SQS 대기열을 Lambda 함수에 연결합니다. SQS 대기열 URL을 웹훅으로 사용합니다.,Lambda 함수의 프록시 역할을 하는 EC2 인스턴스를 시작합니다. 인스턴스의 퍼블릭 IP를 타사 분석 서비스에 대한 웹훅으로 제공합니다.,Lambda 함수에 대한 API 게이트웨이 엔드포인트를 생성합니다. 엔드포인트를 타사 분석 서비스에 웹훅으로 제공합니다.,Lambda 함수 URL을 생성하고 이를 타사 분석 서비스의 웹훅으로 사용합니다.,,,0,,
udemy,CLF-01,378,"A tech company is running two production web servers hosted on Reserved EC2 instances with EBS-backed root volumes. These instances have a consistent CPU load of 90%. Traffic is being distributed to these instances by an Elastic Load Balancer. In addition, they also have Multi-AZ RDS MySQL databases for their production, test, and development environments.  What recommendation would you make to reduce cost in this AWS environment without affecting availability and performance of mission-critical systems? Choose the best answer.",B,B,Consider removing the Elastic Load Balancer,Consider not using a Multi-AZ RDS deployment for the development and test database,Consider using Spot instances instead of reserved EC2 instances,Consider using On-demand instances instead of Reserved EC2 instances,,,,"한 기술 회사는 EBS 지원 루트 볼륨이 있는 Reserved EC2 인스턴스에서 호스팅되는 두 개의 프로덕션 웹 서버를 실행하고 있습니다. 이러한 인스턴스의 CPU 로드는 90%입니다. 트래픽은 Elastic Load Balancer에 의해 이러한 인스턴스에 분산되고 있습니다. 또한 프로덕션, 테스트 및 개발 환경을 위한 다중 AZ RDS MySQL 데이터베이스도 있습니다.  미션 크리티컬 시스템의 가용성과 성능에 영향을 주지 않고 이 AWS 환경에서 비용을 줄이기 위해 어떤 권장 사항을 만드시겠습니까? 가장 좋은 답을 선택하세요.",Elastic Load Balancer 제거 고려,개발 및 테스트 데이터베이스에 다중 AZ RDS 배포를 사용하지 않는 것이 좋습니다.,예약된 EC2 인스턴스 대신 스팟 인스턴스 사용 고려,예약 EC2 인스턴스 대신 온디맨드 인스턴스 사용 고려,,,0,,
udemy,CLF-01,379,A company requires corporate IT governance and cost oversight of all of its AWS resources across its divisions around the world. Their corporate divisions want to maintain administrative control of the discrete AWS resources they consume and ensure that those resources are separate from other divisions.Which of the following options will support the autonomy of each corporate division while enabling the corporate IT to maintain governance and cost oversight? (Select TWO.),BC,BC,Create separate VPCs for each division within the corporate IT AWS account. Launch an AWS Transit Gateway with equal-cost multipath routing (ECMP) and VPN tunnels for intra-VPC communication.,Enable IAM cross-account access for all corporate IT administrators in each child account.,Use AWS Consolidated Billing by creating AWS Organizations to link the divisions’ accounts to a parent corporate account.,Use AWS Trusted Advisor and AWS Resource Groups Tag Editor,Create separate Availability Zones for each division within the corporate IT AWS account. Improve communication between the two AZs using the AWS Global Accelerator.,,,회사는 전 세계 부서의 모든 AWS 리소스에 대한 기업 IT 거버넌스와 비용 감독이 필요합니다. 기업 부서는 소비하는 개별 AWS 리소스에 대한 관리 제어를 유지하고 해당 리소스가 다른 부서와 분리되도록 하기를 원합니다.다음 중 기업 IT가 거버넌스 및 비용 감독을 유지하면서 각 기업 부서의 자율성을 지원하는 옵션은 무엇입니까? (2개를 선택하세요.),회사 IT AWS 계정 내의 각 부서에 대해 별도의 VPC를 생성합니다. ECMP(등가 다중 경로 라우팅) 및 VPC 내 통신을 위한 VPN 터널을 사용하여 AWS Transit Gateway를 시작합니다.,각 하위 계정의 모든 회사 IT 관리자에 대해 IAM 교차 계정 액세스를 활성화합니다.,부서의 계정을 상위 회사 계정에 연결하는 AWS Organizations를 생성하여 AWS Consolidated Billing을 사용합니다.,AWS Trusted Advisor 및 AWS 리소스 그룹 태그 편집기 사용,회사 IT AWS 계정 내의 각 부서에 대해 별도의 가용 영역을 만듭니다. AWS Global Accelerator를 사용하여 두 AZ 간의 통신을 개선합니다.,,0,,
udemy,CLF-01,380,"A company has a fleet of running Spot EC2 instances behind an Application Load Balancer. The incoming traffic comes from various users across multiple AWS regions, and you would like to have the user's session shared among the fleet of instances.A Solutions Architect is required to set up a distributed session management layer that will provide scalable and shared data storage for the user sessions that supports multithreaded performance. The cache layer must also detect any node failures and replace the failed ones automatically.Which of the following would be the best choice to meet the requirement while still providing sub-millisecond latency for the users?",C,C,Amazon ElastiCache for Redis Global Datastore,AWS ELB sticky sessions,Amazon ElastiCache for Memcached with Auto Discovery,Amazon RDS database with RDS Proxy,,,,회사에는 Application Load Balancer 뒤에서 Spot EC2 인스턴스를 실행하는 플릿이 있습니다. 수신 트래픽은 여러 AWS 지역의 다양한 사용자로부터 발생하며 사용자의 세션을 여러 인스턴스 간에 공유하려고 합니다.멀티스레드 성능을 지원하는 사용자 세션에 대해 확장 가능한 공유 데이터 저장소를 제공할 분산 세션 관리 계층을 설정하려면 Solutions Architect가 필요합니다. 캐시 계층은 또한 모든 노드 장애를 감지하고 실패한 노드를 자동으로 교체해야 합니다.다음 중 사용자에게 1밀리초 미만의 대기 시간을 제공하면서 요구 사항을 충족하는 가장 좋은 선택은 무엇입니까?,Redis 글로벌 데이터 스토어용 Amazon ElastiCache,AWS ELB 고정 세션,자동 검색 기능이 있는 Memcached용 Amazon ElastiCache,RDS Proxy를 사용하는 Amazon RDS 데이터베이스,,,0,,
udemy,CLF-01,381,A software development company has hundreds of Amazon EC2 instances with multiple Application Load Balancers (ALBs) across multiple AWS Regions. The public applications hosted in their EC2 instances are accessed on their on-premises network. The company needs to reduce the number of IP addresses that it needs to regularly whitelist on the corporate firewall device.Which of the following approach can be used to fulfill this requirement?,A,A,Use AWS Global Accelerator and create an endpoint group for each AWS Region. Associate the Application Load Balancer from each region to the corresponding endpoint group.,Launch a Network Load Balancer with an associated Elastic IP address. Set the ALBs in multiple Regions as targets.,Use AWS Global Accelerator and create multiple endpoints for all the available AWS Regions. Associate all the private IP addresses of the EC2 instances to the corresponding endpoints.,Create a new Lambda function that tracks the changes in the IP addresses of all ALBs across multiple AWS Regions. Schedule the function to run and update the corporate firewall every hour using Amazon CloudWatch Events.,,,,소프트웨어 개발 회사에는 여러 AWS 리전에 걸쳐 여러 ALB(Application Load Balancer)가 있는 수백 개의 Amazon EC2 인스턴스가 있습니다. EC2 인스턴스에서 호스팅되는 공용 애플리케이션은 온프레미스 네트워크에서 액세스됩니다. 회사는 회사 방화벽 장치에서 정기적으로 화이트리스트에 추가해야 하는 IP 주소의 수를 줄여야 합니다.다음 중 이 요구 사항을 충족하는 데 사용할 수 있는 방법은 무엇입니까?,AWS Global Accelerator를 사용하고 각 AWS 리전에 대한 엔드포인트 그룹을 생성합니다. 각 리전의 Application Load Balancer를 해당 엔드포인트 그룹에 연결합니다.,연결된 탄력적 IP 주소로 Network Load Balancer를 시작합니다. 여러 지역의 ALB를 대상으로 설정합니다.,AWS Global Accelerator를 사용하고 사용 가능한 모든 AWS 리전에 대해 여러 엔드포인트를 생성합니다. EC2 인스턴스의 모든 프라이빗 IP 주소를 해당 엔드포인트에 연결합니다.,여러 AWS 리전에서 모든 ALB의 IP 주소 변경 사항을 추적하는 새 Lambda 함수를 생성합니다. Amazon CloudWatch Events를 사용하여 매시간 회사 방화벽을 실행하고 업데이트하도록 기능을 예약합니다.,,,0,,
udemy,CLF-01,382,A company plans to launch an application that tracks the GPS coordinates of delivery trucks in the country. The coordinates are transmitted from each delivery truck every five seconds. You need to design an architecture that will enable real-time processing of these coordinates from multiple consumers. The aggregated data will be analyzed in a separate reporting application.Which AWS service should you use for this scenario?,C,C,Amazon Simple Queue Service,AWS Data Pipeline,Amazon Kinesis,Amazon AppStream,,,,한 회사가 국내 배달 트럭의 GPS 좌표를 추적하는 애플리케이션을 출시할 계획입니다. 좌표는 각 배달 트럭에서 5초마다 전송됩니다. 여러 소비자로부터 이러한 좌표를 실시간으로 처리할 수 있는 아키텍처를 설계해야 합니다. 집계된 데이터는 별도의 보고 애플리케이션에서 분석됩니다.이 시나리오에서는 어떤 AWS 서비스를 사용해야 합니까?,Amazon 단순 대기열 서비스,AWS 데이터 파이프라인,아마존 키네시스,아마존 앱스트림,,,0,,
udemy,CLF-01,383,A company deployed a fleet of Windows-based EC2 instances with IPv4 addresses launched in a private subnet. Several software installed in the EC2 instances are required to be updated via the Internet.Which of the following services can provide the firm a highly available solution to safely allow the instances to fetch the software patches from the Internet but prevent outside network from initiating a connection?,D,D,NAT Instance,VPC Endpoint,Egress-Only Internet Gateway,NAT Gateway,,,,한 회사에서 프라이빗 서브넷에서 시작된 IPv4 주소를 사용하여 Windows 기반 EC2 인스턴스 플릿을 배포했습니다. EC2 인스턴스에 설치된 여러 소프트웨어는 인터넷을 통해 업데이트해야 합니다.다음 중 인스턴스가 인터넷에서 소프트웨어 패치를 안전하게 가져오도록 허용하지만 외부 네트워크가 연결을 시작하지 못하도록 하는 고가용성 솔루션을 회사에 제공할 수 있는 서비스는 무엇입니까?,NAT 인스턴스,VPC 엔드포인트,외부 전용 인터넷 게이트웨이,NAT 게이트웨이,,,0,,
udemy,CLF-01,384,"An airline company receives a lot of requests to book flights, update booking details, and flight check-ins. Since these requests flood the customer support teams, the management wants to build a self-service solution that can handle these requests without a human agent. This solution should be text-based wherein users can type their concerns in a chat box and an AI will analyze their intention, provide answers, or fulfill pre-defined actions automatically.Which of the following options is the recommended solution for the above requirements?",C,C,Deploy a conversational chatbot using Amazon Rekognition. Define conversation flow for specific user intentions. Create AWS Lambda functions that can be invoked depending on user intentions.,"Create a conversational chatbot using Amazon Comprehend for natural-language processing (NLU). Depending on the user’s intent, invoke AWS Lambda functions that can perform the needed actions.",Deploy a conversational chatbot using Amazon Lex. Define conversation flow for specific user intentions. Integrate AWS Lambda functions as code hooks to perform actions based on user requests.,Work with an AWS Managed Service Provider (MSP) to deploy a conversational chatbot using Amazon Polly for natural-language processing (NLU). Integrate AWS Lambda functions as code hooks to perform actions based on user requests.,,,,"항공사는 항공편 예약, 예약 세부 정보 업데이트 및 항공편 체크인에 대한 많은 요청을 받습니다. 이러한 요청이 고객 지원 팀에 넘쳐나기 때문에 경영진은 상담원 없이 이러한 요청을 처리할 수 있는 셀프 서비스 솔루션을 구축하고자 합니다. 이 솔루션은 사용자가 채팅 상자에 우려 사항을 입력할 수 있고 AI가 의도를 분석하고 답변을 제공하거나 미리 정의된 작업을 자동으로 수행하는 텍스트 기반이어야 합니다.다음 옵션 중 위의 요구 사항에 권장되는 솔루션은 무엇입니까?",Amazon Rekognition을 사용하여 대화형 챗봇을 배포합니다. 특정 사용자 의도에 대한 대화 흐름을 정의합니다. 사용자 의도에 따라 호출할 수 있는 AWS Lambda 함수를 생성합니다.,NLU(자연어 처리)를 위해 Amazon Comprehend를 사용하여 대화형 챗봇을 생성합니다. 사용자의 의도에 따라 필요한 작업을 수행할 수 있는 AWS Lambda 함수를 호출합니다.,Amazon Lex를 사용하여 대화형 챗봇을 배포합니다. 특정 사용자 의도에 대한 대화 흐름을 정의합니다. AWS Lambda 함수를 코드 후크로 통합하여 사용자 요청에 따라 작업을 수행합니다.,AWS 관리형 서비스 공급자(MSP)와 협력하여 NLU(자연어 처리)를 위해 Amazon Polly를 사용하여 대화형 챗봇을 배포합니다. AWS Lambda 함수를 코드 후크로 통합하여 사용자 요청에 따라 작업을 수행합니다.,,,0,,
udemy,CLF-01,385,"A Solutions Architect created a brand new IAM User with a default setting using AWS CLI. This is intended to be used to send API requests to Amazon S3, DynamoDB, Lambda, and other AWS resources of the company’s cloud infrastructure.Which of the following must be done to allow the user to make API calls to the AWS resources?",B,B,Do nothing as the IAM User is already capable of sending API calls to your AWS resources.,Create a set of Access Keys for the user and attach the necessary permissions.,Assign an IAM Policy to the user to allow it to send API calls.,Enable Multi-Factor Authentication for the user.,,,,"Solutions Architect는 AWS CLI를 사용하여 기본 설정으로 완전히 새로운 IAM 사용자를 생성했습니다. 이는 회사 클라우드 인프라의 Amazon S3, DynamoDB, Lambda 및 기타 AWS 리소스에 API 요청을 보내는 데 사용하기 위한 것입니다.사용자가 AWS 리소스에 대한 API 호출을 허용하려면 다음 중 무엇을 수행해야 합니까?",IAM 사용자가 이미 AWS 리소스에 API 호출을 보낼 수 있으므로 아무 작업도 수행하지 마십시오.,사용자에 대한 액세스 키 세트를 생성하고 필요한 권한을 연결합니다.,API 호출을 보낼 수 있도록 사용자에게 IAM 정책을 할당합니다.,사용자에 대해 Multi-Factor Authentication을 활성화합니다.,,,0,,
udemy,CLF-01,386,"A company plans to implement a network monitoring system in AWS. The Solutions Architect launched an EC2 instance to host the monitoring system and used CloudWatch to monitor, store, and access the log files of the instance.Which of the following provides an automated way to send log data to CloudWatch Logs from the Amazon EC2 instance?",A,A,CloudWatch Logs agent,AWS Transfer for SFTP,CloudTrail Processing Library,CloudTrail with log file validation,,,,"회사는 AWS에서 네트워크 모니터링 시스템을 구현할 계획입니다. Solutions Architect는 EC2 인스턴스를 시작하여 모니터링 시스템을 호스팅하고 CloudWatch를 사용하여 인스턴스의 로그 파일을 모니터링, 저장 및 액세스했습니다.다음 중 Amazon EC2 인스턴스에서 CloudWatch Logs로 로그 데이터를 전송하는 자동화된 방법을 제공하는 것은 무엇입니까?",CloudWatch Logs 에이전트,SFTP용 AWS 전송,CloudTrail 처리 라이브러리,로그 파일 검증이 포함된 CloudTrail,,,0,,
udemy,CLF-01,387,"An application is loading hundreds of JSON documents into an Amazon S3 bucket every hour which is registered in AWS Lake Formation as a data catalog. The Data Analytics team uses Amazon Athena to run analyses on these data, but due to the volume, most queries take a long time to complete.What change should be made to improve the query performance while ensuring data security?",C,C,Compress the data into GZIP format before storing it in the S3 bucket. Apply an IAM policy with aws:SourceArn and aws:SourceAccount global condition context keys in Lake Formation that prevents cross-service confused deputy problems and other security issues.,Apply minification on the data and implement the Lake Formation tag-based access control (LF-TBAC) authorization strategy to ensure security.,Transform the JSON data into Apache Parquet format. Ensure that the user has an lakeformation:GetDataAccess IAM permission for underlying data access control.,Convert the JSON documents into CSV format. Provide fine-grained named resource access control to specific databases or tables in AWS Lake Formation.,,,,애플리케이션은 AWS Lake Formation에 데이터 카탈로그로 등록된 Amazon S3 버킷에 매시간 수백 개의 JSON 문서를 로드하고 있습니다. 데이터 분석 팀은 Amazon Athena를 사용하여 이러한 데이터에 대한 분석을 실행하지만 볼륨으로 인해 대부분의 쿼리를 완료하는 데 오랜 시간이 걸립니다.데이터 보안을 보장하면서 쿼리 성능을 향상시키려면 어떤 변경이 필요합니까?,데이터를 S3 버킷에 저장하기 전에 GZIP 형식으로 압축합니다. 교차 서비스 혼동 대리인 문제 및 기타 보안 문제를 방지하는 Lake Formation의 글로벌 조건 컨텍스트 키와 aws:SourceArn함께 IAM 정책을 적용합니다 .aws:SourceAccount,데이터에 축소를 적용하고 Lake Formation 태그 기반 액세스 제어(LF-TBAC) 권한 부여 전략을 구현하여 보안을 보장합니다.,JSON 데이터를 Apache Parquet 형식으로 변환합니다. 사용자에게 lakeformation:GetDataAccess기본 데이터 액세스 제어에 대한 IAM 권한이 있는지 확인하십시오.,JSON 문서를 CSV 형식으로 변환합니다. AWS Lake Formation의 특정 데이터베이스 또는 테이블에 세분화된 명명된 리소스 액세스 제어를 제공합니다.,,,0,,
udemy,CLF-01,388,"A web application, which is hosted in your on-premises data center and uses a MySQL database, must be migrated to AWS Cloud. You need to ensure that the network traffic to and from your RDS database instance is encrypted using SSL. For improved security, you have to use the profile credentials specific to your EC2 instance to access your database, instead of a password.   Which of the following should you do to meet the above requirement?",C,C,Launch a new RDS database instance with the Backtrack feature enabled.,Launch the mysql client using the --ssl-ca parameter when connecting to the database.,Set up an RDS database and enable the IAM DB Authentication.,Configure your RDS database to enable encryption.,,,,온프레미스 데이터 센터에서 호스팅되고 MySQL 데이터베이스를 사용하는 웹 애플리케이션은 AWS 클라우드로 마이그레이션해야 합니다. RDS 데이터베이스 인스턴스로 들어오고 나가는 네트워크 트래픽이 SSL을 사용하여 암호화되었는지 확인해야 합니다. 보안을 강화하려면 암호 대신 EC2 인스턴스에 특정한 프로필 자격 증명을 사용하여 데이터베이스에 액세스해야 합니다.   위의 요구 사항을 충족하려면 다음 중 무엇을 해야 합니까?,역추적 기능이 활성화된 새 RDS 데이터베이스 인스턴스를 시작합니다.,--ssl-ca데이터베이스에 연결할 때 매개 변수를 사용하여 mysql 클라이언트를 시작합니다 .,RDS 데이터베이스를 설정하고 IAM DB 인증을 활성화합니다.,암호화를 활성화하도록 RDS 데이터베이스를 구성합니다.,,,0,,
udemy,CLF-01,389,"An application is using a Lambda function to process complex financial data that run for 15 minutes on average. Most invocations were successfully processed. However, you noticed that there are a few terminated invocations throughout the day, which caused data discrepancy in the application. Which of the following is the most likely cause of this issue?",D,D,The failed Lambda Invocations contain a ServiceException error which means that the AWS Lambda service encountered an internal error.,The Lambda function contains a recursive code and has been running for over 15 minutes.,The concurrent execution limit has been reached.,The failed Lambda functions have been running for over 15 minutes and reached the maximum execution time.,,,,애플리케이션은 Lambda 함수를 사용하여  평균 15분 동안 실행되는 복잡한 재무 데이터를 처리합니다. 대부분의 호출이 성공적으로 처리되었습니다. 그러나 하루 종일 종료된 호출이 몇 개 있어 애플리케이션에서 데이터 불일치가 발생했음을 알았습니다.다음 중 이 문제의 원인으로 가장 가능성이 높은 것은 무엇입니까?,실패한 Lambda 호출에는 ServiceExceptionAWS Lambda 서비스에 내부 오류가 발생했음을 의미하는 오류가 포함되어 있습니다.,Lambda 함수에는 재귀 코드가 포함되어 있으며 15분 이상 실행되었습니다.,동시 실행 제한에 도달했습니다.,실패한 Lambda 함수가 15분 이상 실행되어 최대 실행 시간에 도달했습니다.,,,0,,
udemy,CLF-01,390,"A company is planning to deploy a High Performance Computing (HPC) cluster in its VPC that requires a scalable, high-performance file system. The storage service must be optimized for efficient workload processing, and the data must be accessible via a fast and scalable file system interface. It should also work natively with Amazon S3 that enables you to easily process your S3 data with a high-performance POSIX interface. Which of the following is the MOST suitable service that you should use for this scenario?",B,B,Amazon Elastic File System (EFS),Amazon FSx for Lustre,Amazon Elastic Block Storage (EBS),Amazon FSx for Windows File Server,,,,회사는 확장 가능한 고성능 파일 시스템이 필요한 VPC에 HPC(고성능 컴퓨팅) 클러스터를 배포할 계획입니다. 스토리지 서비스는 효율적인 워크로드 처리를 위해 최적화되어야 하며 빠르고 확장 가능한 파일 시스템 인터페이스를 통해 데이터에 액세스할 수 있어야 합니다. 또한 고성능 POSIX 인터페이스로 S3 데이터를 쉽게 처리할 수 있는 Amazon S3와 기본적으로 작동해야 합니다.다음 중 이 시나리오에 사용해야 하는 가장 적합한 서비스는 무엇입니까?,Amazon 탄력적 파일 시스템(EFS),Lustre용 Amazon FSx,Amazon 탄력적 블록 스토리지(EBS),Windows 파일 서버용 Amazon FSx,,,0,,
