exam_category,exam_type,quiz_id,en_quiz_title,most_vote,correct,en_A,en_B,en_C,en_D,en_E,en_F,en_G,ko_quiz_title,ko_A,ko_B,ko_C,ko_D,ko_E,ko_G,subject,choice_count,ko_F
udemy,DVA-02,66,"A company selling smart security cameras uses an S3 bucket behind a CloudFront web distribution to store its static content, which it shares with customers worldwide. The company has recently released a new firmware update intended only for its premium customers, and unauthorized access should be denied with a user authentication process that has minimal latency.How can a developer refactor the current setup to achieve this requirement with the MOST efficient solution?",A,A,Use Lambda@Edge and Amazon Cognito to authenticate and authorize premium customers to download the firmware update.,Restrict access to the S3 bucket only to premium customers using an Origin Access Control (OAC).,Use Signed URLs and Signed Cookies in CloudFront to distribute the firmware update file.,Use the AWS Serverless Application Model (AWS SAM) and Amazon Cognito to authenticate the premium customers.,,,,"스마트 보안 카메라를 판매하는 회사는 CloudFront 웹 배포 뒤의 S3 버킷을 사용하여 전 세계 고객과 공유하는 정적 콘텐츠를 저장합니다. 회사는 최근 프리미엄 고객만을 대상으로 한 새로운 펌웨어 업데이트를 출시했으며, 대기 시간을 최소화하는 사용자 인증 프로세스를 통해 무단 액세스를 거부해야 합니다.개발자는 가장 효율적인 솔루션으로 이 요구 사항을 달성하기 위해 현재 설정을 어떻게 리팩터링할 수 있습니까?",Lambda@Edge 및 Amazon Cognito를 사용하여 프리미엄 고객이 펌웨어 업데이트를 다운로드하도록 인증하고 권한을 부여합니다.,OAC(Origin Access Control)를 사용하여 S3 버킷에 대한 액세스를 프리미엄 고객으로 제한합니다.,CloudFront에서 서명된 URL과 서명된 쿠키를 사용하여 펌웨어 업데이트 파일을 배포합니다.,AWS Serverless Application Model(AWS SAM)과 Amazon Cognito를 사용하여 프리미엄 고객을 인증합니다.,,,0,,
udemy,DVA-02,67,"A DynamoDB table has several top-level attributes such as id, course_id, course_title, price, rating and many others. The database queries of your application returns all of the item attributes by default but you only want to fetch specific attributes such as the course_id and price per request. As the developer, how can you refactor your application to accomplish this requirement?",B,B,Use filter expressions,Use projection expression,Use condition expressions,Use expression attribute names,,,,"idDynamoDB 테이블에는 , course_id, course_title, price및 기타 여러 가지 최상위 속성이 있습니다 rating. course_id애플리케이션의 데이터베이스 쿼리는 기본적으로 모든 항목 속성을 반환하지만 요청별로 및 같은 특정 속성만 가져오려고 합니다 price.개발자로서 이 요구 사항을 달성하기 위해 애플리케이션을 어떻게 리팩터링할 수 있습니까?",필터 표현식 사용,투영 표현 사용,조건 표현식 사용,표현식 속성 이름 사용,,,0,,
udemy,DVA-02,68,"Your application is processing one Kinesis data stream which has four shards, and each instance has one KCL worker. To scale up processing in your application, you reshard your stream to increase the number of open shards to six. What is the MAXIMUM number of EC2 instances that you should launch to achieve optimum performance?",D,D,3,5,12,6,,,,애플리케이션은 4개의 샤드가 있는 하나의 Kinesis 데이터 스트림을 처리하고 있으며 각 인스턴스에는 하나의 KCL 작업자가 있습니다. 애플리케이션에서 처리를 확장하려면 스트림을 다시 샤딩하여 열려 있는 샤드 수를 6개로 늘립니다.최적의 성능을 달성하려면 최대 몇 개의 EC2 인스턴스를 시작해야 합니까?,삼,5,12,6,,,0,,
udemy,DVA-02,69,"A web application is uploading large files, which are over 4 GB in size, in an S3 bucket called data.tutorialsdojo.com every 30 minutes. You want to minimize the time required to upload each file. Which of the following should you do to minimize upload time?",B,B,Enable Transfer Acceleration in the bucket.,Use the Multipart upload API.,Use the Putltem API.,Use the BatchWriteItem API.,,,,웹 애플리케이션은 30분마다 data.tutorialsdojo.com이라는 S3 버킷에 4GB가 넘는 대용량 파일을 업로드하고 있습니다. 각 파일을 업로드하는 데 필요한 시간을 최소화하려고 합니다.다음 중 업로드 시간을 최소화하려면 무엇을 해야 합니까?,버킷에서 Transfer Acceleration을 활성화합니다.,멀티파트 업로드 API를 사용하세요.,Putltem API를 사용하세요.,BatchWriteItem API를 사용합니다.,,,0,,
udemy,DVA-02,70,"A developer is moving a legacy web application from their on-premises data center to AWS. The application is used simultaneously by thousands of users, and their session states are stored in memory. The on-premises server usually reaches 100% CPU Utilization every time there is a surge in the number of people accessing the application.Which of the following is the best way to re-factor the performance and availability of the application's session management once it is migrated to AWS?",D,D,Use Sticky Sessions with Local Session Caching.,Use an ElastiCache for Memcached cluster to store the user session state of the application.,Store the user session state of the application using CloudFront.,Use an ElastiCache for Redis cluster to store the user session state of the application.,,,,개발자가 기존 웹 애플리케이션을 온프레미스 데이터 센터에서 AWS로 이동하고 있습니다. 수천 명의 사용자가 동시에 애플리케이션을 사용하고 해당 세션 상태는 메모리에 저장됩니다. 온프레미스 서버는 일반적으로 애플리케이션에 액세스하는 사람 수가 급증할 때마다 CPU 사용률 100%에 도달합니다.다음 중 AWS로 마이그레이션한 후 애플리케이션 세션 관리의 성능과 가용성을 리팩터링하는 가장 좋은 방법은 무엇입니까?,로컬 세션 캐싱과 함께 고정 세션을 사용합니다.,Memcached 클러스터용 ElastiCache를 사용하여 애플리케이션의 사용자 세션 상태를 저장합니다.,CloudFront를 사용하여 애플리케이션의 사용자 세션 상태를 저장합니다.,Redis용 ElastiCache 클러스터를 사용하여 애플리케이션의 사용자 세션 상태를 저장합니다.,,,0,,
udemy,DVA-02,71,"An application hosted in an Auto Scaling group of On-Demand EC2 instances is used to process data polled from an SQS queue and the generated output is stored in an S3 bucket. To improve security, you were tasked to ensure that all objects in the S3 bucket are encrypted at rest using server-side encryption with AWS KMS–Managed Keys (SSE-KMS). Which of the following is required to properly implement this requirement?",A,A,Add a bucket policy which denies any s3:PutObject action unless the request includes the x-amz-server-side-encryption header.,Add a bucket policy which denies any s3:PostObject action unless the request includes the x-amz-server-side-encryption header.,Add a bucket policy which denies any s3:PostObject action unless the request includes the x-amz-server-side-encryption-aws-kms-key-id header.,Add a bucket policy which denies any s3:PutObject action unless the request includes the x-amz-server-side-encryption-aws-kms-key-id header.,,,,온디맨드 EC2 인스턴스의 Auto Scaling 그룹에 호스팅된 애플리케이션은 SQS 대기열에서 폴링된 데이터를 처리하는 데 사용되며 생성된 출력은 S3 버킷에 저장됩니다. 보안을 강화하기 위해 SSE-KMS(AWS KMS 관리형 키)를 사용한 서버 측 암호화를 사용하여 S3 버킷의 모든 객체가 저장 상태에서 암호화되도록 해야 합니다.다음 중 이 요구 사항을 올바르게 구현하려면 어떤 것이 필요합니까?,s3:PutObject요청에 헤더가 포함되지 않는 한 모든 작업을 거부하는 버킷 정책을 추가합니다 x-amz-server-side-encryption.,s3:PostObject요청에 헤더가 포함되지 않는 한 모든 작업을 거부하는 버킷 정책을 추가합니다 x-amz-server-side-encryption.,s3:PostObject요청에 헤더가 포함되지 않는 한 모든 작업을 거부하는 버킷 정책을 추가합니다 x-amz-server-side-encryption-aws-kms-key-id.,s3:PutObject요청에 헤더가 포함되지 않는 한 모든 작업을 거부하는 버킷 정책을 추가합니다 x-amz-server-side-encryption-aws-kms-key-id.,,,0,,
udemy,DVA-02,72,"You are building a distributed system using KMS where you need to encrypt data at a later time. An API must be called that returns only the encrypted copy of the data key which you will use for encryption. After an hour, you will decrypt the data key by calling the Decrypt API then using the returned plaintext data key to finally encrypt the data. Which is the MOST suitable KMS API that the system should use to securely implement the requirements described above?",A,A,GenerateDataKeyWithoutPlaintext,GenerateDataKey,Encrypt,GenerateRandom,,,,나중에 데이터를 암호화해야 하는 KMS를 사용하여 분산 시스템을 구축하고 있습니다. 암호화에 사용할 데이터 키의 암호화된 복사본만 반환하는 API를 호출해야 합니다. 한 시간 후에 Decrypt API를 호출한 다음 반환된 일반 텍스트 데이터 키를 사용하여 최종적으로 데이터를 암호화함으로써 데이터 키를 해독합니다.위에 설명된 요구 사항을 안전하게 구현하기 위해 시스템이 사용해야 하는 가장 적합한 KMS API는 무엇입니까?,GenerateDataKeyWithoutPlaintext,GenerateDataKey,Encrypt,GenerateRandom,,,0,,
udemy,DVA-02,73,"The users of a social media website must be authenticated using social identity providers such as Twitter, Facebook, and Google. Users can login to the site which will allow them to upload their selfies, memes, and other media files in an S3 bucket. As an additional feature, you should also enable guest user access to certain sections of the website. Which of the following should you do to accomplish this task?",B,B,Integrate AWS Single Sign-On with your website.,Create an Identity Pool in Amazon Cognito and enable access to unauthenticated identities.,Create a User Pool in Amazon Cognito and enable access to unauthenticated identities.,Create a custom identity broker which integrates with the AWS Security Token Service and supports unauthenticated access.,,,,"소셜 미디어 웹사이트의 사용자는 Twitter, Facebook, Google과 같은 소셜 ID 공급자를 통해 인증을 받아야 합니다. 사용자는 사이트에 로그인하여 셀카, 밈 및 기타 미디어 파일을 S3 버킷에 업로드할 수 있습니다. 추가 기능으로 웹 사이트의 특정 섹션에 대한 게스트 사용자 액세스도 활성화해야 합니다.이 작업을 수행하려면 다음 중 무엇을 해야 합니까?",AWS Single Sign-On을 웹 사이트와 통합하세요.,Amazon Cognito에서 자격 증명 풀을 생성하고 인증되지 않은 자격 증명에 대한 액세스를 활성화합니다.,Amazon Cognito에서 사용자 풀을 생성하고 인증되지 않은 자격 증명에 대한 액세스를 활성화합니다.,AWS Security Token Service와 통합되고 인증되지 않은 액세스를 지원하는 사용자 지정 자격 증명 브로커를 생성합니다.,,,0,,
udemy,DVA-02,74,"A developer has deployed a Lambda function that runs in DEV, UAT, and PROD environments. The function uses different parameters that varies based on the environment it is running in. The parameters are currently hardcoded in the function.Which action should the developer do to reference the appropriate parameters without modifying the code every time the environment changes?",D,D,"Publish three versions of the Lambda function. Assign the aliases DEV, UAT, and PROD to each version.",Create individual Lambda Layers for each environment,Create a stage variable called ENV and invoke the Lambda function by its alias name.,Use environment variables to set the parameters per environment.,,,,"개발자가 DEV, UAT 및 PROD 환경에서 실행되는 Lambda 함수를 배포했습니다. 이 함수는 실행 중인 환경에 따라 달라지는 다양한 매개변수를 사용합니다. 매개변수는 현재 함수에 하드코딩되어 있습니다.환경이 바뀔 때마다 코드를 수정하지 않고 적절한 매개변수를 참조하려면 개발자는 어떤 조치를 취해야 할까요?","세 가지 버전의 Lambda 함수를 게시합니다. DEV, UAT 및 PROD 별칭을 각 버전에 할당합니다.",각 환경에 대한 개별 Lambda 레이어 생성,라는 단계 변수를 생성 ENV하고 별칭 이름으로 Lambda 함수를 호출합니다.,환경 변수를 사용하여 환경별로 매개변수를 설정합니다.,,,0,,
udemy,DVA-02,75,An application performs various workflows and processes long-running tasks that take a long time to complete. The users are complaining that the application is unresponsive since the workflow substantially increased the time it takes to complete a user request. Which of the following is the BEST way to improve the performance of the application?,B,B,Use a multicontainer docker environment in Elastic Beanstalk to process the long-running tasks asynchronously.,Use an Elastic Beanstalk worker environment to process the tasks asynchronously.,Use an Amazon ECS Cluster with a Fargate launch type to process the tasks asynchronously.,Spawn a worker process locally in the EC2 instances and process the tasks asynchronously.,,,,애플리케이션은 다양한 워크플로를 수행하고 완료하는 데 오랜 시간이 걸리는 장기 실행 작업을 처리합니다. 워크플로우로 인해 사용자 요청을 완료하는 데 걸리는 시간이 크게 늘어났기 때문에 사용자는 애플리케이션이 응답하지 않는다고 불평하고 있습니다.다음 중 애플리케이션 성능을 향상시키는 가장 좋은 방법은 무엇입니까?,Elastic Beanstalk의 멀티컨테이너 Docker 환경을 사용하여 장기 실행 작업을 비동기식으로 처리하세요.,Elastic Beanstalk 작업자 환경을 사용하여 작업을 비동기식으로 처리합니다.,Fargate 시작 유형과 함께 Amazon ECS 클러스터를 사용하여 작업을 비동기식으로 처리합니다.,EC2 인스턴스에서 로컬로 작업자 프로세스를 생성하고 작업을 비동기식으로 처리합니다.,,,0,,
udemy,DVA-02,76,"A developer is managing a distributed system that consists of an Application Load Balancer, an SQS queue, and an Auto Scaling group of EC2 instances. The system has been integrated with CloudFront to better serve clients worldwide. To enhance the security of the in-flight data, the developer was instructed to establish an end-to-end SSL connection between the origin and the end-users.Which TWO options will allow the developer to meet this requirement using CloudFront? (Select TWO.)",AB,AB,Configure the Origin Protocol Policy to use HTTPS only,Configure the Viewer Protocol Policy to use HTTPS only,Set up an Origin Access Control (OAC) setting,Configure your ALB to only allow traffic on port 443 using an SSL certificate from AWS Config.,Associate a Web ACL using AWS Web Application Firewall (WAF) with your CloudFront Distribution.,,,"개발자는 Application Load Balancer, SQS 대기열 및 EC2 인스턴스의 Auto Scaling 그룹으로 구성된 분산 시스템을 관리하고 있습니다. 이 시스템은 전 세계 고객에게 더 나은 서비스를 제공하기 위해 CloudFront와 통합되었습니다. 이동 중인 데이터의 보안을 강화하기 위해 개발자는 원본과 최종 사용자 간에 엔드투엔드 SSL 연결을 설정하라는 지시를 받았습니다.개발자가 CloudFront를 사용하여 이 요구 사항을 충족할 수 있는 두 가지 옵션은 무엇입니까? (2개를 선택하세요.)",HTTPS만 사용하도록 원본 프로토콜 정책 구성,HTTPS만 사용하도록 뷰어 프로토콜 정책 구성,OAC(원본 액세스 제어) 설정 지정,AWS Config의 SSL 인증서를 사용하여 포트 443의 트래픽만 허용하도록 ALB를 구성합니다.,AWS WAF(Web Application Firewall)를 사용하여 웹 ACL을 CloudFront 배포와 연결합니다.,,0,,
udemy,DVA-02,77,A Software Engineer is developing an application that will be hosted on an EC2 instance and read messages from a standard SQS queue. The average time that it takes for the producers to send a new message to the queue is 10 seconds.Which of the following is the MOST efficient way for the application to query the new messages from the queue?,A,A,Configure the SQS queue to use Long Polling.,Configure an SQS Delay Queue with a value of 10 seconds.,Configure each message in the SQS queue to have a custom visibility timeout of 10 seconds.,Configure the SQS queue to use Short Polling.,,,,소프트웨어 엔지니어는 EC2 인스턴스에 호스팅되고 표준 SQS 대기열에서 메시지를 읽는 애플리케이션을 개발 중입니다. 생산자가 새 메시지를 대기열에 보내는 데 걸리는 평균 시간은 10초입니다.다음 중 애플리케이션이 큐에서 새 메시지를 쿼리하는 가장 효율적인 방법은 무엇입니까?,긴 폴링을 사용하도록 SQS 대기열을 구성합니다.,SQS 지연 대기열을 10초 값으로 구성합니다.,SQS 대기열의 각 메시지가 10초의 사용자 정의 표시 시간 초과를 갖도록 구성합니다.,짧은 폴링을 사용하도록 SQS 대기열을 구성합니다.,,,0,,
udemy,DVA-02,78,"A reporting application is hosted in Elastic Beanstalk and uses DynamoDB as its database. If a user requests data, the application scans the entire table and returns the requested data. In the coming weeks, it is expected that the table will grow due to the surge of new users and requested reports.Which of the following should be done as a preparation to improve the application's performance with minimal cost? (Select TWO.)",AC,AC,Reduce page size,Increase the Write Compute Unit (WCU) of the table,Use Query operations instead,Use DynamoDB Accelerator (DAX),Increase page size,,,보고 애플리케이션은 Elastic Beanstalk에서 호스팅되며 DynamoDB를 데이터베이스로 사용합니다. 사용자가 데이터를 요청하면 애플리케이션은 전체 테이블을 스캔하고 요청된 데이터를 반환합니다. 앞으로 몇 주 동안 신규 사용자 급증과 보고서 요청으로 인해 테이블이 늘어날 것으로 예상됩니다.최소한의 비용으로 애플리케이션의 성능을 향상시키기 위해 다음 중 준비해야 할 것은 무엇입니까? (2개를 선택하세요.),페이지 크기 줄이기,테이블의 WCU(쓰기 컴퓨팅 단위)를 늘립니다.,대신 쿼리 작업을 사용하세요,DynamoDB Accelerator(DAX) 사용,페이지 크기 늘리기,,0,,
udemy,DVA-02,79,An API gateway with a Lambda proxy integration takes a long time to complete its processing. There were also occurrences where some requests timed out. You want to monitor the responsiveness of your API calls as well as the underlying Lambda function. Which of the following CloudWatch metrics should you use to troubleshoot this issue? (Select TWO.),DE,DE,Count,CacheMissCount,CacheHitCount,Latency,IntegrationLatency,,,Lambda 프록시 통합이 포함된 API 게이트웨이는 처리를 완료하는 데 오랜 시간이 걸립니다. 일부 요청이 시간 초과되는 경우도 있었습니다. API 호출과 기본 Lambda 함수의 응답성을 모니터링하려고 합니다.이 문제를 해결하려면 다음 중 어떤 CloudWatch 지표를 사용해야 합니까? (2개를 선택하세요.),세다,캐시 누락 수,캐시 적중 횟수,지연 시간,통합지연,,0,,
udemy,DVA-02,80,A developer is creating an analytics REST API service that is powered by API Gateway. Analysts from a separate AWS account must interact with the service through an IAM role. The IAM role already has a policy that grants permission to invoke the API.What else should the developer do to meet the requirement without too much overhead?,D,D,Create an API Key for the API. Attach a resource policy to the API that grants permission to the specified IAM role to invoke the GetAPIKeys action.,Create a Cognito User Pool authorizer. Add the IAM role to the user pool. Authenticate the requester’s identity using Cognito. Ask the analysts to pass the token returned by Cognito in their request headers.,"Create a Lambda function authorizer for the API. In the Lambda function, write a logic that verifies the requester's identity by extracting the information from the context object.",Set AWS_IAM as the method authorization type for the API. Attach a resource policy to the API that grants permission to the specified IAM role to invoke the execute-api:Invoke action.,,,,개발자가 API Gateway로 구동되는 분석 REST API 서비스를 만들고 있습니다. 별도의 AWS 계정의 분석가는 IAM 역할을 통해 서비스와 상호 작용해야 합니다. IAM 역할에는 API 호출 권한을 부여하는 정책이 이미 있습니다.너무 많은 오버헤드 없이 요구 사항을 충족하려면 개발자가 또 무엇을 해야 합니까?,API에 대한 API 키를 생성합니다. 작업 을 호출하기 위해 지정된 IAM 역할에 권한을 부여하는 API에 리소스 정책을 연결합니다 GetAPIKeys.,Cognito 사용자 풀 권한 부여자를 생성합니다. 사용자 풀에 IAM 역할을 추가합니다. Cognito를 사용하여 요청자의 신원을 인증합니다. 분석가에게 Cognito가 반환한 토큰을 요청 헤더에 전달하도록 요청하세요.,API에 대한 Lambda 함수 권한 부여자를 생성합니다. Lambda 함수에서 객체로부터 정보를 추출하여 요청자의 신원을 확인하는 로직을 작성합니다 context.,AWS_IAMAPI에 대한 메소드 인증 유형으로 설정합니다 . 작업 을 호출하기 위해 지정된 IAM 역할에 권한을 부여하는 API에 리소스 정책을 연결합니다 execute-api:Invoke.,,,0,,
udemy,DVA-02,81,A developer is instructed to configure a worker daemon to queue messages based on a specific schedule using a worker environment hosted in Elastic Beanstalk. Periodic tasks should be defined to automatically add jobs to your worker environment's queue at regular intervals.Which configuration file should the developer add to the source bundle to meet the above requirement?,C,C,env.yaml,appspec.yml,cron.yaml,Dockerrun.aws.json,,,,개발자는 Elastic Beanstalk에서 호스팅되는 작업자 환경을 사용하여 특정 일정에 따라 메시지를 대기열에 추가하도록 작업자 데몬을 구성하라는 지시를 받습니다. 정기적인 간격으로 작업자 환경의 대기열에 작업을 자동으로 추가하려면 정기적인 작업을 정의해야 합니다.위 요구 사항을 충족하려면 개발자가 소스 번들에 어떤 구성 파일을 추가해야 합니까?,env.yaml,appspec.yml,cron.yaml,Dockerrun.aws.json,,,0,,
udemy,DVA-02,82,An internal web application is hosted in a custom VPC with multiple private subnets only. Every EC2 instance that will be provisioned on this VPC will require access to an S3 bucket to pull configuration files as well as to push application logs.Which of the following options is the most suitable solution to use in this scenario?,D,D,"Use the AWS SDK for your application and issue the aws configure CLI command to store your access keys, which will be referred to by the SDK.",Create an IAM Role and attach it to each EC2 instance.,Store the IAM user and password in the application code to access the S3 bucket.,Create a VPC endpoint for S3.,,,,내부 웹 애플리케이션은 프라이빗 서브넷이 여러 개인 사용자 지정 VPC에서만 호스팅됩니다. 이 VPC에 프로비저닝될 모든 EC2 인스턴스는 구성 파일을 가져오고 애플리케이션 로그를 푸시하기 위해 S3 버킷에 액세스해야 합니다.다음 옵션 중 이 시나리오에 사용하기에 가장 적합한 솔루션은 무엇입니까?,애플리케이션에 AWS SDK를 사용하고 aws configureCLI 명령을 실행하여 SDK에서 참조할 액세스 키를 저장하세요.,IAM 역할을 생성하고 이를 각 EC2 인스턴스에 연결합니다.,S3 버킷에 액세스하려면 애플리케이션 코드에 IAM 사용자와 비밀번호를 저장하세요.,S3용 VPC 엔드포인트를 생성합니다.,,,0,,
udemy,DVA-02,83,A developer is building a web application which requires a multithreaded event-based key/value cache store that will cache result sets from database calls. You need to run large nodes with multiple cores for your cache layer and it should scale up or down as the demand on your system increases and decreases. Which of the following is the MOST suitable service that you should use?,C,C,Amazon CloudFront,AWS Greengrass,Amazon ElastiCache for Memcached,Amazon ElastiCache for Redis,,,,개발자는 데이터베이스 호출의 결과 세트를 캐시하는 다중 스레드 이벤트 기반 키/값 캐시 저장소가 필요한 웹 애플리케이션을 구축하고 있습니다. 캐시 레이어에 대해 여러 코어가 있는 대규모 노드를 실행해야 하며 시스템 수요가 증가하거나 감소함에 따라 확장 또는 축소되어야 합니다.다음 중 귀하가 사용해야 할 가장 적합한 서비스는 무엇입니까?,아마존 클라우드프론트,AWS 그린그래스,Memcached용 Amazon ElastiCache,Redis용 Amazon ElastiCache,,,0,,
udemy,DVA-02,84,An application hosted in an Amazon ECS Cluster processes a large data stream and stores the result in a DynamoDB table. There is an urgent requirement to detect new entries in the table and automatically trigger a Lambda function to run some verification tests on the processed data.Which of the following options can satisfy the requirement with minimal configuration?,B,B,Run the Lambda function using SNS each time the ECS Cluster successfully processes financial data.,Enable DynamoDB Streams to detect the new entries and automatically trigger the Lambda function.,Set up an Amazon EventBridge rule to automatically trigger the Lambda function whenever a new entry is created in the DynamoDB table.,"Detect the new entries in the DynamoDB table using AWS Copilot, then automatically invoke the Lambda function for processing.",,,,Amazon ECS 클러스터에 호스팅된 애플리케이션은 대규모 데이터 스트림을 처리하고 결과를 DynamoDB 테이블에 저장합니다. 테이블에서 새 항목을 감지하고 Lambda 함수를 자동으로 트리거하여 처리된 데이터에 대해 일부 확인 테스트를 실행해야 하는 긴급한 요구 사항이 있습니다.다음 중 최소한의 구성으로 요구 사항을 충족할 수 있는 옵션은 무엇입니까?,ECS 클러스터가 재무 데이터를 성공적으로 처리할 때마다 SNS를 사용하여 Lambda 함수를 실행합니다.,DynamoDB 스트림을 활성화하여 새 항목을 감지하고 자동으로 Lambda 함수를 트리거합니다.,DynamoDB 테이블에 새 항목이 생성될 때마다 Lambda 함수를 자동으로 트리거하도록 Amazon EventBridge 규칙을 설정합니다.,AWS Copilot을 사용하여 DynamoDB 테이블에서 새 항목을 감지한 다음 처리를 위해 Lambda 함수를 자동으로 호출합니다.,,,0,,
udemy,DVA-02,85,"A developer is refactoring a Lambda function that currently processes data using a public GraphQL API. There’s a new requirement to store query results in a database hosted in a VPC. The function has been configured with additional VPC-specific information, and the database connection has been successfully established. However, the engineer has discovered that the function can no longer connect to the internet after testing.Which of the following should the developer do to fix this issue? (Select TWO.)",BE,BE,Set up elastic network interfaces (ENIs) to enable your Lambda function to connect securely to other resources within your private VPC.,Ensure that the associated security group of the Lambda function allows outbound connections.,Submit a limit increase request to AWS to raise the concurrent executions limit of your Lambda function.,Configure your function to forward payloads that were not processed to a dead-letter queue (DLQ) using Amazon SQS.,Add a NAT gateway to your VPC.,,,개발자는 현재 공개 GraphQL API를 사용하여 데이터를 처리하는 Lambda 함수를 리팩터링하고 있습니다. VPC에서 호스팅되는 데이터베이스에 쿼리 결과를 저장해야 하는 새로운 요구 사항이 있습니다. 추가 VPC 관련 정보로 기능이 구성되었으며 데이터베이스 연결이 성공적으로 설정되었습니다. 그러나 엔지니어는 테스트 후 해당 기능이 더 이상 인터넷에 연결될 수 없음을 발견했습니다.다음 중 이 문제를 해결하려면 개발자가 수행해야 하는 작업은 무엇입니까? (2개를 선택하세요.),Lambda 함수가 프라이빗 VPC 내의 다른 리소스에 안전하게 연결할 수 있도록 탄력적 네트워크 인터페이스(ENI)를 설정하십시오.,Lambda 함수의 연결된 보안 그룹이 아웃바운드 연결을 허용하는지 확인하십시오.,Lambda 함수의 동시 실행 한도를 높이려면 AWS에 한도 증가 요청을 제출하십시오.,Amazon SQS를 사용하여 처리되지 않은 페이로드를 배달 못한 편지 대기열(DLQ)로 전달하도록 함수를 구성합니다.,VPC에 NAT 게이트웨이를 추가합니다.,,0,,
udemy,DVA-02,86,"A programmer is developing a Node.js application that will be run on a Linux server in their on-premises data center. The application will access various AWS services such as S3, DynamoDB, and ElastiCache using the AWS SDK.Which of the following is the MOST suitable way to provide access for the developer to accomplish the specified task?",C,C,"Go to the AWS Console and create a new IAM User with the appropriate permissions. In the application server, create the credentials file at ~/.aws/credentials with the username and the hashed password of the IAM User.",Create an IAM role with the appropriate permissions to access the required AWS services. Assign the role to the on-premises Linux server.,"Go to the AWS Console and create a new IAM user with programmatic access. In the application server, create the credentials file at ~/.aws/credentials with the access keys of the IAM user.","Create an IAM role with the appropriate permissions to access the required AWS services and assign the role to the on-premises Linux server. Whenever the application needs to access any AWS services, request for temporary security credentials from STS using the AssumeRole API.",,,,"프로그래머가 온프레미스 데이터 센터의 Linux 서버에서 실행될 Node.js 애플리케이션을 개발하고 있습니다. 애플리케이션은 AWS SDK를 사용하여 S3, DynamoDB 및 ElastiCache와 같은 다양한 AWS 서비스에 액세스합니다.다음 중 개발자가 지정된 작업을 수행할 수 있도록 액세스를 제공하는 가장 적합한 방법은 무엇입니까?",AWS 콘솔로 이동하여 적절한 권한이 있는 새 IAM 사용자를 생성합니다. 애플리케이션 서버에서 ~/.aws/credentialsIAM 사용자의 사용자 이름과 해시된 비밀번호를 사용하여 자격 증명 파일을 생성합니다.,필요한 AWS 서비스에 액세스할 수 있는 적절한 권한이 있는 IAM 역할을 생성합니다. 온프레미스 Linux 서버에 역할을 할당합니다.,AWS 콘솔로 이동하여 프로그래밍 방식으로 액세스할 수 있는 새 IAM 사용자를 생성합니다. 애플리케이션 서버에서 ~/.aws/credentialsIAM 사용자의 액세스 키를 사용하여 자격 증명 파일을 생성합니다.,필요한 AWS 서비스에 액세스할 수 있는 적절한 권한이 있는 IAM 역할을 생성하고 온프레미스 Linux 서버에 역할을 할당합니다. 애플리케이션이 AWS 서비스에 액세스해야 할 때마다 AssumeRoleAPI를 사용하여 STS에 임시 보안 자격 증명을 요청하세요.,,,0,,
udemy,DVA-02,87,"You developed a shell script which uses AWS CLI to create a new Lambda function. However, you received an InvalidParameterValueException after running the script. What is the MOST likely cause of this issue?",C,C,You have exceeded your maximum total code size per account.,The resource already exists.,You provided an IAM role in the CreateFunction API which AWS Lambda is unable to assume.,The AWS Lambda service encountered an internal error.,,,,AWS CLI를 사용하여 새 Lambda 함수를 생성하는 셸 스크립트를 개발했습니다. InvalidParameterValueException그러나 스크립트를 실행한 후에 는 을 받았습니다 .이 문제의 가장 가능성 있는 원인은 무엇입니까?,계정당 최대 총 코드 크기를 초과했습니다.,리소스가 이미 존재합니다.,CreateFunctionAWS Lambda가 맡을 수 없는 API 에 IAM 역할을 제공했습니다 .,AWS Lambda 서비스에 내부 오류가 발생했습니다.,,,0,,
udemy,DVA-02,88,Your serverless AWS Lambda functions are integrated with Amazon API gateway using Lambda proxy integration. The API caching feature is enabled in the API Gateway with a TTL value of 300 seconds. A client would like to fetch the latest data from your endpoints every time a request is sent and invalidate the existing cache.What should the client do in order to get the latest data?,A,A,Have the client send a request with the Cache-Control: max-age=0 header.,Have the client send a request with the Cached: false header.,Modify cache TTL value to a shorter period.,Override API caching by allowing the client to send requests to the endpoint directly.,,,,서버리스 AWS Lambda 함수는 Lambda 프록시 통합을 사용하여 Amazon API 게이트웨이와 통합됩니다. API 캐싱 기능은 TTL 값이 300초로 API 게이트웨이에서 활성화됩니다. 클라이언트는 요청이 전송될 때마다 엔드포인트에서 최신 데이터를 가져오고 기존 캐시를 무효화하려고 합니다.최신 데이터를 얻으려면 클라이언트가 무엇을 해야 합니까?,클라이언트가 Cache-Control: max-age=0헤더와 함께 요청을 보내도록 합니다.,클라이언트가 Cached: false헤더와 함께 요청을 보내도록 합니다.,캐시 TTL 값을 더 짧은 기간으로 수정합니다.,클라이언트가 엔드포인트에 직접 요청을 보낼 수 있도록 허용하여 API 캐싱을 재정의합니다.,,,0,,
udemy,DVA-02,89,An online stock trading platform is hosted in an Auto Scaling group of EC2 instances with an Application Load Balancer in front to distribute the incoming traffic evenly. The developer must capture information about the IP traffic going to and from network interfaces in your VPC to comply with financial regulatory requirements.Which of the following options should the developer do to meet the requirement?,B,B,Use CloudTrail logs to track all API calls and capture information about the IP traffic going to and from your VPC.,Create a flow log in your VPC.,Use AWS Inspector to capture information about the IP traffic going to and from the network interfaces of your EC2 instances.,Install and run the AWS X-Ray daemon to your EC2 instances using an instance metadata script.,,,,온라인 주식 거래 플랫폼은 들어오는 트래픽을 고르게 분산하기 위해 Application Load Balancer가 앞에 있는 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅됩니다. 개발자는 금융 규제 요구 사항을 준수하기 위해 VPC의 네트워크 인터페이스를 오가는 IP 트래픽에 대한 정보를 캡처해야 합니다.요구 사항을 충족하려면 다음 중 개발자가 수행해야 하는 옵션은 무엇입니까?,CloudTrail 로그를 사용하면 모든 API 호출을 추적하고 VPC로 들어오고 나가는 IP 트래픽에 대한 정보를 캡처할 수 있습니다.,VPC에서 흐름 로그를 생성합니다.,AWS Inspector를 사용하여 EC2 인스턴스의 네트워크 인터페이스를 오가는 IP 트래픽에 대한 정보를 캡처합니다.,인스턴스 메타데이터 스크립트를 사용하여 EC2 인스턴스에 AWS X-Ray 데몬을 설치하고 실행합니다.,,,0,,
udemy,DVA-02,90,"An application, which already uses X-Ray, generates thousands of trace data every hour. The developer wants to use a filter expression that will limit the results based on custom attributes or keys that he specifies.How should the developer refactor the application in order to filter the results in the X-Ray console?",B,B,Include the custom attributes as new segment fields in the segment document.,Add the custom attributes as annotations in your segment document.,Add the custom attributes as metadata in your segment document.,Create a new sampling rule based on the custom attributes.,,,,이미 X-Ray를 사용하고 있는 애플리케이션은 매시간 수천 개의 추적 데이터를 생성합니다. 개발자는 자신이 지정하는 사용자 정의 속성이나 키를 기반으로 결과를 제한하는 필터 표현식을 사용하려고 합니다.X-Ray 콘솔에서 결과를 필터링하려면 개발자가 애플리케이션을 어떻게 리팩터링해야 합니까?,세그먼트 문서의 새 세그먼트 필드로 사용자 정의 속성을 포함합니다.,세그먼트 문서에 사용자 정의 속성을 주석으로 추가합니다.,세그먼트 문서에 사용자 정의 속성을 메타데이터로 추가합니다.,맞춤 속성을 기반으로 새 샘플링 규칙을 만듭니다.,,,0,,
udemy,DVA-02,91,A mobile game is using a DynamoDB table named GameScore that keeps track of users and scores. Each item in the table is identified by a partition key (UserId) and a sort key (GameTitle). The diagram below shows how the items in the table are organized:A developer wants to write a leaderboard application to display the top scores for each game.How can the developer meet the requirement in the MOST efficient manner?,A,A,Create a global secondary index. Assign the GameTitle attribute as the partition key and the TopScore attribute as the sort key.,Create a local secondary index. Assign the TopScore attribute as the partition key and the GameTitle attribute as the sort key.,Create a local secondary index. Assign the GameTitle attribute as the partition key and the TopScore attribute as the sort key.,Use the Scan operation and filter the results based on a GameTitle value.,,,,모바일 게임은 사용자와 점수를 추적하는 GameScore라는 DynamoDB 테이블을 사용하고 있습니다. 테이블의 각 항목은 파티션 키(UserId)와 정렬 키(GameTitle)로 식별됩니다. 아래 다이어그램은 테이블의 항목이 구성되는 방식을 보여줍니다.개발자는 각 게임의 최고 점수를 표시하는 리더보드 애플리케이션을 작성하려고 합니다.개발자는 어떻게 가장 효율적인 방식으로 요구 사항을 충족할 수 있습니까?,글로벌 보조 인덱스를 생성합니다. GameTitle 속성을 파티션 키로 할당하고 TopScore 속성을 정렬 키로 할당합니다.,로컬 보조 인덱스를 생성합니다. TopScore 속성을 파티션 키로 할당하고 GameTitle 속성을 정렬 키로 할당합니다.,로컬 보조 인덱스를 생성합니다. GameTitle 속성을 파티션 키로 할당하고 TopScore 속성을 정렬 키로 할당합니다.,작업을 사용 Scan하고 GameTitle 값을 기준으로 결과를 필터링합니다.,,,0,,
udemy,DVA-02,92,"You have created a Node.js Lambda function that updates a DynamoDB table and sends an email notification via Amazon SNS. However, upon testing, the function is not working as expected. Which of the following is the BEST way to troubleshoot this issue?",A,A,Use AWS X-Ray,Use AWS CloudTrail,Use Amazon CloudWatch,Use Amazon Inspector,,,,DynamoDB 테이블을 업데이트하고 Amazon SNS를 통해 이메일 알림을 보내는 Node.js Lambda 함수를 생성했습니다. 그러나 테스트 결과 기능이 예상대로 작동하지 않습니다. 다음 중 이 문제를 해결하는 가장 좋은 방법은 무엇입니까?,AWS X-Ray 사용,AWS CloudTrail 사용,Amazon CloudWatch 사용,Amazon Inspector 사용,,,0,,
udemy,DVA-02,93,A serverless application is composed of several Lambda functions which reads data from RDS. These functions must share the same connection string that should be encrypted to improve data security.   Which of the following is the MOST secure way to meet the above requirement?,A,A,Create a Secure String Parameter using the AWS Systems Manager Parameter Store.,Use AWS Lambda environment variables encrypted with CloudHSM.,Use AWS Lambda environment variables encrypted with KMS which will be shared by the Lambda functions.,Create an IAM Execution Role that has access to RDS and attach it to the Lambda functions.,,,,서버리스 애플리케이션은 RDS에서 데이터를 읽는 여러 Lambda 함수로 구성됩니다. 이러한 함수는 데이터 보안을 강화하기 위해 암호화해야 하는 동일한 연결 문자열을 공유해야 합니다.   다음 중 위의 요구 사항을 충족하는 가장 안전한 방법은 무엇입니까?,AWS 시스템 관리자 매개변수 저장소를 사용하여 보안 문자열 매개변수를 생성합니다.,CloudHSM으로 암호화된 AWS Lambda 환경 변수를 사용합니다.,Lambda 함수에서 공유할 KMS로 암호화된 AWS Lambda 환경 변수를 사용합니다.,RDS에 액세스할 수 있는 IAM 실행 역할을 생성하고 이를 Lambda 함수에 연결합니다.,,,0,,
udemy,DVA-02,94,"A mobile game has a serverless backend consisting of an API Gateway backed by Lambda functions and a DynamoDB table in provisioned capacity mode, where player data is stored. While the game has maintained a consistent level of traffic, recent growth in the player base has caused response times to slow down. To improve performance, the developer wants to reduce the number of database queries for data that rarely change.What approach can the developer take to achieve this goal cost-effectively and with less development overhead?",A,A,Set up an Amazon DynamoDB Accelerator (DAX) caching layer in front of the DynamoDB table.,Switch the DynamoDB table’s capacity mode to On-demand.,Use DynamoDB Session Handler to handle the saving and retrieval of player data.,Create an Amazon MemoryDB for Redis database in front of the DynamoDB table to cache data.,,,,모바일 게임에는 Lambda 함수로 지원되는 API 게이트웨이와 플레이어 데이터가 저장되는 프로비저닝된 용량 모드의 DynamoDB 테이블로 구성된 서버리스 백엔드가 있습니다. 게임은 일관된 수준의 트래픽을 유지했지만 최근 플레이어 기반의 증가로 인해 응답 시간이 느려졌습니다. 성능을 향상시키기 위해 개발자는 거의 변경되지 않는 데이터에 대한 데이터베이스 쿼리 수를 줄이고 싶어합니다.개발 오버헤드를 줄이고 비용 효율적으로 이 목표를 달성하기 위해 개발자는 어떤 접근 방식을 취할 수 있습니까?,DynamoDB 테이블 앞에 Amazon DynamoDB Accelerator(DAX) 캐싱 계층을 설정합니다.,DynamoDB 테이블의 용량 모드를 온디맨드로 전환합니다.,DynamoDB 세션 핸들러를 사용하여 플레이어 데이터 저장 및 검색을 처리합니다.,데이터를 캐시하기 위해 DynamoDB 테이블 앞에 Redis용 Amazon MemoryDB 데이터베이스를 생성합니다.,,,0,,
udemy,DVA-02,95,A mobile app is using a backend API hosted in AWS. You want to develop a push notification feature that can send messages directly to mobile apps whenever there is a new version of the app available. The notification message can include a link to download and install the update. Which of the following is the BEST service to use to develop this feature?,A,A,SNS,SQS,CodeDeploy,SES,,,,모바일 앱은 AWS에서 호스팅되는 백엔드 API를 사용하고 있습니다. 새로운 버전의 앱이 출시될 때마다 모바일 앱에 직접 메시지를 보낼 수 있는 푸시 알림 기능을 개발하려고 합니다. 알림 메시지에는 업데이트를 다운로드하고 설치할 수 있는 링크가 포함될 수 있습니다.다음 중 이 기능을 개발하는 데 사용할 수 있는 가장 좋은 서비스는 무엇입니까?,SNS,SQS,코드배포,SES,,,0,,
udemy,DVA-02,96,"To improve their information security management system (ISMS), a company recently released a new policy which requires all database credentials to be encrypted and be automatically rotated to avoid unauthorized access.  Which of the following is the MOST appropriate solution to secure the credentials?",C,C,Enable IAM DB authentication which rotates the credentials by default.,Create a parameter to the Systems Manager Parameter Store using the PutParameter API with a type of SecureString.,Create a secret in AWS Secrets Manager and enable automatic rotation of the database credentials.,Create an IAM Role which has full access to the database. Attach the role to the services which require access.,,,,ISMS(정보 보안 관리 시스템)를 개선하기 위해 최근 한 회사는 모든 데이터베이스 자격 증명을 암호화하고 무단 액세스를 방지하기 위해 자동으로 교체하도록 요구하는 새로운 정책을 발표했습니다.  다음 중 자격 증명을 보호하는 데 가장 적합한 솔루션은 무엇입니까?,기본적으로 자격 증명을 교체하는 IAM DB 인증을 활성화합니다.,유형이 PutParameter API인 Systems Manager Parameter Store에 대한 매개변수를 생성합니다 SecureString.,AWS Secrets Manager에서 비밀을 생성하고 데이터베이스 자격 증명의 자동 교체를 활성화합니다.,데이터베이스에 대한 전체 액세스 권한이 있는 IAM 역할을 생성합니다. 액세스가 필요한 서비스에 역할을 연결합니다.,,,0,,
udemy,DVA-02,97,"A website is hosted in an Auto Scaling group of EC2 instances behind an Application Load Balancer. It also uses CloudFront with a default domain name to distribute its static assets and dynamic contents. However, the website has a poor search ranking as it doesn't use a secure HTTPS/SSL on its site. Which are the possible solutions that the developer can implement in order to set up HTTPS communication between the viewers and CloudFront? (Select TWO.)",CD,CD,Use a self-signed SSL/TLS certificate in the ALB which is stored in a private S3 bucket.,Use a self-signed certificate in the ALB.,Set the Viewer Protocol Policy to use HTTPS Only.,Set the Viewer Protocol Policy to use Redirect HTTP to HTTPS.,Configure the ALB to use its default SSL/TLS certificate.,,,웹 사이트는 Application Load Balancer 뒤에 있는 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅됩니다. 또한 기본 도메인 이름과 함께 CloudFront를 사용하여 정적 자산과 동적 콘텐츠를 배포합니다. 그러나 해당 웹사이트는 사이트에서 보안 HTTPS/SSL을 사용하지 않기 때문에 검색 순위가 낮습니다. 최종 사용자와 CloudFront 간의 HTTPS 통신을 설정하기 위해 개발자가 구현할 수 있는 가능한 솔루션은 무엇입니까 ? (2개를 선택하세요.),프라이빗 S3 버킷에 저장된 ALB에서 자체 서명된 SSL/TLS 인증서를 사용합니다.,ALB에서 자체 서명된 인증서를 사용합니다.,를 Viewer Protocol Policy사용하도록 설정합니다 HTTPS Only.,를 Viewer Protocol Policy사용하도록 설정합니다 Redirect HTTP to HTTPS.,기본 SSL/TLS 인증서를 사용하도록 ALB를 구성합니다.,,0,,
udemy,DVA-02,98,"A company has created a private S3 bucket named tdojo. The Developer IAM role must be granted read access to all objects within this bucket. However, objects stored under the qa folder should be restricted to the QA IAM role only.Which S3 bucket policy will effectively implement the principle of least privilege access while satisfying the given requirements?",D,D,"{  ""Version"": ""2012-10-17"",  ""Statement"": [    {    ""Effect"": ""Allow"",    ""Principal"": {      ""AWS"": [          ""arn:aws:iam::123456789123:role/Developer"",          ""arn:aws:iam::123456789123:role/QA""      ]    },    ""Action"": [    ""s3:GetObject""    ],    ""Resource"": [       ""arn:aws:s3:::tdojo/*"",       ""arn:aws:s3:::tdojo/qa/*""    ]    }  ]}","{  ""Version"": ""2012-10-17"",  ""Statement"": [    {    ""Effect"": ""Allow"",    ""Principal"": {      ""AWS"": [          ""arn:aws:iam::123456789123:role/Developer""      ]    },    ""Action"": [    ""s3:*""    ],    ""Resource"": ""arn:aws:s3:::tdojo/*""    },    {    ""Effect"": ""Allow"",    ""Principal"": {      ""AWS"": [          ""arn:aws:iam::123456789123:role/QA""      ]    },    ""Action"": [    ""s3:GetObject""    ],    ""Resource"": ""arn:aws:s3:::tdojo/qa/*""    }  ]}","{  ""Version"": ""2012-10-17"",  ""Statement"": [    {    ""Effect"": ""Allow"",    ""Principal"": {      ""AWS"": [          ""arn:aws:iam::123456789123:role/Developer""      ]    },    ""Action"": [    ""s3:GetObject"",    ""s3:PutObject""    ],    ""Resource"": ""arn:aws:s3:::tdojo/*""    },    {    ""Effect"": ""Allow"",    ""Principal"": {      ""AWS"": [          ""arn:aws:iam::123456789123:role/QA""      ]    },    ""Action"": [    ""s3:GetObject""    ],    ""Resource"": ""arn:aws:s3:::tdojo/qa/*""    }  ]}","{  ""Version"": ""2012-10-17"",  ""Statement"": [    {    ""Effect"": ""Allow"",    ""Principal"": {      ""AWS"": [          ""arn:aws:iam::123456789123:role/Developer""      ]    },    ""Action"": [    ""s3:GetObject""    ],    ""Resource"": ""arn:aws:s3:::tdojo/*""    },    {    ""Effect"": ""Allow"",    ""Principal"": {      ""AWS"": [          ""arn:aws:iam::123456789123:role/QA""      ]    },    ""Action"": [    ""s3:GetObject""    ],    ""Resource"": ""arn:aws:s3:::tdojo/qa/*""    }  ]}",,,,한 회사에서 이름이 붙은 프라이빗 S3 버킷을 만들었습니다 tdojo. 개발자 IAM 역할에는 이 버킷 내의 모든 객체에 대한 읽기 액세스 권한이 부여되어야 합니다. 그러나 qa폴더 아래에 저장된 객체는 QA IAM 역할로만 제한되어야 합니다.주어진 요구 사항을 충족하면서 최소 권한 액세스 원칙을 효과적으로 구현하는 S3 버킷 정책은 무엇입니까?,"{  ""버전"" : ""2012-10-17"" ,   ""진술"" : [     {    ""효과"" : ""허용"" ,     ""주체"" : {       ""AWS"" : [           ""arn:aws:iam::123456789123:역할/개발자"" ,          ""arn:aws:iam::123456789123:역할/QA""      ]    },    ""액션"" : [     ""s3:GetObject""    ],    ""리소스"" : [        ""arn:aws:s3:::tdojo/*"" ,       ""arn:aws:s3:::tdojo/qa/*""    ]    }  ]}","{  ""버전"" : ""2012-10-17"" ,   ""진술"" : [     {    ""효과"" : ""허용"" ,     ""주체"" : {       ""AWS"" : [           ""arn:aws:iam::123456789123:역할/개발자""      ]    },    ""액션"" : [     ""s3:*""    ],    ""리소스"" : ""arn:aws:s3:::tdojo/*""     },    {    ""효과"" : ""허용"" ,     ""주체"" : {       ""AWS"" : [           ""arn:aws:iam::123456789123:역할/QA""      ]    },    ""액션"" : [     ""s3:GetObject""    ],    ""리소스"" : ""arn:aws:s3:::tdojo/qa/*""     }  ]}","{  ""버전"" : ""2012-10-17"" ,   ""진술"" : [     {    ""효과"" : ""허용"" ,     ""주체"" : {       ""AWS"" : [           ""arn:aws:iam::123456789123:역할/개발자""      ]    },    ""액션"" : [     ""s3:GetObject"" ,    ""s3:PutObject""    ],    ""리소스"" : ""arn:aws:s3:::tdojo/*""     },    {    ""효과"" : ""허용"" ,     ""주체"" : {       ""AWS"" : [           ""arn:aws:iam::123456789123:역할/QA""      ]    },    ""액션"" : [     ""s3:GetObject""    ],    ""리소스"" : ""arn:aws:s3:::tdojo/qa/*""     }  ]}","{  ""버전"" : ""2012-10-17"" ,   ""진술"" : [     {    ""효과"" : ""허용"" ,     ""주체"" : {       ""AWS"" : [           ""arn:aws:iam::123456789123:역할/개발자""      ]    },    ""액션"" : [     ""s3:GetObject""    ],    ""리소스"" : ""arn:aws:s3:::tdojo/*""     },    {    ""효과"" : ""허용"" ,     ""주체"" : {       ""AWS"" : [           ""arn:aws:iam::123456789123:역할/QA""      ]    },    ""액션"" : [     ""s3:GetObject""    ],    ""리소스"" : ""arn:aws:s3:::tdojo/qa/*""     }  ]}",,,0,,
udemy,DVA-02,99,"You are deploying a serverless application composed of Lambda, API Gateway, CloudFront, and DynamoDB using CloudFormation. The AWS SAM syntax should be used to declare resources in your template which requires you to specify the version of the AWS Serverless Application Model (AWS SAM). Which of the following sections is required, aside from the Resources section, that should be in your CloudFormation template?",D,D,Parameters,Format Version,Mappings,Transform,,,,"CloudFormation을 사용하여 Lambda, API Gateway, CloudFront 및 DynamoDB로 구성된 서버리스 애플리케이션을 배포하고 있습니다. AWS SAM 구문은 AWS Serverless Application Model(AWS SAM)의 버전을 지정해야 하는 템플릿에서 리소스를 선언하는 데 사용해야 합니다.ResourcesCloudFormation 템플릿에 있어야 하는 섹션 외에 다음 중 필요한 섹션은 무엇입니까 ?",매개변수,형식 버전,매핑,변환,,,0,,
udemy,DVA-02,100,A Lambda function has been integrated with DynamoDB Streams as its event source. There has been a new version of the function that needs to be deployed using CodeDeploy where the traffic must be shifted in two increments. It should shift 10 percent of the incoming traffic to the new version in the first increment and then the remaining 90 percent should be deployed five minutes later. Which of the following deployment configurations is the MOST suitable to satisfy this requirement?,A,A,Canary,Rolling with additional batch,All-at-once,Linear,,,,Lambda 함수는 이벤트 소스로 DynamoDB 스트림과 통합되었습니다. 트래픽이 두 증분으로 이동되어야 하는 CodeDeploy를 사용하여 배포해야 하는 새로운 버전의 기능이 있습니다. 첫 번째 증분에서 들어오는 트래픽의 10%를 새 버전으로 이동해야 하며 나머지 90%는 5분 후에 배포되어야 합니다.다음 배포 구성 중 이 요구 사항을 충족하는 데 가장 적합한 것은 무엇입니까?,카나리아,추가 배치로 롤링,한꺼번에,선의,,,0,,
udemy,DVA-02,101,A developer is working on a Lambda function which has an event source mapping to process requests from API Gateway. The function will consistently have 10 requests per second and it will take a maximum of 50 seconds to complete each request. What should the developer do to prevent the function from throttling?,A,A,Do nothing since Lambda will automatically scale to handle the load.,Submit a Service Limit Increase request to AWS to raise your concurrent executions limit.,Use Dead Letter Queues (DLQ) to reprocess failed requests.,Implement traffic shifting in Lambda using Aliases.,,,,개발자는 API 게이트웨이의 요청을 처리하기 위한 이벤트 소스 매핑이 있는 Lambda 함수를 작업하고 있습니다  . 이 함수에는 지속적으로 초당 10개의 요청이 있으며 각 요청을 완료하는 데 최대 50초가 걸립니다.함수가 제한되는 것을 방지하려면 개발자는 어떻게 해야 합니까?,Lambda가 로드를 처리하기 위해 자동으로 확장되므로 아무 것도 하지 마십시오.,Service Limit Increase동시 실행 한도를 높이려면 AWS에 요청을 제출하십시오 .,실패한 요청을 재처리하려면 DLQ(배달 못한 편지 대기열)를 사용하십시오.,별칭을 사용하여 Lambda에서 트래픽 이동을 구현합니다.,,,0,,
udemy,DVA-02,102,A developer is building a new Docker application using ECS. She needs to allow containers to access ports on the host container instance to send or receive traffic using port mapping. Which component of ECS should the developer configure to properly implement this task?,C,C,Container Agent,Container instance,Task definition,Service scheduler,,,,개발자가 ECS를 사용하여 새로운 Docker 애플리케이션을 구축하고 있습니다. 포트 매핑을 사용하여 트래픽을 보내거나 받으려면 컨테이너가 호스트 컨테이너 인스턴스의 포트에 액세스하도록 허용해야 합니다.이 작업을 올바르게 구현하려면 개발자가 ECS의 어떤 구성 요소를 구성해야 합니까?,컨테이너 에이전트,컨테이너 인스턴스,작업 정의,서비스 스케줄러,,,0,,
udemy,DVA-02,103,A developer uses AWS SAM templates to deploy a serverless application. He needs to embed the application from the AWS Serverless Application Repository or from an S3 bucket as a nested application. Which of the following resource type is the most SUITABLE one that the developer should use?,C,C,AWS::Serverless::Api,AWS::Serverless::Function,AWS::Serverless::Application,AWS::Serverless::LayerVersion,,,,개발자는 AWS SAM 템플릿을 사용하여 서버리스 애플리케이션을 배포합니다. AWS Serverless Application Repository 또는 S3 버킷의 애플리케이션을 중첩 애플리케이션으로 포함해야 합니다.다음 리소스 유형 중 개발자가 사용해야 하는 가장 적합한 리소스 유형은 무엇입니까?,AWS::Serverless::Api,AWS::Serverless::Function,AWS::Serverless::Application,AWS::Serverless::LayerVersion,,,0,,
udemy,DVA-02,104,A mission-critical application must have a monitoring system that can provide immediate insight into its sub-minute activity. A developer must create a script that collects data on users who are currently logged in to the system every 10 seconds.Which solution best meets the above requirements?,B,B,Enable detailed monitoring.,Publish a high-resolution custom metric to CloudWatch.,Enable enhanced monitoring.,Publish a custom metric to CloudWatch using the PutMetricData API with the --storage-resolution parameter set to its default value.,,,,미션 크리티컬 애플리케이션에는 1분 미만의 활동에 대한 즉각적인 통찰력을 제공할 수 있는 모니터링 시스템이 있어야 합니다. 개발자는 현재 시스템에 로그인되어 있는 사용자에 대한 데이터를 10초마다 수집하는 스크립트를 작성해야 합니다.위의 요구 사항을 가장 잘 충족하는 솔루션은 무엇입니까?,상세한 모니터링을 활성화합니다.,CloudWatch에 고해상도 사용자 지정 지표를 게시합니다.,향상된 모니터링을 활성화합니다.,--storage-resolution매개변수가 기본값으로 설정된 PutMetricData API를 사용하여 CloudWatch에 사용자 지정 지표를 게시합니다 .,,,0,,
udemy,DVA-02,105,"A serverless application consisting of Lambda functions integrated with API Gateway, and DynamoDB processes ad hoc requests that its users send. Due to the recent spike in incoming traffic, some of your customers are complaining that they are getting HTTP 504 errors from time to time.Which of the following is the MOST likely cause of this issue?",A,A,API Gateway request has timed out because the underlying Lambda function has been running for more than 29 seconds.,"Since the incoming requests are increasing, the API Gateway automatically enabled throttling which caused the HTTP 504 errors.",An authorization failure occurred between API Gateway and the Lambda function.,The usage plan quota has been exceeded for the Lambda function.,,,,API Gateway와 DynamoDB가 통합된 Lambda 함수로 구성된 서버리스 애플리케이션은 사용자가 보내는 임시 요청을 처리합니다. 최근 수신 트래픽의 급증으로 인해 일부 고객이 때때로 HTTP 504 오류가 발생한다고 불평하고 있습니다.다음 중 이 문제의 가장 큰 원인은 무엇입니까?,기본 Lambda 함수가 29초 이상 실행되어 API Gateway 요청 시간이 초과되었습니다.,들어오는 요청이 증가하고 있기 때문에 API 게이트웨이는 자동으로 조절을 활성화하여 HTTP 504 오류를 발생시켰습니다.,API Gateway와 Lambda 함수 사이에 인증 오류가 발생했습니다.,Lambda 함수에 대한 사용량 계획 할당량이 초과되었습니다.,,,0,,
udemy,DVA-02,106,An application hosted in a multicontainer Docker platform in Elastic Beanstalk uses DynamoDB to handle the session data of its users. These data are only used in a particular timeframe and the stale data can be deleted after the user logged out of the system.Which of the following is the most suitable way to delete the session data?,D,D,Delete the stale data by regularly performing a scan on the table.,Use conditional writes to add the session data to the DynamoDB table and then automatically delete it based on the condition you specify.,Use atomic counters to track the validity of the session data and delete once it becomes stale.,Enable TTL for the session data in the DynamoDB table.,,,,"Elastic Beanstalk의 멀티컨테이너 Docker 플랫폼에 호스팅된 애플리케이션은 DynamoDB를 사용하여 사용자의 세션 데이터를 처리합니다. 이러한 데이터는 특정 기간에만 사용되며, 오래된 데이터는 사용자가 시스템에서 로그아웃한 후 삭제될 수 있습니다.다음 중 세션 데이터를 삭제하는 가장 적합한 방법은 무엇입니까?",테이블에 대해 정기적으로 스캔을 수행하여 오래된 데이터를 삭제하십시오.,conditional writesDynamoDB 테이블에 세션 데이터를 추가한 후 지정한 조건에 따라 자동으로 삭제하는 데 사용됩니다 .,atomic counters세션 데이터의 유효성을 추적하고 오래되면 삭제하는 데 사용됩니다 .,DynamoDB 테이블의 세션 데이터에 대해 TTL을 활성화합니다.,,,0,,
udemy,DVA-02,107,"A developer is instructed to set up a new serverless architecture composed of AWS Lambda, API Gateway, and DynamoDB in a single stack. The new architecture should allow the developer to locally build, test, and debug serverless applications. Which of the following should the developer use to satisfy the above requirement?",D,D,CloudFormation,Elastic Beanstalk,OpsWorks,AWS Serverless Application Model (AWS SAM),,,,"개발자는 단일 스택에 AWS Lambda, API Gateway 및 DynamoDB로 구성된 새로운 서버리스 아키텍처를 설정하라는 지시를 받습니다. 새로운 아키텍처를 통해 개발자는 서버리스 애플리케이션을 로컬에서 구축, 테스트 및 디버그할 수 있습니다.위의 요구 사항을 충족하기 위해 개발자는 다음 중 무엇을 사용해야 합니까?",CloudFormation,엘라스틱 콩나무,OpsWorks,AWS 서버리스 애플리케이션 모델(AWS SAM),,,0,,
udemy,DVA-02,108,"A developer has recently completed a new version of a serverless application that is ready to be deployed using AWS SAM. There is a requirement that the traffic should shift from the previous Lambda function to the new version in the shortest time possible, but you still don't want to shift traffic all-at-once immediately.Which deployment configuration is the MOST suitable one to use in this scenario?",B,B,CodeDeployDefault.LambdaLinear10PercentEvery2Minutes,CodeDeployDefault.LambdaCanary10Percent5Minutes,CodeDeployDefault.LambdaLinear10PercentEvery1Minute,CodeDeployDefault.HalfAtATime,,,,개발자는 최근 AWS SAM을 사용하여 배포할 준비가 된 서버리스 애플리케이션의 새 버전을 완성했습니다. 가능한 한 최단 시간 내에 트래픽이 이전 Lambda 함수에서 새 버전으로 전환되어야 한다는 요구 사항이 있지만 여전히 트래픽을 한꺼번에 즉시 전환하고 싶지는 않습니다.이 시나리오에 사용하기에 가장 적합한 배포 구성은 무엇입니까?,CodeDeployDefault.LambdaLinear10PercentEvery2Minutes,CodeDeployDefault.LambdaCanary10Percent5Minutes,CodeDeployDefault.LambdaLinear10PercentEvery1Minute,CodeDeployDefault.HalfAtATime,,,0,,
udemy,DVA-02,109,"Your customers require access to the REST APIs of your web application which is hosted on EC2 instances behind a load balancer in your VPC. To accommodate this request, your web services should be integrated with API Gateway that has a custom data mapping. You need to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. Which of the following integration types is the MOST suitable one to use in API Gateway to meet this requirement?",B,B,HTTP_PROXY,HTTP,AWS_PROXY,AWS,,,,고객은 VPC의 로드 밸런서 뒤에 있는 EC2 인스턴스에서 호스팅되는 웹 애플리케이션의 REST API에 액세스해야 합니다. 이 요청을 수용하려면 웹 서비스를 사용자 지정 데이터 매핑이 있는 API 게이트웨이와 통합해야 합니다. 수신 요청 데이터가 통합 요청에 매핑되는 방식과 결과 통합 응답 데이터가 메서드 응답에 매핑되는 방식을 지정해야 합니다.다음 중 이 요구 사항을 충족하기 위해 API Gateway에서 사용하기에 가장 적합한 통합 유형은 무엇입니까?,HTTP_PROXY,HTTP,AWS_PROXY,AWS,,,0,,
udemy,DVA-02,110,A web application is using an ElastiCache cluster that is suffering from cache churn. A developer needs to reconfigure the application so that data are retrieved from the database only in the event that there is a cache miss.Which pseudocode illustrates the caching strategy that the developer needs to implement?,B,B,"get_item(item_id):    item_value = cache.get(item_id)    if item_value is not None:        item_value = database.query(""SELECT * FROM Items WHERE id = ?"", item_id)        cache.add(item_id, item_value)        return item_value    else:        return item_value","get_item(item_id):    item_value = cache.get(item_id)    if item_value is None:        item_value = database.query(""SELECT * FROM Items WHERE id = ?"", item_id)        cache.add(item_id, item_value)    return item_value","get_item(item_id):    item_value = database.query(""SELECT * FROM Items WHERE id = ?"", item_id)    if item_value is None:        item_value = cache.set(item_id, item_value)        cache.add(item_id, item_value)    return item_value","get_item(item_id, item_value):    item_value = database.query(""UPDATE Items WHERE id = ?"", item_id, item_value)    cache.add(item_id, item_value)    return 'ok'",,,,웹 애플리케이션이 캐시 이탈 문제가 있는 ElastiCache 클러스터를 사용하고 있습니다. 개발자는 캐시 누락이 발생한 경우에만 데이터베이스에서 데이터를 검색하도록 애플리케이션을 재구성해야 합니다.개발자가 구현해야 하는 캐싱 전략을 보여주는 의사 코드는 무엇입니까?,"get_item ( 항목_ID ):    item_value = 캐시 . 가져오기 ( 항목_ID )    item_value 가 None 이 아닌 경우 :          item_value = 데이터베이스 . 쿼리 ( ""SELECT * FROM 항목 WHERE id = ?"" , item_id )        캐시 . 추가 ( 항목_ID , 항목_값 )        item_value 반환    그 밖의 :        item_value 반환","get_item ( 항목_ID ):    item_value = 캐시 . 가져오기 ( 항목_ID )    item_value 가 None 인 경우 :         item_value = 데이터베이스 . 쿼리 ( ""SELECT * FROM 항목 WHERE id = ?"" , item_id )        캐시 . 추가 ( 항목_ID , 항목_값 )    item_value 반환","get_item ( 항목_ID ):    item_value = 데이터베이스 . 쿼리 ( ""SELECT * FROM 항목 WHERE id = ?"" , item_id )    item_value 가 None 인 경우 :         item_value = 캐시 . 세트 ( 항목_ID , 항목_값 )        캐시 . 추가 ( 항목_ID , 항목_값 )    item_value 반환","get_item ( 항목_ID , 항목_값 ):    item_value = 데이터베이스 . 쿼리 ( ""id = ?"" , item_id , item_value ) 에서 항목 업데이트    캐시 . 추가 ( 항목_ID , 항목_값 )    '알았어' 로 돌아가",,,0,,
udemy,DVA-02,111,"A recently deployed Lambda function has an intermittent issue in processing customer data. You enabled the active tracing option in order to detect, analyze, and optimize performance issues of your function using the X-Ray service. Which of the following environment variables are used by AWS Lambda to facilitate communication with X-Ray? (Select TWO.)",BD,BD,AUTO_INSTRUMENT,_X_AMZN_TRACE_ID,AWS_XRAY_DEBUG_MODE,AWS_XRAY_CONTEXT_MISSING,AWS_XRAY_TRACING_NAME,,,"최근 배포된 Lambda 함수에 고객 데이터 처리 시 간헐적인 문제가 있습니다. X-Ray 서비스를 사용하여 함수의 성능 문제를 감지, 분석 및 최적화하기 위해 활성 추적 옵션을 활성화했습니다.다음 중 X-Ray와의 통신을 촉진하기 위해 AWS Lambda에서 사용되는 환경 변수는 무엇입니까? (2개를 선택하세요.)",AUTO_INSTRUMENT,_X_AMZN_TRACE_ID,AWS_XRAY_DEBUG_MODE,AWS_XRAY_CONTEXT_MISSING,AWS_XRAY_TRACING_NAME,,0,,
udemy,DVA-02,112,"A recruitment agency has a large collection of resumes stored in an Amazon S3 bucket. The agency wants to perform an analysis on these files, but for privacy compliance reasons, they need to ensure that certain personally identifiable information (PII) is redacted before being processed by their internal service.Which solution can meet the requirements in the most cost-effective way?",A,A,Use Amazon S3 Object Lambda to redact PII before it is returned to the application.,Implement a solution with AWS Glue to transform the data and redact PII before storing it in an S3 bucket.,"Use a Lambda function to create a redacted copy of each file in a separate S3 bucket. Then, set up an Amazon S3 Access Point to serve these files.",Configure an Amazon S3 Access Point and set up an Amazon CloudFront distribution with a Lambda@Edge function to redact the PII as data is fetched from the S3 bucket.,,,,채용 대행사는 Amazon S3 버킷에 저장된 대규모 이력서 컬렉션을 보유하고 있습니다. 기관에서는 이러한 파일에 대한 분석을 수행하려고 하지만 개인 정보 보호 규정 준수 이유로 내부 서비스에서 처리되기 전에 특정 개인 식별 정보(PII)가 수정되었는지 확인해야 합니다.가장 비용 효율적인 방법으로 요구 사항을 충족할 수 있는 솔루션은 무엇입니까?,Amazon S3 객체 Lambda를 사용하여 PII가 애플리케이션에 반환되기 전에 수정하십시오.,AWS Glue로 솔루션을 구현하여 데이터를 변환하고 PII를 수정한 후 S3 버킷에 저장하세요.,Lambda 함수를 사용하여 별도의 S3 버킷에 각 파일의 수정된 복사본을 생성합니다. 그런 다음 Amazon S3 액세스 포인트를 설정하여 이러한 파일을 제공합니다.,Amazon S3 액세스 포인트를 구성하고 Lambda@Edge 함수로 Amazon CloudFront 배포를 설정하여 S3 버킷에서 데이터를 가져올 때 PII를 수정합니다.,,,0,,
udemy,DVA-02,113,A Lambda function downloads the same 250 MB file between invocations and stores it in memory for processing. This leads to frequent timeouts and negatively impacts the performance of the serverless application.Which change should be made to resolve the issue most effectively?,C,C,Increase the memory allocation of the function.,Increase the timeout of the function.,Store the file in the /tmp directory of the execution context and reuse it on succeeding invocations.,Increase the ephemeral storage size of the function,,,,Lambda 함수는 호출 간에 동일한 250MB 파일을 다운로드하고 처리를 위해 메모리에 저장합니다. 이로 인해 시간 초과가 자주 발생하고 서버리스 애플리케이션의 성능에 부정적인 영향을 미칩니다.문제를 가장 효과적으로 해결하려면 어떤 변경을 해야 합니까?,함수의 메모리 할당을 늘립니다.,함수의 시간 초과를 늘립니다.,/tmp실행 컨텍스트의 디렉터리 에 파일을 저장 하고 후속 호출에서 다시 사용합니다.,함수의 임시 저장소 크기 늘리기,,,0,,
udemy,DVA-02,114,"A new IT policy requires you to trace all calls that your Node.js application sends to external HTTP web APIs as well as SQL database queries. You have to instrument your application, which is hosted in Elastic Beanstalk, in order to properly trace the calls via the X-Ray console. What should you do to comply with the given requirement?",C,C,Enable active tracing in the Elastic Beanstalk by including the healthcheckurl.config configuration file in the .ebextensions directory of your source code.,Create a Docker image that runs the X-Ray daemon.,Enable the X-Ray daemon by including the xray-daemon.config configuration file in the .ebextensions directory of your source code.,Use a user data script to run the daemon automatically.,,,,새로운 IT 정책에서는 Node.js 애플리케이션이 SQL 데이터베이스 쿼리뿐만 아니라 외부 HTTP 웹 API로 보내는 모든 호출을 추적해야 합니다. X-Ray 콘솔을 통해 호출을 적절하게 추적하려면 Elastic Beanstalk에서 호스팅되는 애플리케이션을 계측해야 합니다.주어진 요구 사항을 준수하려면 어떻게 해야 합니까?,소스 코드 디렉터리 healthcheckurl.config에 구성 파일을 포함하여 Elastic Beanstalk에서 활성 추적을 활성화합니다 ..ebextensions,X-Ray 데몬을 실행하는 Docker 이미지를 생성합니다.,소스 코드 디렉터리 xray-daemon.config에 구성 파일을 포함하여 X-Ray 데몬을 활성화합니다 ..ebextensions,사용자 데이터 스크립트를 사용하여 데몬을 자동으로 실행합니다.,,,0,,
udemy,DVA-02,115,"A developer is building an e-commerce application which will be hosted in an ECS Cluster. To minimize the number of instances in use, she must select a strategy which will place tasks based on the least available amount of CPU or memory. Which of the following task placement strategy should the developer implement?",D,D,spread,distinctInstance,random,binpack,,,,개발자가 ECS 클러스터에서 호스팅될 전자상거래 애플리케이션을 구축하고 있습니다. 사용 중인 인스턴스 수를 최소화하려면 사용 가능한 최소 CPU 또는 메모리 양을 기준으로 작업을 배치하는 전략을 선택해야 합니다.다음 중 개발자가 구현해야 하는 작업 배치 전략은 무엇입니까?,spread,distinctInstance,random,binpack,,,0,,
udemy,DVA-02,116,"A serverless application composed of Lambda, API Gateway, and DynamoDB has been running without issues for quite some time. As part of the IT compliance of the company, a developer was instructed to ensure that all of the new changes made to the items in DynamoDB are recorded and stored in another DynamoDB table in another region.In this scenario, which of the following is the MOST ideal way to comply with the requirements?",D,D,Enable DynamoDB Point-in-Time Recovery to automatically sync the two tables.,Set up DynamoDB Accelerator,Create an Amazon EventBridge rule that tracks table-level events in DynamoDB. Set a Lambda function as a rule target to process and save new changes to the other table.,Enable DynamoDB Streams and configure a Lambda function to process and save new changes to the other table.,,,,"Lambda, API Gateway, DynamoDB로 구성된 서버리스 애플리케이션은 꽤 오랫동안 문제 없이 실행되었습니다. 회사의 IT 규정 준수의 일환으로 개발자는 DynamoDB의 항목에 대한 모든 새로운 변경 사항이 다른 지역의 다른 DynamoDB 테이블에 기록되고 저장되는지 확인하라는 지시를 받았습니다.이 시나리오에서 다음 중 요구 사항을 준수하는 가장 이상적인 방법은 무엇입니까?",DynamoDB 특정 시점 복구를 활성화하여 두 테이블을 자동으로 동기화합니다.,DynamoDB 가속기 설정,DynamoDB에서 테이블 수준 이벤트를 추적하는 Amazon EventBridge 규칙을 생성합니다. Lambda 함수를 규칙 대상으로 설정하여 새로운 변경 사항을 처리하고 다른 테이블에 저장합니다.,DynamoDB 스트림을 활성화하고 Lambda 함수를 구성하여 새로운 변경 사항을 처리하고 다른 테이블에 저장합니다.,,,0,,
udemy,DVA-02,117,"A developer is working on an application which stores data to an Amazon DynamoDB table with the DynamoDB Streams feature enabled. He set up an event source mapping with DynamoDB Streams and AWS Lambda function to monitor any table changes then store the original data of the overwritten item in S3. When an item is updated, it should only send a copy of the item's previous value to an S3 bucket and maintain the new value in the DynamoDB table.Which StreamViewType is the MOST suitable one to use in the DynamoDB configuration to fulfill this scenario?",B,B,KEYS_ONLY,OLD_IMAGE,NEW_AND_OLD_IMAGES,NEW_IMAGE,,,,개발자는 DynamoDB 스트림 기능이 활성화된 Amazon DynamoDB 테이블에 데이터를 저장하는 애플리케이션을 작업하고 있습니다. 그는 DynamoDB Streams 및 AWS Lambda 함수를 사용하여 이벤트 소스 매핑을 설정하여 테이블 변경 사항을 모니터링한 다음 덮어쓴 항목의 원본 데이터를 S3에 저장합니다. 항목이 업데이트되면 항목의 이전 값 복사본만 S3 버킷으로 보내고 DynamoDB 테이블에 새 값을 유지해야 합니다.StreamViewType이 시나리오를 이행하기 위해 DynamoDB 구성에서 사용하기에 가장 적합한 것은 무엇 입니까?,KEYS_ONLY,OLD_IMAGE,NEW_AND_OLD_IMAGES,NEW_IMAGE,,,0,,
udemy,DVA-02,118,A developer needs to encrypt all objects being uploaded by their application to the S3 bucket to comply with the company's security policy. The bucket will use server-side encryption with Amazon S3-Managed encryption keys (SSE-S3) to encrypt the data using 256-bit Advanced Encryption Standard (AES-256) block cipher. Which of the following request headers should the developer use?,A,A,x-amz-server-side-encryption,x-amz-server-side-encryption-customer-algorithm,x-amz-server-side-encryption-customer-key,x-amz-server-side-encryption-customer-key-MD5,,,,개발자는 회사의 보안 정책을 준수하기 위해 애플리케이션에서 S3 버킷에 업로드하는 모든 객체를 암호화해야 합니다. 버킷은 Amazon S3 관리형 암호화 키(SSE-S3)와 함께 서버 측 암호화를 사용하여 256비트 고급 암호화 표준(AES-256) 블록 암호를 사용하여 데이터를 암호화합니다.다음 중 개발자는 어떤 요청 헤더를 사용해야 합니까?,x-amz-server-side-encryption,x-amz-server-side-encryption-customer-algorithm,x-amz-server-side-encryption-customer-key,x-amz-server-side-encryption-customer-key-MD5,,,0,,
udemy,DVA-02,119,"You have several API Gateway APIs with Lambda Integration for each release life cycle of your application. There is a requirement to consolidate multiple releases into a single API Gateway for the ALPHA, BETA, RC (Release Candidate), and PROD releases. For example, their clients can connect to their ALPHA release by using the alpha.tutorialsdojo.com endpoint and beta release through the beta.tutorialsdojo.com endpoint. As the AWS developer, how can you satisfy this requirement?",B,B,Modify the Integration Request of the API Gateway to manage different endpoints for each release.,Set up Stage Variables for each release.,Use Layers to the underlying Lambda functions of the API Gateway.,Modify the Integration Response of the API Gateway to add different endpoints for each release.,,,,"애플리케이션의 각 릴리스 수명 주기에 대해 Lambda 통합이 포함된 여러 API 게이트웨이 API가 있습니다. ALPHA, BETA, RC(릴리스 후보) 및 PROD 릴리스의 경우 여러 릴리스를 단일 API 게이트웨이로 통합해야 한다는 요구 사항이 있습니다. 예를 들어 클라이언트는 alpha.tutorialsdojo.com엔드포인트를 사용하여 ALPHA 릴리스에 연결하고 엔드포인트를 통해 베타 릴리스를 연결할 수 있습니다 beta.tutorialsdojo.com.AWS 개발자로서 이 요구 사항을 어떻게 충족할 수 있습니까?",각 릴리스에 대해 서로 다른 엔드포인트를 관리하려면 API 게이트웨이의 통합 요청을 수정하세요.,각 릴리스에 대한 단계 변수를 설정합니다.,API 게이트웨이의 기본 Lambda 함수에 레이어를 사용합니다.,각 릴리스에 대해 서로 다른 엔드포인트를 추가하려면 API 게이트웨이의 통합 응답을 수정하세요.,,,0,,
udemy,DVA-02,120,A technical manager needs permission to create new repositories and delete them in CodeCommit. This will enable her to manage all of the code repositories of each development teams and remove duplicate or unused ones. Which of the following permissions should be given in order to comply with the standard security advice of granting least privilege?,D,D,codecommit:*,codecommit:CreateBranch and codecommit:DeleteBranch,codecommit:GitPull and codecommit:GitPush,codecommit:CreateRepository and codecommit:DeleteRepository,,,,기술 관리자가 CodeCommit에서 새 리포지토리를 생성하고 삭제하려면 권한이 필요합니다. 이를 통해 그녀는 각 개발 팀의 모든 코드 저장소를 관리하고 중복되거나 사용되지 않는 코드 저장소를 제거할 수 있습니다.최소 권한 부여에 대한 표준 보안 조언을 준수하려면 다음 중 어떤 권한을 부여해야 합니까?,codecommit:*,codecommit:CreateBranch그리고codecommit:DeleteBranch,codecommit:GitPull그리고codecommit:GitPush,codecommit:CreateRepository그리고codecommit:DeleteRepository,,,0,,
udemy,DVA-02,121,"There have been reports that your application, which has a MySQL RDS database, becomes unresponsive from time to time. You were instructed to collect all SQL statements that took longer to execute for troubleshooting.What should you do to properly troubleshoot this issue with the LEAST amount of effort?",B,B,Instrument your application using the X-Ray SDK.,Enable slow query log in RDS.,Enable active tracing using AWS X-Ray.,Use Amazon Inspector to get all the slow queries.,,,,MySQL RDS 데이터베이스가 있는 애플리케이션이 때때로 응답하지 않는다는 보고가 있었습니다. 문제 해결을 위해 실행하는 데 시간이 오래 걸리는 모든 SQL 문을 수집하라는 지시를 받았습니다.최소한의 노력으로 이 문제를 올바르게 해결하려면 어떻게 해야 합니까?,X-Ray SDK를 사용하여 애플리케이션을 계측합니다.,RDS에서 느린 쿼리 로그를 활성화합니다.,AWS X-Ray를 사용하여 활성 추적을 활성화합니다.,느린 쿼리를 모두 얻으려면 Amazon Inspector를 사용하십시오.,,,0,,
udemy,DVA-02,122,A developer is planning to deploy a high-performance online trading application which requires a database that can scale globally and can handle frequent schema changes. The database should also support flexible schemas that enable faster and more iterative development. Which is the MOST suitable database service that you should use to achieve this requirement?,A,A,DynamoDB,RDS,Redshift,Aurora,,,,개발자는 전역적으로 확장할 수 있고 빈번한 스키마 변경을 처리할 수 있는 데이터베이스가 필요한 고성능 온라인 거래 애플리케이션을 배포할 계획입니다. 또한 데이터베이스는 보다 빠르고 반복적인 개발을 가능하게 하는 유연한 스키마를 지원해야 합니다.이 요구 사항을 달성하기 위해 사용해야 하는 가장 적합한 데이터베이스 서비스는 무엇입니까?,DynamoDB,RDS,적색편이,오로라,,,0,,
udemy,DVA-02,123,"A developer is building a photo-sharing application that automatically enhances images uploaded by users to Amazon S3. When a user uploads an image, its S3 path is sent to an image-processing application hosted on AWS Lambda. The Lambda function applies the selected filter to the image and stores it back to S3.If the upload is successful, the application will return a prompt telling the user that the request has been accepted. The entire processing typically takes an average of 5 minutes to complete, which causes the application to become unresponsive.Which of the following is the MOST suitable and cost-effective option which will prevent the application from being unresponsive?",B,B,Use a combination of Lambda and Step Functions to orchestrate service components and asynchronously process the requests.,Configure the application to asynchronously process the requests and change the invocation type of the Lambda function to Event.,Configure the application to asynchronously process the requests and use the default invocation type of the Lambda function.,Use AWS Serverless Application Model (AWS SAM) to allow asynchronous requests to your Lambda function.,,,,개발자는 사용자가 Amazon S3에 업로드한 이미지를 자동으로 향상시키는 사진 공유 애플리케이션을 구축하고 있습니다. 사용자가 이미지를 업로드하면 해당 S3 경로가 AWS Lambda에서 호스팅되는 이미지 처리 애플리케이션으로 전송됩니다. Lambda 함수는 선택한 필터를 이미지에 적용하고 이를 S3에 다시 저장합니다.업로드가 성공하면 애플리케이션은 사용자에게 요청이 수락되었음을 알리는 프롬프트를 반환합니다. 전체 처리를 완료하는 데 일반적으로 평균 5분이 걸리므로 애플리케이션이 응답하지 않게 됩니다.다음 중 애플리케이션이 응답하지 않는 것을 방지하는 가장 적합하고 비용 효율적인 옵션은 무엇입니까?,Lambda와 Step Functions의 조합을 사용하여 서비스 구성 요소를 조정하고 요청을 비동기식으로 처리합니다.,요청을 비동기식으로 처리하도록 애플리케이션을 구성하고 Lambda 함수의 호출 유형을 Event.,요청을 비동기식으로 처리하고 Lambda 함수의 기본 호출 유형을 사용하도록 애플리케이션을 구성합니다.,AWS Serverless Application Model(AWS SAM)을 사용하여 Lambda 함수에 대한 비동기 요청을 허용합니다.,,,0,,
udemy,DVA-02,124,"A web application is running in an ECS Cluster and updates data in DynamoDB several times a day. The clients retrieve data directly from the DynamoDB through APIs exposed by Amazon API Gateway. Although API caching is enabled, there are specific clients that want to retrieve the latest data from DynamoDB for every API request sent. What should be done to only allow authorized clients to invalidate an API Gateway cache entry when submitting API requests? (Select TWO.)",CD,CD,Modify the cache settings to retrieve the latest data from DynamoDB if the request header's authorization signature matches your API's trusted clients list.,Provide your clients an authorization token from STS to query data directly from DynamoDB.,The client must send a request which contains the Cache-Control: max-age=0 header.,Tick the Require Authorization checkbox in the Cache Settings of your API via the console.,The client must send a request which contains the Cache-Control: max-age=1 header.,,,웹 애플리케이션은 ECS 클러스터에서 실행되며 하루에 여러 번 DynamoDB의 데이터를 업데이트합니다. 클라이언트는 Amazon API Gateway에서 제공하는 API를 통해 DynamoDB에서 직접 데이터를 검색합니다. API 캐싱이 활성화되어 있더라도 전송된 모든 API 요청에 대해 DynamoDB에서 최신 데이터를 검색하려는 특정 클라이언트가 있습니다.API 요청을 제출할 때 승인된 클라이언트만 API 게이트웨이 캐시 항목을 무효화하도록 허용하려면 어떻게 해야 합니까? (2개를 선택하세요.),요청 헤더의 인증 서명이 API의 신뢰할 수 있는 클라이언트 목록과 일치하는 경우 DynamoDB에서 최신 데이터를 검색하도록 캐시 설정을 수정합니다.,DynamoDB에서 직접 데이터를 쿼리하려면 클라이언트에 STS의 인증 토큰을 제공하세요.,클라이언트는 헤더가 포함된 요청을 보내야 합니다 Cache-Control: max-age=0.,Require Authorization콘솔을 통해 API의 캐시 설정에 있는 확인란을 선택합니다 .,클라이언트는 헤더가 포함된 요청을 보내야 합니다 Cache-Control: max-age=1.,,0,,
udemy,DVA-02,125,"A developer configured an Amazon API Gateway proxy integration named MyAPI to work with a Lambda function. However, when the API is being called, the developer receives a 502 Bad Gateway error. She tried invoking the underlying function, but it properly returned the result in XML format.What is the MOST likely root cause of this issue?",D,D,The endpoint request timed-out.,The API name of the Amazon API Gateway proxy is invalid.,There has been an occasional out-of-order invocation due to heavy loads.,There is an incompatible output returned from a Lambda proxy integration backend.,,,,개발자는 MyAPILambda 함수와 작동하도록 명명된 Amazon API Gateway 프록시 통합을 구성했습니다. 그러나 API가 호출되면 개발자는 502 Bad Gateway오류를 수신합니다. 그녀는 기본 함수를 호출하려고 시도했지만 결과가 XML 형식으로 제대로 반환되었습니다.이 문제의 가장 가능성 있는 근본 원인은 무엇입니까?,엔드포인트 요청 시간이 초과되었습니다.,Amazon API Gateway 프록시의 API 이름이 잘못되었습니다.,로드가 너무 많아서 가끔 호출 순서가 잘못되었습니다.,Lambda 프록시 통합 백엔드에서 호환되지 않는 출력이 반환되었습니다.,,,0,,
udemy,DVA-02,126,A developer has an application that uses a Lambda function to process data from an Aurora MySQL DB Instance in a Virtual Private Cloud (VPC). The database throws a MySQL: ERROR 1040: Too many connections error whenever there is a surge in incoming traffic.Which is the most suitable solution for resolving the issue?,C,C,Increase the concurrency limit of the Lambda function,Increase the value of the max_connections parameter of the Aurora MySQL DB Instance.,Provision an RDS Proxy between the Lambda function and the RDS database instance,Increase the allocated memory of your function.,,,,개발자는 Lambda 함수를 사용하여 Virtual Private Cloud(VPC)에 있는 Aurora MySQL DB 인스턴스의 데이터를 처리하는 애플리케이션을 보유하고 있습니다. MySQL: ERROR 1040: Too many connections들어오는 트래픽이 급증할 때마다 데이터베이스에서 오류가 발생합니다 .문제 해결에 가장 적합한 솔루션은 무엇입니까?,Lambda 함수의 동시성 제한 늘리기,max_connectionsAurora MySQL DB 인스턴스의 파라미터 값을 늘립니다 .,Lambda 함수와 RDS 데이터베이스 인스턴스 간에 RDS Proxy 프로비저닝,함수에 할당된 메모리를 늘리세요.,,,0,,
udemy,DVA-02,127,"You want to update a Lambda function on your production environment and ensure that when you publish the updated version, you still have a quick way to roll back to the older version in case you encountered a problem. To prevent any sudden user interruptions, you want to gradually increase the traffic going to the new version. Which of the following implementation is the BEST option to use?",B,B,Use ELB to route traffic to both Lambda functions.,Use Traffic Shifting with Lambda Aliases.,Use Route 53 weighted routing to two Lambda functions.,Use stage variables in your Lambda function.,,,,프로덕션 환경에서 Lambda 함수를 업데이트하고 업데이트된 버전을 게시할 때 문제가 발생할 경우 이전 버전으로 롤백할 수 있는 빠른 방법이 있는지 확인하려고 합니다. 갑작스러운 사용자 중단을 방지하려면 새 버전으로 이동하는 트래픽을 점진적으로 늘리려고 합니다.다음 구현 중 사용하기에 가장 좋은 옵션은 무엇입니까?,ELB를 사용하여 트래픽을 두 Lambda 함수로 라우팅합니다.,Lambda 별칭과 함께 트래픽 이동을 사용합니다.,두 개의 Lambda 함수에 Route 53 가중치 라우팅을 사용합니다.,Lambda 함수에서 단계 변수를 사용합니다.,,,0,,
udemy,DVA-02,128,You are developing a Lambda function which processes event notifications from Amazon S3. It is expected that the function will have: - 50 requests per second - 100 seconds to complete each request What should you do to prevent any issues when the function has been deployed and becomes operational?,B,B,Implement exponential backoff in your application.,Request for AWS to increase the limit of your concurrent executions.,No additional action needed since Lambda will automatically scale based on the incoming requests.,Increase the concurrency limit of the function.,,,,Amazon S3의 이벤트 알림을 처리하는 Lambda 함수를 개발 중입니다. 이 기능은 다음을 가질 것으로 예상됩니다.- 초당 50개 요청- 각 요청을 완료하는 데 100초기능이 배포되고 운영될 때 문제를 방지하려면 어떻게 해야 합니까?,애플리케이션에 지수 백오프를 구현합니다.,동시 실행 한도를 늘려달라고 AWS에 요청합니다.,Lambda는 들어오는 요청에 따라 자동으로 크기를 조정하므로 추가 작업이 필요하지 않습니다.,함수의 동시성 제한을 늘립니다.,,,0,,
udemy,DVA-02,129,"A programmer is developing a shell script that uses AWS CLI to list all objects of a given bucket. However, the script is timing out if the bucket has tens of thousands of objects.Which solution would most likely rectify the issue?",C,C,Enable CORS,Use S3 Select,Add pagination parameters in the AWS CLI command,Enable Amazon S3 Transfer Acceleration,,,,프로그래머는 AWS CLI를 사용하여 특정 버킷의 모든 객체를 나열하는 셸 스크립트를 개발하고 있습니다. 그러나 버킷에 수만 개의 객체가 있으면 스크립트가 시간 초과됩니다.어떤 솔루션이 문제를 해결할 가능성이 가장 높습니까?,CORS 활성화,S3 선택 사용,AWS CLI 명령에 페이지 매김 매개변수 추가,Amazon S3 Transfer Acceleration 활성화,,,0,,
udemy,DVA-02,130,"A website hosted in AWS has a custom CloudWatch metric to track all HTTP server errors in the site every minute, which occurs intermittently. An existing CloudWatch Alarm has already been configured for this metric but you would like to re-configure this to properly monitor the application. The alarm should only be triggered when all three data points in the most recent three consecutive periods are above the threshold.Which of the following options is the MOST appropriate way to monitor the website based on the given threshold?",C,C,Use high-resolution metrics.,Use metric math in CloudWatch to properly compute the threshold.,Set both the Evaluation Period and Datapoints to Alarm to 3.,Set both the Period and Datapoints to Alarm to 3.,,,,AWS에서 호스팅되는 웹 사이트에는 간헐적으로 발생하는 사이트의 모든 HTTP 서버 오류를 매분 추적하는 사용자 지정 CloudWatch 지표가 있습니다. 이 지표에 대해 기존 CloudWatch 경보가 이미 구성되어 있지만 애플리케이션을 적절하게 모니터링하도록 이를 재구성하고 싶습니다. 가장 최근 연속 3개 기간의 데이터 포인트 3개가 모두 임계값을 초과하는 경우에만 경보가 트리거되어야 합니다.다음 옵션 중 주어진 임계값을 기준으로 웹사이트를 모니터링하는 가장 적절한 방법은 무엇입니까?,고해상도 측정항목을 사용하세요.,CloudWatch에서 지표 수학을 사용하여 임계값을 적절하게 계산합니다.,Evaluation Period및 을 모두 Datapoints to Alarm3으로 설정합니다.,Period및 을 모두 Datapoints to Alarm3으로 설정합니다.,,,0,,
udemy,DVA-02,131,"In order to quickly troubleshoot their systems, your manager instructed you to record the calls that your application makes to all AWS services and resources. You developed a custom code that will send the segment documents directly to X-Ray by using the PutTraceSegments API. What should you include in your segment document to meet the above requirement?",C,C,metadata,tracing header,subsegments,annotations,,,,시스템 문제를 신속하게 해결하기 위해 관리자는 애플리케이션이 모든 AWS 서비스 및 리소스에 대해 수행하는 호출을 기록하도록 지시했습니다. API 를 사용하여 세그먼트 문서를 X-Ray로 직접 보내는 사용자 지정 코드를 개발했습니다 PutTraceSegments.위의 요구 사항을 충족하려면 세그먼트 문서에 무엇을 포함해야 합니까?,metadata,추적 헤더,하위 세그먼트,주석,,,0,,
udemy,DVA-02,132,You are developing a serverless application in AWS in which you have to control the code execution performance and costs of your Lambda functions. There is a requirement to increase the CPU available to your function in order to efficiently process records from an Amazon Kinesis data stream. Which of the following is the BEST way to meet this requirement?,B,B,Use Lambda@Edge.,Increase the allocated memory of the function.,Configure the function to use unreserved account concurrency.,Increase the concurrent execution limit of the function.,,,,귀하는 코드 실행 성능과 Lambda 함수의 비용을 제어해야 하는 서버리스 애플리케이션을 AWS에서 개발하고 있습니다. Amazon Kinesis 데이터 스트림의 레코드를 효율적으로 처리하려면 함수에 사용 가능한 CPU를 늘려야 합니다.다음 중 이 요구 사항을 충족하는 가장 좋은 방법은 무엇입니까?,Lambda@Edge를 사용하세요.,함수에 할당된 메모리를 늘립니다.,예약되지 않은 계정 동시성을 사용하도록 기능을 구성합니다.,함수의 동시 실행 제한을 늘립니다.,,,0,,
udemy,DVA-02,133,A developer is building the cloud architecture of an application which will be hosted in a large EC2 instance. The application will process the data and it will upload results to an S3 bucket. Which of the following is the SAFEST way to implement this architecture?,D,D,Store the access keys in the instance then use the AWS SDK to upload the results to S3.,Use an IAM Inline Policy to grant the application the necessary permissions to upload data to S3.,Install the AWS CLI then use it to upload the results to S3.,Use an IAM Role to grant the application the necessary permissions to upload data to S3.,,,,개발자가 대규모 EC2 인스턴스에서 호스팅될 애플리케이션의 클라우드 아키텍처를 구축하고 있습니다. 애플리케이션은 데이터를 처리하고 결과를 S3 버킷에 업로드합니다.다음 중 이 아키텍처를 구현하는 가장 안전한 방법은 무엇입니까?,인스턴스에 액세스 키를 저장한 다음 AWS SDK를 사용하여 결과를 S3에 업로드합니다.,IAM 인라인 정책을 사용하여 S3에 데이터를 업로드하는 데 필요한 권한을 애플리케이션에 부여합니다.,AWS CLI를 설치한 후 이를 사용하여 결과를 S3에 업로드합니다.,IAM 역할을 사용하여 S3에 데이터를 업로드하는 데 필요한 권한을 애플리케이션에 부여합니다.,,,0,,
udemy,DVA-02,134,"Your manager assigned you a task of implementing server-side encryption with customer-provided encryption keys (SSE-C) to your S3 bucket, which will allow you to set your own encryption keys. Amazon S3 will manage both the encryption and decryption process using your key when you access your objects, which will remove the burden of maintaining any code to perform data encryption and decryption. To properly upload data to this bucket, which of the following headers must be included in your request?",C,C,"x-amz-server-side-encryption, x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers",x-amz-server-side-encryption and x-amz-server-side-encryption-aws-kms-key-id headers,"x-amz-server-side​-encryption​-customer-algorithm, x-amz-server-side-encryption-customer-key and x-amz-server-side-encryption-customer-key-MD5 headers",x-amz-server-side-encryption-customer-key header only,,,,관리자는 고객 제공 암호화 키(SSE-C)를 사용하여 S3 버킷에 서버 측 암호화를 구현하는 작업을 할당했습니다. 이를 통해 사용자는 자신의 암호화 키를 설정할 수 있습니다. Amazon S3는 객체에 액세스할 때 키를 사용하여 암호화 및 암호 해독 프로세스를 모두 관리하므로 데이터 암호화 및 암호 해독을 수행하기 위해 코드를 유지 관리해야 하는 부담을 덜어줍니다.이 버킷에 데이터를 올바르게 업로드하려면 다음 중 요청에 포함되어야 하는 헤더는 무엇입니까?,x-amz-server-side-encryption및 헤더 x-amz-server-side-encryption-customer-key_x-amz-server-side-encryption-customer-key-MD5,x-amz-server-side-encryption및 x-amz-server-side-encryption-aws-kms-key-id헤더,x-amz-server-side​-encryption​-customer-algorithm및 헤더 x-amz-server-side-encryption-customer-key_x-amz-server-side-encryption-customer-key-MD5,x-amz-server-side-encryption-customer-key헤더만,,,0,,
udemy,DVA-02,135,"A developer wants to use multi-factor authentication (MFA) to protect programmatic calls to specific AWS API operations like Amazon EC2 StopInstances. He needs to call an API where he can submit the MFA code that is associated with his MFA device. Using the temporary security credentials that are returned from the call, he can then make programmatic calls to API operations that require MFA authentication. Which API should the developer use to properly implement this security feature?",D,D,AssumeRoleWithSAML,GetFederationToken,AssumeRoleWithWebIdentity,GetSessionToken,,,,개발자는 멀티 팩터 인증(MFA)을 사용하여 Amazon EC2 StopInstances와 같은 특정 AWS API 작업에 대한 프로그래밍 방식 호출을 보호하려고 합니다. 그는 자신의 MFA 디바이스와 연결된 MFA 코드를 제출할 수 있는 API를 호출해야 합니다. 호출에서 반환된 임시 보안 자격 증명을 사용하여 MFA 인증이 필요한 API 작업을 프로그래밍 방식으로 호출할 수 있습니다.이 보안 기능을 올바르게 구현하려면 개발자가 어떤 API를 사용해야 합니까?,AssumeRoleWithSAML,GetFederationToken,AssumeRoleWithWebIdentity,GetSessionToken,,,0,,
udemy,DVA-02,136,"An online forum requires a new table in DynamoDB named Thread in which the partition key is ForumName and the sort key is Subject. The following diagram shows how the items in the table would be organized: For reporting purposes, the application needs to find all of the threads that have been posted in a particular forum within the last three months. Which of the following is the MOST effective solution that you should implement?",A,A,Add a local secondary index while creating the new Thread table. Use the Query operation to utilize the LastPostDateTime attribute as the sort key.,Create a global secondary index and use the Query operation to utilize the LastPostDateTime attribute as the sort key.,Configure the application to Query the entire Thread table and discard any posts that were not within the specified time frame.,Configure the application to Scan the entire Thread table and discard any posts that were not within the specified time frame.,,,,Thread온라인 포럼에는 파티션 키가 ForumName이고 정렬 키가 인 DynamoDB의 새 테이블이 필요합니다 Subject. 다음 다이어그램은 테이블의 항목이 구성되는 방식을 보여줍니다.보고 목적으로 애플리케이션은 지난 3개월 이내에  특정 포럼에 게시된 모든 스레드를 찾아야 합니다 .  다음 중 구현해야 할 가장 효과적인 솔루션은 무엇입니까?,새 Thread 테이블을 생성하는 동안 로컬 보조 인덱스를 추가합니다. LastPostDateTime 속성을 정렬 키로 활용하려면 쿼리 작업을 사용하세요.,글로벌 보조 인덱스를 생성하고 쿼리 작업을 통해 LastPostDateTime 속성을 정렬 키로 활용합니다.,전체 Thread 테이블을 쿼리하고 지정된 시간 범위 내에 있지 않은 게시물을 삭제하도록 애플리케이션을 구성합니다.,전체 스레드 테이블을 스캔하고 지정된 시간 범위 내에 있지 않은 모든 게시물을 삭제하도록 애플리케이션을 구성합니다.,,,0,,
udemy,DVA-02,137,"A developer monitors multiple sensors inside a data center which detects various environmental conditions that may affect their running servers. In the current architecture, the data is initially processed by an AWS Lambda function and then stored in a remote data warehouse. To make the system more durable and scalable, the developer plans to use an Amazon SQS FIFO queue to store the data, which will be polled by the Lambda function. There is a known issue with the sensor devices sending duplicate data intermittently.What action can the developer take to lessen the chances of processing duplicate messages?",C,C,Refactor the Lambda function to store the message's content and drop the incoming messages with similar content within a 5-minute period.,Configure the Amazon SQS queue to automatically drop a duplicate message whenever it arrives within the message's VisibilityTimeout.,Add a MessageDeduplicationId parameter to the SendMessage API request.,Use an Amazon SQS Standard queue instead of a FIFO queue to avoid any duplicate messages.,,,,개발자는 실행 중인 서버에 영향을 줄 수 있는 다양한 환경 조건을 감지하는 데이터 센터 내부의 여러 센서를 모니터링합니다. 현재 아키텍처에서는 데이터가 처음에 AWS Lambda 함수에 의해 처리된 후 원격 데이터 웨어하우스에 저장됩니다. 시스템의 내구성과 확장성을 높이기 위해 개발자는 Amazon SQS FIFO 대기열을 사용하여 Lambda 함수에 의해 폴링되는 데이터를 저장할 계획입니다. 중복 데이터를 간헐적으로 전송하는 센서 장치에 알려진 문제가 있습니다.중복 메시지 처리 가능성을 줄이기 위해 개발자는 어떤 조치를 취할 수 있나요?,Lambda 함수를 리팩터링하여 메시지 콘텐츠를 저장하고 5분 이내에 유사한 콘텐츠가 포함된 수신 메시지를 삭제합니다.,중복 메시지가 메시지 내에 도착할 때마다 자동으로 삭제하도록 Amazon SQS 대기열을 구성합니다 VisibilityTimeout.,MessageDeduplicationIdAPI 요청 에 매개변수를 추가합니다 SendMessage.,중복 메시지를 방지하려면 FIFO 대기열 대신 Amazon SQS 표준 대기열을 사용하십시오.,,,0,,
udemy,DVA-02,138,"Due to the popularity of serverless computing, your manager instructed you to share your technical expertise to the whole software development department of your company. You are planning to deploy a simple Node.js 'Hello World' Lambda function to AWS using CloudFormation. Which of the following is the EASIEST way of deploying the function to AWS?",D,D,Upload the code in S3 then specify the S3Key and S3Bucket parameters under the AWS::Lambda::Function resource in the CloudFormation template.,Include your function source inline in the Code parameter of the AWS::Lambda::Function resource in the CloudFormation template.,Upload the code in S3 as a ZIP file then specify the S3 path in the ZipFile parameter of the AWS::Lambda::Function resource in the CloudFormation template.,Include your function source inline in the ZipFile parameter of the AWS::Lambda::Function resource in the CloudFormation template.,,,,서버리스 컴퓨팅의 인기로 인해 관리자는 기술 전문 지식을 회사의 전체 소프트웨어 개발 부서와 공유하도록 지시했습니다. CloudFormation을 사용하여 간단한 Node.js 'Hello World' Lambda 함수를 AWS에 배포할 계획입니다.다음 중 AWS에 함수를 배포하는 가장 쉬운 방법은 무엇입니까?,S3에 코드를 업로드한 다음 CloudFormation 템플릿의 리소스 아래에 S3Key및 매개변수를 지정합니다.S3BucketAWS::Lambda::Function,CloudFormation 템플릿의 리소스 Code매개변수 에 함수 소스 인라인을 포함합니다 .AWS::Lambda::Function,S3의 코드를 ZIP 파일로 업로드한 다음 CloudFormation 템플릿의 리소스 ZipFile매개변수 에 S3 경로를 지정합니다 .AWS::Lambda::Function,CloudFormation 템플릿의 리소스 ZipFile매개변수 에 함수 소스 인라인을 포함합니다 .AWS::Lambda::Function,,,0,,
udemy,DVA-02,139,"To accommodate a new application deployment, you have created a new EBS volume to be attached to your EC2 instance. After attaching the newly created EBS volume to the Linux EC2 instance, which of the following steps are you going to do next in order to use this volume?",B,B,Mount the volume since it already has a pre-configured file system.,Create a file system on this volume.,Assign a file system on this volume using the AWS Console.,No action needed. AWS automatically configures the EBS volume for use on your instance.,,,,새로운 애플리케이션 배포를 수용하기 위해 EC2 인스턴스에 연결할 새 EBS 볼륨을 생성했습니다. 새로 생성된 EBS 볼륨을 Linux EC2 인스턴스에 연결한 후 이 볼륨을 사용하기 위해 다음 단계 중 어떤 단계를 수행하시겠습니까?,볼륨에 이미 사전 구성된 파일 시스템이 있으므로 볼륨을 마운트합니다.,이 볼륨에 파일 시스템을 생성합니다.,AWS 콘솔을 사용하여 이 볼륨에 파일 시스템을 할당합니다.,조치가 필요하지 않습니다. AWS는 인스턴스에서 사용할 EBS 볼륨을 자동으로 구성합니다.,,,0,,
udemy,DVA-02,140,A developer is using API Gateway Lambda Authorizer to provide authentication for every API request and control access to your API. The requirement is to implement an authentication strategy which is similar to OAuth or SAML. Which of the following is the MOST suitable method that the developer should use in this scenario?,B,B,Cross-Account Lambda Authorizer,Token-based Authorization,AWS STS-based Authentication,Request Parameter-based Authorization,,,,개발자는 API Gateway Lambda Authorizer를 사용하여 모든 API 요청에 대한 인증을 제공하고 API에 대한 액세스를 제어합니다. 요구 사항은 OAuth 또는 SAML과 유사한 인증 전략을 구현하는 것입니다.다음 중 이 시나리오에서 개발자가 사용해야 하는 가장 적합한 방법은 무엇입니까?,교차 계정 Lambda 권한 부여자,토큰 기반 인증,AWS STS 기반 인증,매개변수 기반 승인 요청,,,0,,
udemy,DVA-02,141,"A leading commercial bank has an online banking portal that is hosted in an Auto Scaling group of EC2 instances with an Application Load Balancer in front to distribute the incoming traffic. The application has been instrumented, and the X-Ray daemon has been installed in all instances to allow debugging and troubleshooting using AWS X-Ray.In this architecture, from which source will AWS X-Ray fetch the client IP address?",A,A,From the X-Forwarded-For header of the request.,From the ipAddress query parameter of the request if it exists.,From the source IP of the IP packet.,From the X-Forwarded-Host header of the request.,,,,한 주요 상업 은행에는 수신 트래픽을 분산하기 위해 Application Load Balancer가 앞에 있는 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅되는 온라인 뱅킹 포털이 있습니다. AWS X-Ray를 사용하여 디버깅하고 문제를 해결할 수 있도록 애플리케이션이 계측되었으며 X-Ray 데몬이 모든 인스턴스에 설치되었습니다.이 아키텍처에서 AWS X-Ray는 어떤 소스에서 클라이언트 IP 주소를 가져옵니까?,X-Forwarded-For요청 헤더 에서 .,ipAddress요청의 쿼리 매개변수(존재하는 경우) 에서 가져 옵니다.,IP 패킷의 소스 IP에서.,X-Forwarded-Host요청 헤더 에서 .,,,0,,
udemy,DVA-02,142,A company has 5 different applications running on several On-Demand EC2 instances. The DevOps team is required to set up a graphical representation of the key performance metrics for each application. These system metrics must be available on a single shared screen for more effective and visible monitoring.Which of the following should the DevOps team do to satisfy this requirement using Amazon CloudWatch?,D,D,Set up a custom CloudWatch Alarm with a unique metric name for each application.,Set up a custom CloudWatch dimension with a unique metric name for each application.,Set up a custom CloudWatch Event with a unique metric name for each application.,Set up a custom CloudWatch namespace with a unique metric name for each application.,,,,회사에는 여러 온디맨드 EC2 인스턴스에서 실행되는 5개의 서로 다른 애플리케이션이 있습니다. DevOps 팀은 각 애플리케이션의 주요 성능 지표를 그래픽으로 표현하도록 설정해야 합니다. 보다 효과적이고 가시적인 모니터링을 위해서는 이러한 시스템 지표를 단일 공유 화면에서 사용할 수 있어야 합니다.Amazon CloudWatch를 사용하여 이 요구 사항을 충족하려면 DevOps 팀이 다음 중 무엇을 수행해야 합니까?,각 애플리케이션에 대해 고유한 지표 이름을 사용하여 사용자 지정 CloudWatch 경보를 설정합니다.,각 애플리케이션에 대한 고유한 지표 이름을 사용하여 사용자 지정 CloudWatch 차원을 설정합니다.,각 애플리케이션에 대해 고유한 지표 이름을 사용하여 사용자 지정 CloudWatch 이벤트를 설정합니다.,각 애플리케이션에 대해 고유한 지표 이름을 사용하여 사용자 지정 CloudWatch 네임스페이스를 설정합니다.,,,0,,
udemy,DVA-02,143,"You are developing an application that will use a Lambda function, which will be invoked asynchronously. The application will be implemented with exponential back-off that will handle failures so that the requests will be retried twice before the event is discarded. If the retries fail with an unexpected error, you have to direct unprocessed events to another service which will analyze the failure. Which of the following is the MOST suitable component that you should implement in the application architecture to meet the above requirement?",D,D,FIFO Queue,Delay Queue,Amazon MQ,Dead Letter Queue,,,,비동기식으로 호출되는 Lambda 함수를 사용할 애플리케이션을 개발 중입니다. 애플리케이션은 이벤트가 삭제되기 전에 요청이 두 번 재시도되도록 실패를 처리하는 지수 백오프로 구현됩니다. 예기치 않은 오류로 인해 재시도가 실패하는 경우 처리되지 않은 이벤트를 실패를 분석할 다른 서비스로 보내야 합니다.다음 중 위의 요구 사항을 충족하기 위해 애플리케이션 아키텍처에 구현해야 하는 가장 적합한 구성 요소는 무엇입니까?,FIFO 대기열,지연 대기열,아마존 MQ,배달 못한 편지 대기열,,,0,,
udemy,DVA-02,144,"You have an application that reads an individual item from a DynamoDB table, modifies it locally, and submits the changes as a new entry to a separate table before proceeding onto the next item. The process is repeated for the next 100 entries, and it consumes a lot of time performing this entire process. Which strategy can be applied to your application in order to shorten the time needed to process all the necessary entries with MINIMAL configuration?",D,D,Deploy your application into a cluster of EC2 instances.,Modify your application to use multithreading.,Use DynamoDB conditional writes.,Use DynamoDB's BatchGetItem and BatchWriteItem API operations.,,,,"DynamoDB 테이블에서 개별 항목을 읽고, 이를 로컬로 수정하고, 다음 항목으로 진행하기 전에 변경 사항을 별도의 테이블에 새 항목으로 제출하는 애플리케이션이 있습니다. 이 프로세스는 다음 100개 항목에 대해 반복되며 이 전체 프로세스를 수행하는 데 많은 시간이 소비됩니다.최소 구성으로 필요한 모든 항목을 처리하는 데 필요한 시간을 단축하기 위해 애플리케이션에 어떤 전략을 적용할 수 있습니까?",EC2 인스턴스 클러스터에 애플리케이션을 배포합니다.,멀티스레딩을 사용하도록 애플리케이션을 수정하십시오.,DynamoDB 조건부 쓰기를 사용합니다.,DynamoDB BatchGetItem및 BatchWriteItemAPI 작업을 사용합니다.,,,0,,
udemy,DVA-02,145,Your development team is currently developing a financial application in AWS. One of the requirements is to create and control the encryption keys used to encrypt your data using the envelope encryption strategy to comply with the strict IT security policy of the company. Which of the following correctly describes the process of envelope encryption?,A,A,Encrypt plaintext data with a data key and then encrypt the data key with a top-level plaintext master key.,Encrypt plaintext data with a data key and then encrypt the data key with a top-level encrypted master key.,Encrypt plaintext data with a master key and then encrypt the master key with a top-level plaintext data key.,Encrypt plaintext data with a master key and then encrypt the master key with a top-level encrypted data key.,,,,귀하의 개발 팀은 현재 AWS에서 금융 애플리케이션을 개발하고 있습니다. 요구 사항 중 하나는 회사의 엄격한 IT 보안 정책을 준수하기 위해 봉투 암호화 전략을 사용하여 데이터를 암호화하는 데 사용되는 암호화 키를 생성하고 제어하는 ​​것입니다.다음 중 봉투 암호화 과정을 올바르게 설명한 것은 무엇입니까?,데이터 키로 일반 텍스트 데이터를 암호화한 다음 최상위 일반 텍스트 마스터 키로 데이터 키를 암호화합니다.,데이터 키로 일반 텍스트 데이터를 암호화한 다음 최상위 암호화 마스터 키로 데이터 키를 암호화합니다.,마스터 키로 일반 텍스트 데이터를 암호화한 다음 최상위 일반 텍스트 데이터 키로 마스터 키를 암호화합니다.,마스터 키로 일반 텍스트 데이터를 암호화한 다음 최상위 암호화 데이터 키로 마스터 키를 암호화합니다.,,,0,,
udemy,DVA-02,146,"A company has a central data repository in Amazon S3 that needs to be accessed by developers belonging to different AWS accounts. The required IAM role has been created with the appropriate S3 permissions.Given that the developers mostly interact with S3 via APIs, which API should the developers call to use the IAM role?",C,C,AssumeRoleWithWebIdentity,AssumeRoleWithSAML,AssumeRole,GetSessionToken,,,,회사에는 Amazon S3에 다른 AWS 계정에 속한 개발자가 액세스해야 하는 중앙 데이터 저장소가 있습니다. 적절한 S3 권한을 사용하여 필요한 IAM 역할이 생성되었습니다.개발자가 주로 API를 통해 S3와 상호 작용한다는 점을 고려하면 개발자는 IAM 역할을 사용하기 위해 어떤 API를 호출해야 합니까?,AssumeRoleWithWebIdentity,AssumeRoleWithSAML,AssumeRole,GetSessionToken,,,0,,
udemy,DVA-02,147,"A developer is building an application that will be hosted in ECS and must be configured to run tasks and services using the Fargate launch type. The application will have four different tasks, each of which will access different AWS resources than the others.Which of the following is the MOST efficient solution that can provide your application in ECS access to the required AWS resources?",A,A,Create 4 different IAM Roles with the required permissions and attach them to each of the 4 ECS tasks.,Create 4 different Service-Linked Roles with the required permissions and attach them to each of the 4 ECS tasks.,Create an IAM Group with all the required permissions and attach them to each of the 4 ECS tasks.,Create 4 different Container Instance IAM Roles with the required permissions and attach them to each of the 4 ECS tasks.,,,,"개발자는 ECS에서 호스팅될 애플리케이션을 구축 중이며 Fargate 시작 유형을 사용하여 작업과 서비스를 실행하도록 구성해야 합니다. 애플리케이션에는 네 가지 작업이 있으며, 각 작업은 다른 작업과 다른 AWS 리소스에 액세스합니다.다음 중 ECS의 애플리케이션에 필요한 AWS 리소스에 대한 액세스를 제공할 수 있는 가장 효율적인 솔루션은 무엇입니까?",필요한 권한이 있는 4개의 서로 다른 IAM 역할을 생성하고 이를 4개의 ECS 작업 각각에 연결합니다.,필요한 권한이 있는 4개의 서로 다른 서비스 연결 역할을 생성하고 이를 4개의 ECS 작업 각각에 연결합니다.,필요한 모든 권한이 있는 IAM 그룹을 생성하고 이를 4개의 ECS 작업 각각에 연결합니다.,필요한 권한이 있는 4개의 서로 다른 컨테이너 인스턴스 IAM 역할을 생성하고 이를 4개의 ECS 작업 각각에 연결합니다.,,,0,,
udemy,DVA-02,148,"You have an Amazon Kinesis data stream which has 20 open shards and 5 EC2 instances running as Kinesis Client Library (KCL) workers. Upon monitoring the metrics of your application in CloudWatch, it shows that your EC2 workers are consuming all of their CPU resources when processing the shards. Which of the following is the BEST solution to optimize your application and prevent workers from maxing out their CPU resources?",C,C,Launch an additional 20 instances,Launch an additional 25 instances,Launch an additional 15 instances,Decrease the number of shards that each instance processes,,,,오픈 샤드 20개와 ​​KCL(Kinesis Client Library) 작업자로 실행되는 EC2 인스턴스 5개가 있는 Amazon Kinesis 데이터 스트림이 있습니다. CloudWatch에서 애플리케이션 지표를 모니터링하면 EC2 작업자가 샤드를 처리할 때 CPU 리소스를 모두 소비하고 있음을 알 수 있습니다.다음 중 애플리케이션을 최적화하고 작업자가 CPU 리소스를 최대화하는 것을 방지하는 가장 좋은 솔루션은 무엇입니까?,추가로 20개의 인스턴스 시작,추가로 25개의 인스턴스 시작,추가로 15개의 인스턴스 시작,각 인스턴스가 처리하는 샤드 수 감소,,,0,,
udemy,DVA-02,149,"A company has a website hosted in a multicontainer Docker environment in Elastic Beanstalk. There is a requirement to integrate the website with API Gateway, where it simply passes client-submitted method requests to the backend. It is important that the client and backend interact directly with no intervention from API Gateway after the API method is set up, except for known issues such as unsupported characters. Which of the following integration types is the MOST suitable one to use to meet this requirement?",D,D,AWS_PROXY,AWS,HTTP,HTTP_PROXY,,,,회사에는 Elastic Beanstalk의 다중 컨테이너 Docker 환경에서 호스팅되는 웹 사이트가 있습니다. 클라이언트가 제출한 메서드 요청을 백엔드에 전달하는 API 게이트웨이와 웹사이트를 통합해야 한다는 요구 사항이 있습니다. 지원되지 않는 문자와 같은 알려진 문제를 제외하고 API 메서드가 설정된 후 API 게이트웨이의 개입 없이 클라이언트와 백엔드가 직접 상호 작용하는 것이 중요합니다.다음 중 이 요구 사항을 충족하는 데 사용하기에 가장 적합한 통합 유형은 무엇입니까?,AWS_PROXY,AWS,HTTP,HTTP_PROXY,,,0,,
udemy,DVA-02,150,"A company is heavily using a range of AWS services to host their enterprise applications. Currently, their deployment process still has a lot of manual steps which is why they plan to automate their software delivery process using continuous integration and delivery (CI/CD) pipelines in AWS. They will use CodePipeline to orchestrate each step of their release process and CodeDeploy for deploying applications to various compute platforms in AWS. In this architecture, which of the following are valid considerations when using CodeDeploy? (Select TWO.)",AC,AC,CodeDeploy can deploy applications to both your EC2 instances as well as your on-premises servers.,You have to install and use the CodeDeploy agent installed on your EC2 instances and ECS cluster.,AWS Lambda compute platform deployments cannot use an in-place deployment type.,"CodeDeploy can deploy applications to EC2, AWS Lambda, and Amazon ECS only.",The CodeDeploy agent communicates using HTTP over port 80.,,,한 회사는 엔터프라이즈 애플리케이션을 호스팅하기 위해 다양한 AWS 서비스를 많이 사용하고 있습니다. 현재 배포 프로세스에는 여전히 수동 단계가 많기 때문에 AWS의 CI/CD(지속적 통합 및 전달) 파이프라인을 사용하여 소프트웨어 제공 프로세스를 자동화할 계획입니다. CodePipeline을 사용하여 릴리스 프로세스의 각 단계를 조정하고 CodeDeploy를 사용하여 AWS의 다양한 컴퓨팅 플랫폼에 애플리케이션을 배포합니다.이 아키텍처에서 CodeDeploy를 사용할 때 다음 중 유효한 고려 사항은 무엇입니까? (2개를 선택하세요.),CodeDeploy는 EC2 인스턴스와 온프레미스 서버 모두에 애플리케이션을 배포할 수 있습니다.,EC2 인스턴스 및 ECS 클러스터에 설치된 CodeDeploy 에이전트를 설치하고 사용해야 합니다.,AWS Lambda 컴퓨팅 플랫폼 배포에서는 전체 배포 유형을 사용할 수 없습니다.,"CodeDeploy는 EC2, AWS Lambda 및 Amazon ECS에만 애플리케이션을 배포할 수 있습니다.",CodeDeploy 에이전트는 포트 80을 통해 HTTP를 사용하여 통신합니다.,,0,,
udemy,DVA-02,151,"A developer is instrumenting an application that will be hosted in a large On-Demand EC2 instance in AWS. All of the downstream calls invoked by the application must be traced properly, including the AWS SDK calls. A user-defined data should also be present to expedite the troubleshooting process.Which of the following are valid considerations in AWS X-Ray that the developer should follow? (Select TWO.)",BC,BC,Set the annotations object with any additional custom data that you want to store in the segment.,Set the namespace subsegment field to aws for AWS SDK calls and remote for other downstream calls.,Set the metadata object with any additional custom data that you want to store in the segment.,Set the namespace subsegment field to remote for AWS SDK calls and aws for other downstream calls.,Set the metadata object with key-value pairs that you want X-Ray to index for search.,,,개발자가 AWS의 대규모 온디맨드 EC2 인스턴스에서 호스팅될 애플리케이션을 계측하고 있습니다. AWS SDK 호출을 포함하여 애플리케이션에서 호출한 모든 다운스트림 호출을 올바르게 추적해야 합니다. 문제 해결 프로세스를 신속하게 처리하려면 사용자 정의 데이터도 있어야 합니다.다음 중 개발자가 따라야 하는 AWS X-Ray의 유효한 고려 사항은 무엇입니까? (2개를 선택하세요.),annotations세그먼트에 저장하려는 추가 사용자 정의 데이터로 개체를 설정합니다 .,AWS SDK 호출 및 기타 다운스트림 호출의 경우 namespace하위 세그먼트 필드를 로 설정합니다 .awsremote,metadata세그먼트에 저장하려는 추가 사용자 정의 데이터로 개체를 설정합니다 .,AWS SDK 호출 및 기타 다운스트림 호출의 경우 namespace하위 세그먼트 필드를 로 설정합니다 .remoteaws,metadataX-Ray가 검색을 위해 인덱싱할 키-값 쌍으로 객체를 설정합니다 .,,0,,
udemy,DVA-02,152,"A prototype application is hosted in an EC2 instance, which has an assigned IAM Role to store data from both the development and production S3 buckets. The instance also has AWS CLI access/secret key installed to handle other ad hoc tasks. You assigned a new IAM Role to the instance which has the permission to access the development bucket only. However, upon testing, the instance can still store files to both buckets.What is the MOST likely root cause of this issue?",D,D,The new IAM Role has an attached inline policy.,"Due to eventual consistency, you must wait 24 hours for the change to appear across all of AWS.",The instance profile role of a running EC2 instance is static and can't be replaced at all.,The application is still using the IAM role that is configured for the AWS CLI key.,,,,프로토타입 애플리케이션은 개발 및 프로덕션 S3 버킷 모두의 데이터를 저장하기 위해 IAM 역할이 할당된 EC2 인스턴스에서 호스팅됩니다. 또한 인스턴스에는 다른 임시 작업을 처리하기 위해 AWS CLI 액세스/비밀 키가 설치되어 있습니다. 개발 버킷에만 액세스할 수 있는 권한이 있는 인스턴스에 새 IAM 역할을 할당했습니다. 그러나 테스트 시 인스턴스는 여전히 두 버킷에 모두 파일을 저장할 수 있습니다.이 문제의 가장 가능성 있는 근본 원인은 무엇입니까?,새로운 IAM 역할에는 인라인 정책이 첨부되어 있습니다.,최종 일관성으로 인해 변경 사항이 AWS 전체에 나타날 때까지 24시간을 기다려야 합니다.,실행 중인 EC2 인스턴스의 인스턴스 프로파일 역할은 정적이며 전혀 대체될 수 없습니다.,애플리케이션은 여전히 ​​AWS CLI 키에 대해 구성된 IAM 역할을 사용하고 있습니다.,,,0,,
udemy,DVA-02,153,"The CTO of a startup needs a version control system for collaborative software development. The system must support version tracking, parallel branching, pull requests, and batch changes across multiple files. Which AWS service will meet these requirements?",B,B,AWS CodeBuild,AWS CodeCommit,AWS CodeStar,AWS CodeDeploy,,,,"스타트업의 CTO에게는 협업 소프트웨어 개발을 위한 버전 관리 시스템이 필요합니다. 시스템은 여러 파일에 걸쳐 버전 추적, 병렬 분기, 끌어오기 요청 및 일괄 변경을 지원해야 합니다.이러한 요구 사항을 충족하는 AWS 서비스는 무엇입니까?",AWS 코드빌드,AWS 코드커밋,AWS 코드스타,AWS 코드배포,,,0,,
udemy,DVA-02,154,"A company has different AWS accounts, namely Account A, Account B, and Account C, which are used for their Development, Test, and Production environments respectively. A developer needs access to perform an audit whenever a new version of the application has been deployed to the Test (Account B) and production (Account C) environments. What is the MOST efficient way to provide the developer access to execute the specified task?",C,C,Set up AWS Organizations and attach a Service Control Policy to the developer to access the other accounts.,Enable AWS multi-factor authentication (MFA) to the IAM User of the developer.,Grant the developer cross-account access to the resources of Accounts B and C.,Create separate identities and passwords for the developer on both the Test and Production accounts.,,,,"회사에는 개발, 테스트 및 프로덕션 환경에 각각 사용되는 계정 A, 계정 B 및 계정 C라는 다양한 AWS 계정이 있습니다. 개발자는 새 버전의 애플리케이션이 테스트(계정 B) 및 프로덕션(계정 C) 환경에 배포될 때마다 감사를 수행할 수 있는 액세스 권한이 필요합니다.개발자에게 지정된 작업을 실행할 수 있는 액세스 권한을 제공하는 가장 효율적인 방법은 무엇입니까?",AWS Organizations를 설정하고 서비스 제어 정책을 개발자에게 연결하여 다른 계정에 액세스합니다.,개발자의 IAM 사용자에게 AWS Multi-Factor Authentication(MFA)을 활성화합니다.,개발자에게 계정 B와 C의 리소스에 대한 교차 계정 액세스 권한을 부여합니다.,테스트 계정과 프로덕션 계정 모두에서 개발자를 위한 별도의 ID와 비밀번호를 만듭니다.,,,0,,
udemy,DVA-02,155,"A serverless application is using API Gateway with a non-proxy Lambda Integration. A developer was tasked to expose a GET method on a new /getcourses resource to invoke the Lambda function, which will allow the consumers to fetch a list of online courses in JSON format. The consumers must include a query string parameter named courseType in their request to get the data. What is the MOST efficient solution that the developer should do to accomplish this requirement?",A,A,Configure the method request of the resource.,Configure the integration request of the resource.,Configure the method response of the resource.,Configure the integration response of the resource.,,,,서버리스 애플리케이션은 프록시가 아닌 Lambda 통합과 함께 API 게이트웨이를 사용하고 있습니다. /getcourses개발자 는 소비자가 JSON 형식으로 온라인 강좌 목록을 가져올 수 있도록 하는 Lambda 함수를 호출하기 위해 새 리소스에 GET 메서드를 노출하는 작업을 맡았습니다 . 소비자는 courseType데이터를 가져오기 위해 요청에 명명된 쿼리 문자열 매개 변수를 포함해야 합니다.이 요구 사항을 달성하기 위해 개발자가 수행해야 하는 가장 효율적인 솔루션은 무엇입니까?,리소스의 메서드 요청을 구성합니다.,리소스의 통합 요청을 구성합니다.,리소스의 메서드 응답을 구성합니다.,리소스의 통합 응답을 구성합니다.,,,0,,
udemy,DVA-02,156,"A developer is instructed to collect data on the number of times that web visitors click the advertisement link of a popular news website. A database entry containing the count will be incremented for every click. Given that the website has millions of readers worldwide, your database should be configured to provide optimal performance to capture all the click events. What is the BEST service that the developer should implement in this scenario?",C,C,Launch an Amazon Redshift for the database and apply a step count of 1 for the IDENTITY column.,Use Amazon RDS for the database and setup SQL AUTO_INCREMENT on your tables.,Set up Amazon DynamoDB for the database and implement atomic counters for UpdateItem operation of the website counter.,Take advantage of Amazon Aurora's performance speed and AUTO_INCREMENT feature for item updates.,,,,개발자는 웹 방문자가 인기 뉴스 웹사이트의 광고 링크를 클릭하는 횟수에 대한 데이터를 수집하라는 지시를 받습니다. 클릭할 때마다 개수가 포함된 데이터베이스 항목이 증가합니다. 웹사이트에는 전 세계적으로 수백만 명의 독자가 있다는 점을 고려하면 모든 클릭 이벤트를 캡처할 수 있는 최적의 성능을 제공하도록 데이터베이스를 구성해야 합니다.이 시나리오에서 개발자가 구현해야 하는 가장 좋은 서비스는 무엇입니까?,데이터베이스에 대해 Amazon Redshift를 시작하고 IDENTITY 열에 단계 수 1을 적용합니다.,데이터베이스에 Amazon RDS를 사용하고 테이블에 SQL AUTO_INCREMENT를 설정하세요.,데이터베이스용 Amazon DynamoDB를 설정하고 UpdateItem웹사이트 카운터 작동을 위한 원자 카운터를 구현합니다.,Amazon Aurora의 성능 속도와 항목 업데이트를 위한 AUTO_INCREMENT 기능을 활용하세요.,,,0,,
udemy,DVA-02,157,An application has recently been migrated from an on-premises data center to a development Elastic Beanstalk environment. A developer will do iterative tests and therefore needs to deploy code changes and view them as quickly as possible.Which of the following options take the LEAST amount of time to complete the deployment?,D,D,Immutable,Rolling with additional batch,Rolling,All at once,,,,최근 애플리케이션이 온프레미스 데이터 센터에서 개발 Elastic Beanstalk 환경으로 마이그레이션되었습니다. 개발자는 반복 테스트를 수행하므로 코드 변경 사항을 배포하고 가능한 한 빨리 확인해야 합니다.다음 옵션 중 배포를 완료하는 데 가장 짧은 시간이 걸리는 옵션은 무엇입니까?,불변,추가 배치로 롤링,구르는,한꺼번에,,,0,,
udemy,DVA-02,158,"An application in your development account is running in an AWS Elastic Beanstalk environment which has an attached Amazon RDS database. You noticed that if you terminate the environment, it also brings down the database which hinders you from performing seamless updates with blue-green deployments. This also poses a critical security risk if the company decides to deploy the application in production. In this scenario, how can you decouple your database instance from your environment without having any data loss?",A,A,"Use the blue / green deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance. Before terminating the old Elastic Beanstalk environment, remove its security group rule first before proceeding.",Use the blue / green deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance and delete the old environment.,Use a Canary deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance and delete the old environment.,Use a Canary deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an RDS DB snapshot of the database and then create a new Elastic Beanstalk environment with the necessary information to connect to the Amazon RDS instance.,,,,개발 계정의 애플리케이션은 Amazon RDS 데이터베이스가 연결된 AWS Elastic Beanstalk 환경에서 실행되고 있습니다. 환경을 종료하면 데이터베이스도 다운되어 블루-그린 배포를 통해 원활한 업데이트를 수행하는 데 방해가 된다는 사실을 확인했습니다. 이는 회사가 프로덕션 환경에 애플리케이션을 배포하기로 결정한 경우에도 심각한 보안 위험을 초래합니다.이 시나리오에서 데이터 손실 없이 어떻게 데이터베이스 인스턴스를 환경에서 분리할 수 있습니까?,블루/그린 배포 전략을 사용하여 Elastic Beanstalk 환경에서 Amazon RDS 인스턴스를 분리합니다. 데이터베이스의 RDS DB 스냅샷을 생성하고 삭제 보호를 활성화합니다. Amazon RDS 인스턴스에 연결하는 데 필요한 정보로 새로운 Elastic Beanstalk 환경을 생성합니다. 이전 Elastic Beanstalk 환경을 종료하기 전에 먼저 보안 그룹 규칙을 제거한 후 진행하세요.,블루/그린 배포 전략을 사용하여 Elastic Beanstalk 환경에서 Amazon RDS 인스턴스를 분리합니다. 데이터베이스의 RDS DB 스냅샷을 생성하고 삭제 보호를 활성화합니다. Amazon RDS 인스턴스에 연결하고 이전 환경을 삭제하는 데 필요한 정보로 새 Elastic Beanstalk 환경을 생성합니다.,Canary 배포 전략을 사용하여 Elastic Beanstalk 환경에서 Amazon RDS 인스턴스를 분리합니다. 데이터베이스의 RDS DB 스냅샷을 생성하고 삭제 보호를 활성화합니다. Amazon RDS 인스턴스에 연결하고 이전 환경을 삭제하는 데 필요한 정보로 새 Elastic Beanstalk 환경을 생성합니다.,Canary 배포 전략을 사용하여 Elastic Beanstalk 환경에서 Amazon RDS 인스턴스를 분리합니다. 데이터베이스의 RDS DB 스냅샷을 생성한 다음 Amazon RDS 인스턴스에 연결하는 데 필요한 정보로 새로운 Elastic Beanstalk 환경을 생성합니다.,,,0,,
udemy,DVA-02,159,"A developer is preparing the application specification (AppSpec) file in CodeDeploy, which will be used to deploy her Lambda functions to AWS. In the deployment, she needs to configure CodeDeploy to run a task before the traffic is shifted to the deployed Lambda function version. Which deployment lifecycle event should she configure in this scenario?",B,B,Start,BeforeAllowTraffic,BeforeInstall,Install,,,,AppSpec개발자는 AWS에 Lambda 함수를 배포하는 데 사용될 CodeDeploy에서 애플리케이션 사양( ) 파일을 준비하고 있습니다 . 배포 시 그녀는 트래픽이 배포된 Lambda 함수 버전으로 이동하기 전에 작업을 실행하도록 CodeDeploy를 구성해야 합니다.이 시나리오에서는 어떤 배포 수명 주기 이벤트를 구성해야 합니까?,시작,이전에 트래픽 허용,설치 전,설치하다,,,0,,
udemy,DVA-02,160,A mobile game is currently being developed and needs to have an authentication service. You need to use an AWS service which provides temporary AWS credentials for users who have been authenticated via their social media logins as well as for guest users who do not require any authentication. How can you BEST achieve this using AWS?,B,B,Use Amazon Cognito Sync.,Use AWS Cognito Identity Pools then enable access to unauthenticated identities.,Use AWS Single Sign-On.,Use AWS Cognito User Pools then enable access to unauthenticated identities.,,,,현재 모바일 게임을 개발 중인데 인증 서비스가 필요합니다. 소셜 미디어 로그인을 통해 인증된 사용자와 인증이 필요하지 않은 게스트 사용자에게 임시 AWS 자격 증명을 제공하는 AWS 서비스를 사용해야 합니다.AWS를 사용하여 이를 가장 잘 달성할 수 있는 방법은 무엇입니까?,Amazon Cognito 동기화를 사용하세요.,AWS Cognito 자격 증명 풀을 사용한 다음 인증되지 않은 자격 증명에 대한 액세스를 활성화합니다.,AWS Single Sign-On을 사용합니다.,AWS Cognito 사용자 풀을 사용한 다음 인증되지 않은 자격 증명에 대한 액세스를 활성화합니다.,,,0,,
udemy,DVA-02,161,"A company has an AWS account with only 2 Lambda functions, which process data and store the results in an S3 bucket. An Application Load Balancer is used to distribute the incoming traffic to the two Lambda functions as registered targets. You noticed that in peak times, the first Lambda function works with optimal performance but the second one is throttling the incoming requests. Which of the following is the MOST likely root cause of this issue?",C,C,The first function is using the unreserved account concurrency while the second function has been set with a concurrency execution limit of 1000.,The first function is using the unreserved account concurrency while the second function has been set with a concurrency execution limit of 800.,The concurrency execution limit provided to the first function is significantly higher than the second function.,The concurrency execution limit provided to the first function is less than the second function.,,,,회사에는 데이터를 처리하고 결과를 S3 버킷에 저장하는 2개의 Lambda 함수만 있는 AWS 계정이 있습니다. Application Load Balancer는 등록된 대상으로 두 개의 Lambda 함수에 수신 트래픽을 분산하는 데 사용됩니다. 피크 시간에는 첫 번째 Lambda 함수가 최적의 성능으로 작동하지만 두 번째 함수는 들어오는 요청을 제한하고 있음을 확인했습니다.다음 중 이 문제의 근본 원인일 가능성이 가장 높은 것은 무엇입니까?,"첫 번째 함수는 예약되지 않은 계정 동시성을 사용하는 반면, 두 번째 함수는 동시 실행 제한을 1000으로 설정했습니다.","첫 번째 함수는 예약되지 않은 계정 동시성을 사용하는 것이고, 두 번째 함수는 동시 실행 제한을 800으로 설정한 것입니다.",첫 번째 함수에 제공되는 동시 실행 제한은 두 번째 함수보다 훨씬 높습니다.,첫 번째 함수에 제공되는 동시 실행 제한은 두 번째 함수보다 작습니다.,,,0,,
udemy,DVA-02,162,You have just created a custom VPC with two public subnets and a custom route table. Which of the following is true with regards to route tables?,B,B,You cannot modify/edit the main route created by default by AWS.,A subnet can only be associated with one route table at a time.,A VPC has a default limit of 5 route tables.,You cannot associate multiple subnets to the same route table.,,,,두 개의 퍼블릭 서브넷과 하나의 사용자 지정 라우팅 테이블이 있는 사용자 지정 VPC를 생성했습니다. 다음 중 라우팅 테이블에 대한 설명으로 옳은 것은 무엇입니까?,AWS에서 기본적으로 생성한 기본 경로는 수정/편집할 수 없습니다.,서브넷은 한 번에 하나의 라우팅 테이블에만 연결할 수 있습니다.,VPC의 라우팅 테이블은 기본적으로 5개로 제한됩니다.,여러 서브넷을 동일한 라우팅 테이블에 연결할 수 없습니다.,,,0,,
udemy,DVA-02,163,"A serverless application, which is composed of multiple Lambda functions, has been deployed using AWS SAM. A developer was instructed to easily manage the deployments of the functions using CodeDeploy. When there is a new deployment, 10 percent of the incoming traffic should be shifted to the new version every 10 minutes until all traffic is shifted from the old version. What should the developer do to properly deploy the functions that satisfies this requirement?",B,B,Deploy the functions using an Immutable deployment configuration.,Deploy the functions using a Linear deployment configuration.,Deploy the functions using an All-at-once deployment configuration.,Deploy the functions using a Canary deployment configuration.,,,,여러 Lambda 함수로 구성된 서버리스 애플리케이션이 AWS SAM을 사용하여 배포되었습니다. 개발자는 CodeDeploy를 사용하여 기능 배포를 쉽게 관리하라는 지시를 받았습니다. 새 배포가 있는 경우 모든 트래픽이 이전 버전에서 전환될 때까지 들어오는 트래픽의 10%가 10분마다 새 버전으로 전환되어야 합니다.이 요구 사항을 충족하는 기능을 올바르게 배포하려면 개발자는 무엇을 해야 합니까?,불변 배포 구성을 사용하여 함수를 배포합니다.,선형 배포 구성을 사용하여 기능을 배포합니다.,일괄 배포 구성을 사용하여 기능을 배포합니다.,Canary 배포 구성을 사용하여 기능을 배포합니다.,,,0,,
udemy,DVA-02,164,A company has a hybrid cloud architecture that connects its on-premises data center with AWS. The DevOps team has been tasked to set up the company's continuous integration and continuous delivery (CI/CD) systems. The application deployment to both Amazon EC2 instances and on-premises servers should also be automated. Which of the following AWS service should be used to accomplish this?,C,C,AWS CodeCommit,AWS CloudFormation,AWS CodeDeploy,Amazon Kinesis,,,,회사에는 온프레미스 데이터 센터를 AWS와 연결하는 하이브리드 클라우드 아키텍처가 있습니다. DevOps 팀은 회사의 CI/CD(지속적 통합 및 지속적 전달) 시스템을 설정하는 임무를 맡았습니다. Amazon EC2 인스턴스와 온프레미스 서버 모두에 대한 애플리케이션 배포도 자동화되어야 합니다.이를 달성하려면 다음 중 어떤 AWS 서비스를 사용해야 합니까?,AWS 코드커밋,AWS 클라우드포메이션,AWS 코드배포,아마존 키네시스,,,0,,
udemy,DVA-02,165,A company has a global multi-player game with a multi-master DynamoDB database topology which stores data in multiple AWS regions. You were assigned to develop a real-time data analytics application which will track and store the recent changes on all the tables from various regions. Only the new data of the recently updated item is needed to be tracked by your application. Which of the following is the MOST suitable way to configure the data analytics application to detect and retrieve the updated database entries automatically?,A,A,Enable DynamoDB Streams and set the value of StreamViewType to NEW_IMAGE. Use Kinesis Adapter in the application to consume streams from DynamoDB.,Enable DynamoDB Streams and set the value of StreamViewType to NEW_AND_OLD_IMAGE. Create a trigger in AWS Lambda to capture stream data and forward it to your application.,Enable DynamoDB Streams and set the value of StreamViewType to NEW_AND_OLD_IMAGE. Use Kinesis Adapter in the application to consume streams from DynamoDB.,Enable DynamoDB Streams and set the value of StreamViewType to NEW_IMAGE. Create a trigger in AWS Lambda to capture stream data and forward it to your application.,,,,한 회사는 여러 AWS 리전에 데이터를 저장하는 멀티 마스터 DynamoDB 데이터베이스 토폴로지를 갖춘 글로벌 멀티 플레이어 게임을 보유하고 있습니다. 귀하는 다양한 지역의 모든 테이블에 대한 최근 변경 사항을 추적하고 저장하는 실시간 데이터 분석 애플리케이션을 개발하도록 배정되었습니다. 애플리케이션에서 추적하려면 최근 업데이트된 항목의 새 데이터만 필요합니다.다음 중 업데이트된 데이터베이스 항목을 자동으로 감지하고 검색하도록 데이터 분석 애플리케이션을 구성하는 가장 적합한 방법은 무엇입니까?,DynamoDB 스트림을 활성화하고 값을 StreamViewTypeNEW_IMAGE로 설정합니다. 애플리케이션에서 Kinesis 어댑터를 사용하여 DynamoDB의 스트림을 사용합니다.,DynamoDB 스트림을 활성화하고 값을 StreamViewTypeNEW_AND_OLD_IMAGE로 설정합니다. AWS Lambda에서 트리거를 생성하여 스트림 데이터를 캡처하고 이를 애플리케이션에 전달합니다.,DynamoDB 스트림을 활성화하고 값을 StreamViewTypeNEW_AND_OLD_IMAGE로 설정합니다. 애플리케이션에서 Kinesis 어댑터를 사용하여 DynamoDB의 스트림을 사용합니다.,DynamoDB 스트림을 활성화하고 값을 StreamViewTypeNEW_IMAGE로 설정합니다. AWS Lambda에서 트리거를 생성하여 스트림 데이터를 캡처하고 이를 애플리케이션에 전달합니다.,,,0,,
udemy,DVA-02,166,"A developer needs to configure the environment name, solution stack, and environment links of his application environment which will be hosted in Elastic Beanstalk. Which configuration file should the developer add in the source bundle to meet the above requirement?",A,A,env.yaml,env.config,cron.yaml,Dockerrun.aws.json,,,,"개발자는 Elastic Beanstalk에서 호스팅할 애플리케이션 환경의 환경 이름, 솔루션 스택, 환경 링크를 구성해야 합니다. 위 요구 사항을 충족하려면 개발자가 소스 번들에 어떤 구성 파일을 추가해야 합니까?",env.yaml,env.config,cron.yaml,Dockerrun.aws.json,,,0,,
udemy,DVA-02,167,"A data analytics company has installed sensors to track the number of people that goes to the mall. The data sets are collected in real-time by an Amazon Kinesis Data Stream which has a consumer that is configured to process data every other day and store the results to S3. Your team noticed that your S3 bucket is only receiving half of the data that is being sent to the Kinesis stream but after checking, you have verified that the sensors are properly sending the data to Amazon Kinesis in real-time without any issues. Which of the following is the MOST likely root cause of this issue?",D,D,The sensors are having intermittent connection issues.,The Amazon Kinesis Data Stream automatically deletes duplicate data.,The Amazon Kinesis Data Stream has too many open shards.,"By default, the data records are only accessible for 24 hours from the time they are added to a Kinesis stream.",,,,한 데이터 분석 회사는 쇼핑몰에 가는 사람들의 수를 추적하기 위해 센서를 설치했습니다. 데이터 세트는 매일 데이터를 처리하고 결과를 S3에 저장하도록 구성된 소비자가 있는 Amazon Kinesis Data Stream에 의해 실시간으로 수집됩니다. 팀에서는 S3 버킷이 Kinesis 스트림으로 전송되는 데이터의 절반만 수신하고 있음을 확인했지만 확인 후 센서가 문제 없이 실시간으로 Amazon Kinesis에 데이터를 올바르게 전송하고 있음을 확인했습니다.다음 중 이 문제의 근본 원인일 가능성이 가장 높은 것은 무엇입니까?,센서에 간헐적으로 연결 문제가 있습니다.,Amazon Kinesis Data Stream은 중복 데이터를 자동으로 삭제합니다.,Amazon Kinesis Data Stream에 열려 있는 샤드가 너무 많습니다.,기본적으로 데이터 레코드는 Kinesis 스트림에 추가된 시점부터 24시간 동안만 액세스할 수 있습니다.,,,0,,
udemy,DVA-02,168,"Your team is developing a new feature on your application which is already hosted in Elastic Beanstalk. After several weeks, the new version of the application is ready to be deployed and you were instructed to handle the deployment. What is the correct way to deploy the new version to Elastic Beanstalk via the CLI?",A,A,Package your application as a zip file and deploy it using the eb deploy command.,Package your application as a zip file and deploy it using the aws elasticbeanstalk update-application command.,Package your application as a tar file and deploy it using the aws elasticbeanstalk update-application command.,Package your application as a tar file and deploy it using the eb deploy command.,,,,귀하의 팀은 이미 Elastic Beanstalk에서 호스팅되는 애플리케이션의 새로운 기능을 개발하고 있습니다. 몇 주 후에 애플리케이션의 새 버전을 배포할 준비가 되었으며 배포를 처리하라는 지시를 받았습니다.CLI를 통해 Elastic Beanstalk에 새 버전을 배포하는 올바른 방법은 무엇입니까?,애플리케이션을 zip파일로 패키징하고 eb deploy명령을 사용하여 배포합니다.,애플리케이션을 zip파일로 패키징하고 aws elasticbeanstalk update-application명령을 사용하여 배포합니다.,애플리케이션을 tar파일로 패키징하고 aws elasticbeanstalk update-application명령을 사용하여 배포합니다.,애플리케이션을 tar파일로 패키징하고 eb deploy명령을 사용하여 배포합니다.,,,0,,
udemy,DVA-02,169,"You are planning to create a DynamoDB table for your employee profile website. This will be used by the Human Resources department to easily view details about each employee. When choosing the partition key of the table, which of the following is the BEST attribute to use?",B,B,position_id because this will help sort the records per department.,employee_id because each employee ID is unique.,employee_name because this will speed up searching of records.,department_id since employees will fall in these departments.,,,,직원 프로필 웹 사이트용 DynamoDB 테이블을 생성할 계획입니다. 이는 인사부에서 각 직원에 대한 세부 정보를 쉽게 확인하는 데 사용됩니다.테이블의 파티션 키를 선택할 때 다음 중 가장 적합한 속성은 무엇입니까?,position_id부서별로 기록을 정리하는 데 도움이 되기 때문입니다.,employee_id각 직원 ID는 고유하기 때문입니다.,employee_name이렇게 하면 기록 검색 속도가 빨라지기 때문입니다.,department_id왜냐하면 직원들이 이 부서에 속하기 때문입니다.,,,0,,
udemy,DVA-02,170,You are developing a serverless application in AWS composed of several Lambda functions and a DynamoDB database. The requirement is to process the requests asynchronously.Which of the following is the MOST suitable way to accomplish this?,C,C,Use the InvokeAsync API to call the Lambda function and set the invocation type request parameter to Event.,Use the InvokeAsync API to call the Lambda function and set the invocation type request parameter to RequestResponse.,Use the Invoke API to call the Lambda function and set the invocation type request parameter to Event.,Use the Invoke API to call the Lambda function and set the invocation type request parameter to RequestResponse.,,,,귀하는 AWS에서 여러 Lambda 함수와 DynamoDB 데이터베이스로 구성된 서버리스 애플리케이션을 개발하고 있습니다. 요구 사항은 요청을 비동기적으로 처리하는 것입니다.다음 중 이를 달성하는 가장 적합한 방법은 무엇입니까?,API를 사용하여 InvokeAsyncLambda 함수를 호출하고 호출 유형 요청 매개변수를 로 설정합니다 Event.,API를 사용하여 InvokeAsyncLambda 함수를 호출하고 호출 유형 요청 매개변수를 로 설정합니다 RequestResponse.,API를 사용하여 InvokeLambda 함수를 호출하고 호출 유형 요청 매개변수를 로 설정합니다 Event.,API를 사용하여 InvokeLambda 함수를 호출하고 호출 유형 요청 매개변수를 로 설정합니다 RequestResponse.,,,0,,
udemy,DVA-02,171,"You recently deployed an application to a newly created AWS account, which uses two identical Lambda functions to process ad-hoc requests. The first function processes incoming requests efficiently but the second one has a longer processing time even though both of the functions have exactly the same code. Based on your monitoring, the Throttles metric of the second function is greater than the first one in Amazon CloudWatch. Which of the following are possible solutions that you can implement to fix this issue? (Select TWO.)",CD,CD,Configure the second function to use an unreserved account concurrency.,Set the concurrency execution limit of the second function to 0.,Decrease the concurrency execution limit of the first function.,Set the concurrency execution limit of both functions to 450.,Set the concurrency execution limit of both functions to 500.,,,최근에 두 개의 동일한 Lambda 함수를 사용하여 임시 요청을 처리하는 새로 생성된 AWS 계정에 애플리케이션을 배포했습니다. 첫 번째 함수는 들어오는 요청을 효율적으로 처리하지만 두 번째 함수는 두 함수가 정확히 동일한 코드를 가지고 있음에도 불구하고 처리 시간이 더 깁니다. 모니터링에 따르면 Throttles두 번째 함수의 지표는 Amazon CloudWatch의 첫 번째 함수보다 큽니다.다음 중 이 문제를 해결하기 위해 구현할 수 있는 솔루션은 무엇입니까? (2개를 선택하세요.),예약되지 않은 계정 동시성을 사용하도록 두 번째 기능을 구성합니다.,두 번째 함수의 동시 실행 제한을 0으로 설정합니다.,첫 번째 함수의 동시 실행 제한을 줄입니다.,두 함수의 동시 실행 제한을 450으로 설정합니다.,두 함수의 동시 실행 제한을 500으로 설정합니다.,,0,,
udemy,DVA-02,172,A Docker application hosted on an ECS cluster has encountered intermittent unavailability issues and timeouts. The lead DevOps engineer instructed you to instrument the application to detect where high latencies are occurring and to determine the specific services and paths impacting application performance.Which of the following steps should you take to accomplish this task properly? (Select TWO.),BC,BC,Manually install the X-Ray daemon to the instances via a user data script.,"Create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster.",Configure the port mappings and network mode settings in your task definition file to allow traffic on UDP port 2000.,Configure the port mappings and network mode settings in the container agent to allow traffic on TCP port 2000.,Add the xray-daemon.config configuration file in your Docker image.,,,ECS 클러스터에 호스팅된 Docker 애플리케이션에서 간헐적인 가용성 문제 및 시간 초과가 발생했습니다. 수석 DevOps 엔지니어는 애플리케이션을 계측하여 높은 대기 시간이 발생하는 위치를 감지하고 애플리케이션 성능에 영향을 미치는 특정 서비스 및 경로를 결정하도록 지시했습니다.이 작업을 올바르게 수행하려면 다음 중 어떤 단계를 수행해야 합니까? (2개를 선택하세요.),사용자 데이터 스크립트를 통해 X-Ray 데몬을 인스턴스에 수동으로 설치합니다.,X-Ray 데몬을 실행하는 Docker 이미지를 생성하여 Docker 이미지 리포지토리에 업로드한 다음 Amazon ECS 클러스터에 배포합니다.,UDP 포트 2000에서 트래픽을 허용하도록 작업 정의 파일에서 포트 매핑 및 네트워크 모드 설정을 구성합니다.,TCP 포트 2000에서 트래픽을 허용하도록 컨테이너 에이전트에서 포트 매핑 및 네트워크 모드 설정을 구성합니다.,xray-daemon.configDocker 이미지에 구성 파일을 추가합니다 .,,0,,
udemy,DVA-02,173,You are working for a startup that recently decided to host their web applications to AWS. All of the source code of their applications are hosted in Github and they are planning to migrate them all to CodeCommit. Which of the following BEST describes the process of migrating an existing Git repository to a CodeCommit repository?,A,A,Create a new CodeCommit repository. Clone your existing Git repository and then push its contents to the newly created CodeCommit repository.,Create a new Git repository. Clone your existing CodeCommit repository and then pull its contents to the newly created Git repository.,Create a new CodeCommit repository. Clone your existing Git repository and then pull its contents to the newly created CodeCommit repository.,Create a new Git repository. Clone your existing CodeCommit repository and then push its contents to the newly created Git repository.,,,,귀하는 최근 웹 애플리케이션을 AWS에 호스팅하기로 결정한 스타트업에서 일하고 있습니다. 애플리케이션의 모든 소스 코드는 Github에서 호스팅되며 모두 CodeCommit으로 마이그레이션할 계획입니다.다음 중 기존 Git 리포지토리를 CodeCommit 리포지토리로 마이그레이션하는 프로세스를 가장 잘 설명한 것은 무엇입니까?,새 CodeCommit 리포지토리를 생성합니다. 기존 Git 리포지토리를 복제한 다음 해당 콘텐츠를 새로 생성된 CodeCommit 리포지토리에 푸시합니다.,새로운 Git 저장소를 생성합니다. 기존 CodeCommit 리포지토리를 복제한 다음 해당 콘텐츠를 새로 생성된 Git 리포지토리로 가져옵니다.,새 CodeCommit 리포지토리를 생성합니다. 기존 Git 리포지토리를 복제한 다음 해당 콘텐츠를 새로 생성된 CodeCommit 리포지토리로 가져옵니다.,새로운 Git 저장소를 생성합니다. 기존 CodeCommit 리포지토리를 복제한 다음 해당 콘텐츠를 새로 생성된 Git 리포지토리에 푸시합니다.,,,0,,
udemy,DVA-02,174,"A write-heavy data analytics application is using DynamoDB database which has global secondary index. Whenever the application is performing heavy write activities on the table, the DynamoDB requests return a ProvisionedThroughputExceededException.  Which of the following is the MOST likely cause of this issue?",C,C,The rate of requests exceeds the allowed throughput.,The provisioned throughput exceeds the current throughput limit for your account.,The provisioned write capacity for the global secondary index is less than the write capacity of the base table.,The provisioned write capacity for the global secondary index is greater than the write capacity of the base table.,,,,쓰기가 많은 데이터 분석 애플리케이션은 글로벌 보조 인덱스가 있는 DynamoDB 데이터베이스를 사용하고 있습니다. 애플리케이션이 테이블에 대해 과도한 쓰기 활동을 수행할 때마다 DynamoDB 요청은 ProvisionedThroughputExceededException.  다음 중 이 문제의 가장 큰 원인은 무엇입니까?,요청 비율이 허용된 처리량을 초과합니다.,프로비저닝된 처리량이 계정의 현재 처리량 한도를 초과합니다.,글로벌 보조 인덱스에 대해 프로비저닝된 쓰기 용량은 기본 테이블의 쓰기 용량보다 작습니다.,글로벌 보조 인덱스에 대해 프로비저닝된 쓰기 용량은 기본 테이블의 쓰기 용량보다 큽니다.,,,0,,
udemy,DVA-02,175,A developer is creating a real-time auction app for second-hand cars using Kinesis Data Streams to ingest bids. The auction rules are as follows:A bid must be processed only onceAn EC2 instance consumer must process bids in the same order they were received.Which solution will meet the requirement?,D,D,Replace the stream with an SQS FIFO queue and use the SendMessageBatch API to write bids. Provide a unique id in the MessageDeduplicationId parameter for each bid request.,Embed a unique ID in each bid record. Use Kinesis PutRecords API to write bids. Assign a timestamp-based value for the PartitionKey parameter.,Replace the stream with an SQS FIFO queue and use the SendMessage API to write bids. Provide a unique id in the MessageDeduplicationId parameter for each bid request.,Embed a unique ID in each bid record. Use Kinesis PutRecord API to write bids. Assign a timestamp-based value for the SequenceNumberForOrdering parameter.,,,,개발자는 Kinesis Data Streams를 사용하여 입찰을 수집하는 중고차용 실시간 경매 앱을 만들고 있습니다. 경매규칙은 다음과 같습니다.입찰은 한 번만 처리되어야 합니다.EC2 인스턴스 소비자는 입찰을 받은 순서대로 처리해야 합니다.어떤 솔루션이 요구 사항을 충족합니까?,스트림을 SQS FIFO 대기열로 바꾸고 SendMessageBatchAPI를 사용하여 입찰을 작성합니다. 각 입찰 요청에 대한 매개변수 에 고유 ID를 제공하세요 MessageDeduplicationId.,각 입찰 기록에 고유 ID를 삽입합니다. Kinesis PutRecordsAPI를 사용하여 입찰을 작성합니다. 매개변수 에 타임스탬프 기반 값을 할당합니다 PartitionKey.,스트림을 SQS FIFO 대기열로 바꾸고 SendMessageAPI를 사용하여 입찰을 작성합니다. 각 입찰 요청에 대한 매개변수 에 고유 ID를 제공하세요 MessageDeduplicationId.,각 입찰 기록에 고유 ID를 삽입합니다. Kinesis PutRecordAPI를 사용하여 입찰을 작성합니다. 매개변수 에 타임스탬프 기반 값을 할당합니다 SequenceNumberForOrdering.,,,0,,
udemy,DVA-02,176,"An application is sending thousands of log files to an S3 bucket everyday. The request to retrieve the list of objects using the AWS CLI aws s3api list-objects command is timing out due to the high volume of data being fetched. In order to rectify this issue, you have to use pagination to control the number of results returned on your request. Which of the following parameters should you include in CLI command for this scenario? (Select TWO.)",BD,BD,--exclude,--max-items,--summarize,--page-size,--size-only,,,애플리케이션은 매일 수천 개의 로그 파일을 S3 버킷으로 보냅니다. aws s3api list-objects가져 오는 데이터 양이 많아 AWS CLI 명령을 사용하여 객체 목록을 검색하는 요청이 시간 초과되었습니다. 이 문제를 해결하려면 페이지 매김을 사용하여 요청 시 반환되는 결과 수를 제어해야 합니다.이 시나리오에서는 다음 중 어떤 매개변수를 CLI 명령에 포함해야 합니까? (2개를 선택하세요.),--exclude,--max-items,--summarize,--page-size,--size-only,,0,,
udemy,DVA-02,177,"An application is hosted in Elastic Beanstalk, which is currently running in Java 7 runtime environment. A new version of the application is ready to be deployed, and the developer was tasked to upgrade the platform to Java 8 to accommodate the changes. All user traffic must be immediately directed to the new version. If problems arise, the developer should be able to quickly revert to the previous version.Which of the following is the MOST appropriate action that the developer should do to upgrade the platform?",C,C,Manually upgrade the Java runtime environment of the EC2 instances in the Elastic Beanstalk environment.,Update the environment's platform version to Java 8.,Perform a Blue/Green Deployment.,Perform a Traffic splitting deployment.,,,,현재 Java 7 런타임 환경에서 실행 중인 Elastic Beanstalk에 애플리케이션이 호스팅됩니다. 새 버전의 애플리케이션을 배포할 준비가 되었으며 개발자는 변경 사항을 수용하기 위해 플랫폼을 Java 8로 업그레이드해야 했습니다. 모든 사용자 트래픽은 즉시 새 버전으로 전달되어야 합니다. 문제가 발생하면 개발자는 신속하게 이전 버전으로 되돌릴 수 있어야 합니다.다음 중 개발자가 플랫폼을 업그레이드하기 위해 수행해야 하는 가장 적절한 조치는 무엇입니까?,Elastic Beanstalk 환경에서 EC2 인스턴스의 Java 런타임 환경을 수동으로 업그레이드합니다.,환경의 플랫폼 버전을 Java 8로 업데이트합니다.,블루/그린 배포를 수행합니다.,트래픽 분할 배포를 수행합니다.,,,0,,
udemy,DVA-02,178,A developer is building a prototype microservices that are running as tasks in an Amazon ECS Cluster. His manager instructed him to define a task placement strategy which needs to be both cost and resource efficient. The task placement should minimize the number of instances in use which will keep the cost down since high availability is not much of a concern for this prototype. What should the developer implement to meet the above requirements?,D,D,"Distribute tasks evenly across Availability Zones, and then re-distribute the tasks among EC2 instances based on the least available amount of CPU/memory within each Availability Zone.",Distribute tasks evenly across all available EC2 instances using the spread task placement strategy.,Place tasks randomly using the random task placement strategy.,Distribute tasks among all registered EC2 instances based on the least available amount of CPU or memory using the binpack task placement strategy.,,,,개발자가 Amazon ECS 클러스터에서 작업으로 실행되는 프로토타입 마이크로서비스를 구축하고 있습니다. 그의 관리자는 그에게 비용과 자원 효율성이 모두 필요한 작업 배치 전략을 정의하라고 지시했습니다. 작업 배치는 사용 중인 인스턴스 수를 최소화하여 이 프로토타입에서는 고가용성이 그다지 중요하지 않기 때문에 비용을 절감해야 합니다.위의 요구 사항을 충족하려면 개발자가 무엇을 구현해야 합니까?,가용 영역 전체에 작업을 균등하게 배포한 다음 각 가용 영역 내에서 사용 가능한 최소 CPU/메모리 양을 기준으로 EC2 인스턴스 간에 작업을 다시 배포합니다.,작업 배치 전략을 사용하여 사용 가능한 모든 EC2 인스턴스에 작업을 균등하게 배포합니다 spread.,작업 배치 전략을 사용하여 작업을 무작위로 배치합니다 random.,binpack작업 배치 전략을 사용하여 사용 가능한 최소 CPU 또는 메모리 양을 기준으로 등록된 모든 EC2 인스턴스에 작업을 배포합니다 .,,,0,,
udemy,DVA-02,179,"An online magazine is deployed in AWS and uses an Application Load Balancer, an Auto Scaling group of EC2 instances, and an RDS MySQL Database. Some of the readers are complaining about the website's sluggish performance when loading the articles. Upon checking, there is a high number of read operations in the database, which affects the website's performance.Which of the following actions should you take to resolve the issue with minimal code change?",A,A,Create an RDS Read Replica instance and configure the application to use this for read queries.,Launch a large ElastiCache Cluster as a database cache for RDS and apply the required code change.,Set up a multi-AZ deployments configuration in RDS.,Upgrade the EC2 instances to a higher instance type.,,,,"온라인 매거진은 AWS에 배포되며 Application Load Balancer, EC2 인스턴스의 Auto Scaling 그룹 및 RDS MySQL 데이터베이스를 사용합니다. 일부 독자들은 기사를 로드할 때 웹사이트의 느린 성능에 대해 불평하고 있습니다. 확인 결과, 데이터베이스에 읽기 작업 수가 많아 웹사이트 성능에 영향을 미치는 것으로 나타났습니다.최소한의 코드 변경으로 문제를 해결하려면 다음 중 어떤 조치를 취해야 합니까?",RDS 읽기 전용 복제본 인스턴스를 생성하고 이를 읽기 쿼리에 사용하도록 애플리케이션을 구성합니다.,대규모 ElastiCache 클러스터를 RDS용 데이터베이스 캐시로 시작하고 필요한 코드 변경 사항을 적용합니다.,RDS에서 다중 AZ 배포 구성을 설정합니다.,EC2 인스턴스를 더 높은 인스턴스 유형으로 업그레이드합니다.,,,0,,
udemy,DVA-02,180,A tech company has a real-time traffic monitoring system which uses Amazon Kinesis Data Stream to collect data and a group of EC2 instances that consume and process the data stream. Your development team is responsible for adjusting the number of shards in the data stream to adapt to changes in the rate of data flow. Which of the following are correct regarding Kinesis resharding which your team should consider in managing the application? (Select TWO.),AE,AE,You can increase the stream's capacity by splitting shards.,You have to split the cold shards to decrease the capacity of the stream.,You have to merge the hot shards to increase the capacity of the stream.,The data records that are flowing to the parent shards will be lost when you reshard.,You can decrease the stream's capacity by merging shards.,,,한 기술 회사에는 Amazon Kinesis Data Stream을 사용하여 데이터와 데이터 스트림을 소비하고 처리하는 EC2 인스턴스 그룹을 수집하는 실시간 트래픽 모니터링 시스템이 있습니다. 개발 팀은 데이터 흐름 속도의 변화에 ​​적응하기 위해 데이터 스트림의 샤드 수를 조정하는 일을 담당합니다.다음 중 귀하의 팀이 애플리케이션을 관리할 때 고려해야 할 Kinesis 리샤딩에 관한 올바른 설명은 무엇입니까? (2개를 선택하세요.),샤드를 분할하여 스트림 용량을 늘릴 수 있습니다.,스트림 용량을 줄이려면 콜드 샤드를 분할해야 합니다.,스트림 용량을 늘리려면 핫 샤드를 병합해야 합니다.,재샤딩하면 상위 샤드로 이동하는 데이터 레코드가 손실됩니다.,샤드를 병합하여 스트림 용량을 줄일 수 있습니다.,,0,,
udemy,DVA-02,181,A Java web application built using AWS SDK for Java with a DynamoDB database is concurrently accessed by thousands of users during peak time. The application is highly write-intensive and there are a lot of incidents where it overwrites stale data from the DynamoDB table. How can you ensure your database writes are protected from being overwritten by other write operations that are occurring at the same time without affecting the application performance?,D,D,Implement overly optimistic locking (OOL).,Implement pessimistic locking with read locking.,Implement pessimistic locking with write locking.,Implement optimistic locking with version number.,,,,DynamoDB 데이터베이스와 함께 Java용 AWS SDK를 사용하여 구축된 Java 웹 애플리케이션은 피크 시간 동안 수천 명의 사용자가 동시에 액세스합니다. 이 애플리케이션은 쓰기 집약적이며 DynamoDB 테이블의 오래된 데이터를 덮어쓰는 사고가 많이 있습니다.애플리케이션 성능에 영향을 주지 않고 동시에 발생하는 다른 쓰기 작업이 데이터베이스 쓰기를 덮어쓰지 않도록 어떻게 보호할 수 있습니까?,지나치게 낙관적인 잠금(OOL)을 구현합니다.,읽기 잠금으로 비관적 잠금을 구현합니다.,쓰기 잠금으로 비관적 잠금을 구현합니다.,버전 번호로 낙관적 잠금을 구현합니다.,,,0,,
udemy,DVA-02,182,"A social media application is using DynamoDB to manage and store the session data of its users. As the number of users grew, the number of items in the table exponentially increased as well. You have to reduce storage usage and also reduce the cost of storing irrelevant data without using provisioned throughput to rectify this issue. Which of the following is the MOST cost-effective solution that you should implement?",A,A,Enable Time To Live (TTL) in the table.,Implement a Lazy Loading caching strategy to your application.,Use a Lambda function with CloudWatch Events to schedule a purge of stale items in the table on a daily basis.,Implement a Write-Through caching strategy in your application.,,,,소셜 미디어 애플리케이션은 DynamoDB를 사용하여 사용자의 세션 데이터를 관리하고 저장합니다. 사용자 수가 늘어나면서 테이블에 포함되는 항목 수도 기하급수적으로 늘어났습니다. 이 문제를 해결하려면 스토리지 사용량을 줄이고 프로비저닝된 처리량을 사용하지 않고 관련 없는 데이터를 저장하는 비용도 줄여야 합니다.다음 중 구현해야 할 가장 비용 효율적인 솔루션은 무엇입니까?,테이블에서 TTL(Time To Live)을 활성화합니다.,애플리케이션에 지연 로딩 캐싱 전략을 구현합니다.,CloudWatch 이벤트와 함께 Lambda 함수를 사용하여 매일 테이블의 오래된 항목 제거를 예약합니다.,애플리케이션에서 Write-Through 캐싱 전략을 구현합니다.,,,0,,
udemy,DVA-02,183,"You are hosting a website in an Amazon S3 bucket named tutorialsdojo and your users load the website using the http://tutorialsdojo.s3-website-us-east-1.amazonaws.com endpoint. You want to use JavaScript on the webpages that are stored in this bucket to be able to make authenticated GET and PUT requests. These requests are directed to the same bucket through the website.s3.amazonaws.com S3 API endpoint. However, you noticed that your web browser blocks the HTTP requests originating from your website. What should you do to rectify this issue?",B,B,Enable Cross-Zone Load Balancing.,Enable Cross-origin resource sharing (CORS) configuration in the bucket.,Enable Cross-Region Replication (CRR).,Enable cross-account access.,,,,귀하는 Amazon S3 버킷이라는 이름의 버킷에서 웹 사이트를 호스팅하고 tutorialsdojo있으며 사용자는 엔드포인트를 사용하여 웹 사이트를 로드합니다 http://tutorialsdojo.s3-website-us-east-1.amazonaws.com. 인증된 GET 및 PUT 요청을 수행할 수 있도록 이 버킷에 저장된 웹페이지에서 JavaScript를 사용하려고 합니다. 이러한 요청은 S3 API 엔드포인트를 통해 동일한 버킷으로 전달됩니다 website.s3.amazonaws.com. 그러나 웹 브라우저가 웹 사이트에서 발생하는 HTTP 요청을 차단하는 것으로 나타났습니다.이 문제를 해결하려면 어떻게 해야 합니까?,교차 영역 로드 밸런싱을 활성화합니다.,버킷에서 CORS(교차 원본 리소스 공유) 구성을 활성화합니다.,CRR(교차 지역 복제)을 활성화합니다.,교차 계정 액세스를 활성화합니다.,,,0,,
udemy,DVA-02,184,"A developer has a set of EC2 instances that runs the Amazon Kinesis Client Library to process a data stream in AWS. Based on the custom metrics, it shows that the instances are maxing out their CPU Utilization, and there are insufficient Kinesis shards to handle the rate of data flowing through the stream.Which of the following is the BEST course of action that the developer should take to solve this issue and prevent this situation from re-occurring in the future?",A,A,Increase both the instance size and the number of open shards.,Increase the instance size to a larger type.,Increase the number of shards.,Increase the number of instances up to the number of open shards.,,,,개발자는 Amazon Kinesis Client Library를 실행하여 AWS에서 데이터 스트림을 처리하는 EC2 인스턴스 세트를 보유하고 있습니다. 사용자 지정 지표를 기반으로 하면 인스턴스가 CPU 사용률을 최대화하고 있으며 스트림을 통해 흐르는 데이터 속도를 처리하기에는 Kinesis 샤드가 부족하다는 것을 알 수 있습니다.다음 중 이 문제를 해결하고 향후 이러한 상황이 다시 발생하지 않도록 개발자가 취해야 하는 가장 좋은 조치는 무엇입니까?,인스턴스 크기와 열린 샤드 수를 모두 늘립니다.,인스턴스 크기를 더 큰 유형으로 늘립니다.,샤드 수를 늘립니다.,열려 있는 샤드 수만큼 인스턴스 수를 늘립니다.,,,0,,
udemy,DVA-02,185,"A company is using AWS Organizations to manage its multiple AWS accounts which is being used by its various departments. To avoid security issues, it is of utmost importance to test the impact of service control policies (SCPs) on your IAM policies and resource policies before applying them. Which of the following services can you use to test and troubleshoot IAM and resource-based policies?",B,B,Systems Manager,IAM Policy Simulator,Amazon Inspector,AWS Config,,,,회사는 AWS Organizations를 사용하여 다양한 부서에서 사용되는 여러 AWS 계정을 관리하고 있습니다. 보안 문제를 방지하려면 SCP(서비스 제어 정책)가 IAM 정책 및 리소스 정책에 미치는 영향을 적용하기 전에 테스트하는 것이 가장 중요합니다.다음 중 IAM 및 리소스 기반 정책을 테스트하고 문제를 해결하는 데 사용할 수 있는 서비스는 무엇입니까?,시스템 관리자,IAM 정책 시뮬레이터,아마존 인스펙터,AWS 구성,,,0,,
udemy,DVA-02,186,You developed a Lambda function which will send status updates to a third party provider for analytics. You need to schedule this function to run every 30 minutes. Which of the following is the MOST manageable and cost-effective way of setting up this task?,D,D,Enable scheduling on the AWS Console of your Lambda function. Define a schedule to run it on 30-minute intervals.,Launch an EC2 instance which has a cron job that triggers the Lambda function every 30 minutes.,Use the Task Scheduler of your Windows PC to trigger the Lambda function every 30 minutes.,"Integrate CloudWatch Events with Lambda, which will automatically trigger the function every 30 minutes.",,,,분석을 위해 제3자 공급자에게 상태 업데이트를 보내는 Lambda 함수를 개발했습니다. 이 기능이 30분마다 실행되도록 예약해야 합니다.다음 중 이 작업을 설정하는 가장 관리하기 쉽고 비용 효율적인 방법은 무엇입니까?,Lambda 함수의 AWS 콘솔에서 예약을 활성화합니다. 30분 간격으로 실행되도록 일정을 정의합니다.,30분마다 Lambda 함수를 트리거하는 cron 작업이 있는 EC2 인스턴스를 시작합니다.,Windows PC의 작업 스케줄러를 사용하여 30분마다 Lambda 함수를 트리거합니다.,CloudWatch 이벤트를 Lambda와 통합하면 30분마다 자동으로 함수가 트리거됩니다.,,,0,,
udemy,DVA-02,187,"Using AWS SAM, a developer recently deployed a serverless application consisting of Lambda functions, API Gateway, Kinesis Data stream, and a DynamoDB table. The application has worked fine for a few days, but lately, there were a lot of ProvisionedThroughputExceeded exceptions being returned by DynamoDB. The developer also noticed that there's a sudden increase in read capacity units (RCU) usage whenever this issue happens.How should the developer refactor the application to find items based on primary key values and use the LEAST amount of RCU?",B,B,Use the Query operation with strong consistency reads.,Use the Query operation with eventual consistency reads.,Use the Scan operation with strong consistency reads.,Use the Scan operation with eventual consistency reads.,,,,"개발자는 최근 AWS SAM을 사용하여 Lambda 함수, API 게이트웨이, Kinesis 데이터 스트림 및 DynamoDB 테이블로 구성된 서버리스 애플리케이션을 배포했습니다. ProvisionedThroughputExceeded애플리케이션은 며칠 동안 제대로 작동했지만 최근에는 DynamoDB에서 많은 예외가 반환되었습니다. 또한 개발자는 이 문제가 발생할 때마다 RCU(읽기 용량 단위) 사용량이 갑자기 증가한다는 사실을 발견했습니다.개발자는 기본 키 값을 기반으로 항목을 찾고 최소한의 RCU를 사용하도록 애플리케이션을 어떻게 리팩터링해야 합니까?",강력한 일관성 읽기와 함께 쿼리 작업을 사용합니다.,최종 일관성 읽기와 함께 쿼리 작업을 사용합니다.,Strong Consistency 읽기와 함께 Scan 작업을 사용합니다.,최종 일관성 읽기와 함께 Scan 작업을 사용합니다.,,,0,,
udemy,DVA-02,188,"The source code of an application is hosted in CodeCommit which has a single master branch. A developer requires certain permissions in order to pull and push code to the repository using the git fetch, git clone, and git push commands. To improve security, the developer should be granted only the permissions required to perform these tasks. Which of the following permissions should the developer have in order to properly access the repository? (Select TWO.)",AD,AD,codecommit:GitPull,codecommit:*,codecommit:UpdateDefaultBranch,codecommit:GitPush,codecommit:GetBranch,,,"애플리케이션의 소스 코드는 단일 master분기가 있는 CodeCommit에서 호스팅됩니다. git fetch개발자가 , git clone및 명령을 사용하여 코드를 저장소로 가져오고 푸시하려면 특정 권한이 필요합니다 git push. 보안을 강화하려면 개발자에게 이러한 작업을 수행하는 데 필요한 권한만 부여해야 합니다.저장소에 올바르게 액세스하려면 다음 중 개발자에게 어떤 권한이 있어야 합니까? (2개를 선택하세요.)",codecommit:GitPull,codecommit:*,codecommit:UpdateDefaultBranch,codecommit:GitPush,codecommit:GetBranch,,0,,
udemy,DVA-02,189,"An ECS Cluster has a running X-Ray Daemon that enables developers to easily debug and troubleshoot their application. However, the trace data being sent to AWS X-Ray is still not as detailed as your manager wants it to be. There is a new requirement that requires the application to provide more granular timing information and more details about its downstream calls to various AWS resources. What should you do to satisfy this requirement?",D,D,Use inferred segment,Use annotations,Use metadata,Use subsegments,,,,ECS 클러스터에는 개발자가 애플리케이션을 쉽게 디버그하고 문제를 해결할 수 있도록 지원하는 실행 중인 X-Ray 데몬이 있습니다. 그러나 AWS X-Ray로 전송되는 추적 데이터는 여전히 관리자가 원하는 만큼 상세하지 않습니다. 애플리케이션이 다양한 AWS 리소스에 대한 다운스트림 호출에 대한 보다 세부적인 타이밍 정보와 세부 정보를 제공해야 하는 새로운 요구 사항이 있습니다.이 요구 사항을 충족하려면 어떻게 해야 합니까?,추론된 세그먼트 사용,주석 사용,메타데이터 사용,하위 세그먼트 사용,,,0,,
udemy,DVA-02,190,"A developer wants to track the number of visitors on their website, which has a DynamoDB database. This is primarily used to give a rough idea on how many people visit the site whenever they launch a new advertisement, which means it can tolerate a slight overcounting or undercounting of website visitors. Which of the following will satisfy the requirement with MINIMAL configuration?",D,D,Enable DynamoDB Streams to track the number of new visitors.,Use conditional writes to update the counter item in the DynamoDB table only if the item has a unique primary key and the new value is greater than the current value.,Use conditional writes to update the counter item in the DynamoDB table and set the ReturnConsumedCapacity parameter to TOTAL.,Use atomic counters to increment the counter item in the DynamoDB table for every new visitor.,,,,"개발자는 DynamoDB 데이터베이스가 있는 웹 사이트의 방문자 수를 추적하려고 합니다. 이는 주로 새 광고를 시작할 때마다 사이트를 방문하는 사람 수에 대한 대략적인 정보를 제공하는 데 사용됩니다. 즉, 웹 사이트 방문자가 약간 과대 계산되거나 과소 계산되는 것을 허용할 수 있습니다.다음 중 최소 구성의 요구 사항을 충족하는 것은 무엇입니까?",DynamoDB Streams를 활성화하여 신규 방문자 수를 추적하세요.,conditional writes항목에 고유한 기본 키가 있고 새 값이 현재 값보다 큰 경우에만 DynamoDB 테이블의 카운터 항목을 업데이트하는 데 사용됩니다 .,conditional writesDynamoDB 테이블의 카운터 항목을 업데이트하고 ReturnConsumedCapacity매개변수를 로 설정하는 데 사용됩니다 TOTAL.,atomic counters모든 새 방문자에 대해 DynamoDB 테이블의 카운터 항목을 늘리는 데 사용됩니다 .,,,0,,
udemy,DVA-02,191,A serverless application consisting of a Lambda function and a DynamoDB database is used to process Amazon S3 events. The Lambda function takes an average of three seconds to process the data and Amazon S3 publishes 10 events per second.What is the concurrent execution that the function will have?,B,B,3,30,10,13,,,,Lambda 함수와 DynamoDB 데이터베이스로 구성된 서버리스 애플리케이션은 Amazon S3 이벤트를 처리하는 데 사용됩니다. Lambda 함수는 데이터를 처리하는 데 평균 3초가 걸리며 Amazon S3는 초당 10개의 이벤트를 게시합니다.함수의 동시 실행은 무엇입니까?,삼,30,10,13,,,0,,
udemy,DVA-02,192,A developer is designing the cloud architecture of an internal application which will be used by about a hundred employees. She needs to ensure that the architecture is elastic enough to adequately match the supply of resources to the demand while maintaining its cost-effectiveness. Which of the following services can provide the MOST elasticity to the architecture? (Select TWO.),AC,AC,EC2 Spot Fleet,CloudFront,DynamoDB,RDS,WAF,,,한 개발자가 약 100명의 직원이 사용할 내부 애플리케이션의 클라우드 아키텍처를 설계하고 있습니다. 그녀는 비용 효율성을 유지하면서 리소스 공급을 수요에 적절하게 맞출 수 있을 만큼 아키텍처의 탄력성을 보장해야 합니다.다음 중 아키텍처에 가장 탄력성을 제공할 수 있는 서비스는 무엇입니까? (2개를 선택하세요.),EC2 스팟 집합,CloudFront,DynamoDB,RDS,WAF,,0,,
udemy,DVA-02,193,A developer has instrumented an application using the X-Ray SDK to collect all data about the requests that an application serves. There is a new requirement to develop a custom debug tool which will enable them to view the full traces of their application without using the X-Ray console. What should the developer do to accomplish this task?,A,A,Use the GetTraceSummaries API to get the list of trace IDs of the application and then retrieve the list of traces using BatchGetTraces API.,Use the GetServiceGraph API to get the list of trace IDs of the application and then retrieve the list of traces using GetTraceSummaries API.,Use the GetGroup API to get the list of trace IDs of the application and then retrieve the list of traces using BatchGetTraces API.,Use the BatchGetTraces API to get the list of trace IDs of the application and then retrieve the list of traces using GetTraceSummaries API.,,,,개발자는 X-Ray SDK를 사용하여 애플리케이션을 계측하여 애플리케이션이 제공하는 요청에 대한 모든 데이터를 수집했습니다. X-Ray 콘솔을 사용하지 않고도 애플리케이션의 전체 추적을 볼 수 있는 사용자 지정 디버그 도구를 개발해야 하는 새로운 요구 사항이 있습니다.이 작업을 수행하려면 개발자가 무엇을 해야 합니까?,API를 사용하여 GetTraceSummaries애플리케이션의 추적 ID 목록을 가져온 다음 BatchGetTracesAPI를 사용하여 추적 목록을 검색합니다.,API를 사용하여 GetServiceGraph애플리케이션의 추적 ID 목록을 가져온 다음 GetTraceSummariesAPI를 사용하여 추적 목록을 검색합니다.,API를 사용하여 GetGroup애플리케이션의 추적 ID 목록을 가져온 다음 BatchGetTracesAPI를 사용하여 추적 목록을 검색합니다.,API를 사용하여 BatchGetTraces애플리케이션의 추적 ID 목록을 가져온 다음 GetTraceSummariesAPI를 사용하여 추적 목록을 검색합니다.,,,0,,
udemy,DVA-02,194,"A company is re-architecting its legacy application to use AWS Lambda and DynamoDB. The table is provisioned to have 10 read capacity units, and each item has a size of 4 KB.How many eventual and strong consistent read requests can the table handle per second?",B,B,20 strongly consistent reads and 10 eventually consistent reads per second,10 strongly consistent reads and 20 eventually consistent reads per second,10 strongly consistent reads and 10 eventually consistent reads per second,5 strongly consistent reads and 20 eventually consistent reads per second,,,,한 회사는 AWS Lambda 및 DynamoDB를 사용하도록 레거시 애플리케이션을 재설계하고 있습니다. 테이블은 10개의 읽기 용량 단위를 갖도록 프로비저닝되었으며 각 항목의 크기는 4KB입니다.테이블이 초당 몇 개의 최종적이고 강력한 일관된 읽기 요청을 처리할 수 있습니까?,초당 강력한 일관된 읽기 20개와 ​​최종적 일관된 읽기 10개,초당 강력한 일관된 읽기 10개 및 최종적 일관된 읽기 20개,초당 강력한 일관된 읽기 10개 및 최종적 일관된 읽기 10개,초당 5개의 강력한 일관된 읽기 및 20개의 최종 일관된 읽기,,,0,,
udemy,DVA-02,195,"A company is deploying the package of its Lambda function, which is compressed as a ZIP file, to AWS. However, they are getting an error in the deployment process because the package is too large. The manager instructed the developer to keep the deployment package small to make the development process much easier and more modularized. This should also help prevent errors that may occur when dependencies are installed and packaged with the function code.Which of the following options is the MOST suitable solution that the developer should implement?",B,B,Compress the deployment package as TAR file instead.,Upload the other dependencies of your function as a separate Lambda Layer instead.,Upload the deployment package to S3.,Zip the deployment package again to further compress the zip file.,,,,한 회사가 ZIP 파일로 압축된 Lambda 함수 패키지를 AWS에 배포하고 있습니다. 그러나 패키지가 너무 커서 배포 프로세스에서 오류가 발생합니다. 관리자는 개발자에게 개발 프로세스를 훨씬 더 쉽게 만들고 모듈화할 수 있도록 배포 패키지를 작게 유지하라고 지시했습니다. 이는 또한 종속성을 설치하고 함수 코드와 함께 패키지할 때 발생할 수 있는 오류를 방지하는 데 도움이 됩니다.다음 옵션 중 개발자가 구현해야 하는 가장 적합한 솔루션은 무엇입니까?,대신 배포 패키지를 TAR 파일로 압축하세요.,대신 함수의 다른 종속성을 별도의 Lambda 계층으로 업로드하십시오.,배포 패키지를 S3에 업로드합니다.,배포 패키지를 다시 압축하여 zip 파일을 추가로 압축합니다.,,,0,,
udemy,DVA-02,196,"You are developing an online learning platform using Lambda, Elastic Beanstalk, and DynamoDB. There is a requirement that whenever a new customer is added to the DynamoDB table, it will invoke a Lambda function that sends a welcome email to the customer. Which of the following is the MOST suitable solution that you should use to implement this feature?",A,A,Enable DynamoDB Streams and configure it as the event source for the Lambda function.,Use CloudWatch Events to track all new data in your table and configure it as the event source for the Lambda function.,Enable DynamoDB Transactions and configure it as the event source for the Lambda function.,Use Amazon Kinesis Data Streams to track all new data in your table and configure it as the event source for the Lambda function.,,,,"귀하는 Lambda, Elastic Beanstalk 및 DynamoDB를 사용하여 온라인 학습 플랫폼을 개발하고 있습니다. DynamoDB 테이블에 새로운 고객이 추가될 때마다 고객에게 환영 이메일을 보내는 Lambda 함수를 호출해야 한다는 요구 사항이 있습니다.다음 중 이 기능을 구현하는 데 사용해야 하는 가장 적합한 솔루션은 무엇입니까?",DynamoDB 스트림을 활성화하고 이를 Lambda 함수의 이벤트 소스로 구성합니다.,CloudWatch 이벤트를 사용하여 테이블의 모든 새 데이터를 추적하고 이를 Lambda 함수의 이벤트 소스로 구성합니다.,DynamoDB 트랜잭션을 활성화하고 이를 Lambda 함수의 이벤트 소스로 구성합니다.,Amazon Kinesis Data Streams를 사용하여 테이블의 모든 새 데이터를 추적하고 이를 Lambda 함수의 이벤트 소스로 구성합니다.,,,0,,
udemy,DVA-02,197,"You are developing a high-traffic online stocks trading application, which will be hosted in an ECS Cluster and will be accessed by thousands of investors for intraday stocks trading. Each task of the cluster should be evenly placed across multiple Availability Zones to avoid any service disruptions. Which of the following is the MOST suitable placementStrategy configuration that you should use in your task definition?",B,B,"""placementStrategy"": [  {    ""field"": ""memory"",    ""type"": ""binpack""  }]","""placementStrategy"": [  {    ""field"": ""attribute:ecs.availability-zone"",    ""type"": ""spread""  }]","""placementStrategy"": [  {    ""type"": ""random""  }]","""placementStrategy"": [  {    ""field"": ""instanceId"",    ""type"": ""spread""  }]",,,,귀하는 ECS 클러스터에서 호스팅되고 일중 주식 거래를 위해 수천 명의 투자자가 액세스할 트래픽이 많은 온라인 주식 거래 애플리케이션을 개발하고 있습니다. 서비스 중단을 방지하려면 클러스터의 각 작업을 여러 가용 영역에 고르게 배치해야 합니다.다음 중 placementStrategy작업 정의에 사용해야 하는 가장 적합한 구성은 무엇입니까?,"""placementStrategy"" : [   {    ""필드"" : ""메모리"" ,     ""유형"" : ""빈팩""   }]","""placementStrategy"" : [   {    ""필드"" : ""속성:ecs.availability-zone"" ,     ""유형"" : ""확산""   }]","""placementStrategy"" : [   {    ""유형"" : ""무작위""   }]","""placementStrategy"" : [   {    ""필드"" : ""instanceId"" ,     ""유형"" : ""확산""   }]",,,0,,
udemy,DVA-02,198,"A company decided to re-use the same Lambda function for multiple stages of their API, but the function should read data from a different Amazon DynamoDB table depending on which stage is being called. In order to accomplish this, they instructed the developer to pass configuration parameters to a Lambda function through mapping templates in API Gateway. Which of the following is the MOST suitable solution that the developer should use to meet this requirement?",B,B,Set up an API Gateway Private Integration to the Lambda function.,Use Stage Variables.,Set up traffic shifting with Lambda Aliases.,Create environment variables in the Lambda function.,,,,한 회사는 API의 여러 단계에 동일한 Lambda 함수를 재사용하기로 결정했습니다. 하지만 이 함수는 호출되는 단계에 따라 다른 Amazon DynamoDB 테이블에서 데이터를 읽어야 합니다. 이를 달성하기 위해 개발자는 API 게이트웨이의 매핑 템플릿을 통해 구성 매개변수를 Lambda 함수에 전달하도록 지시했습니다.다음 중 개발자가 이 요구 사항을 충족하기 위해 사용해야 하는 가장 적합한 솔루션은 무엇입니까?,Lambda 함수에 대한 API 게이트웨이 프라이빗 통합을 설정합니다.,단계 변수를 사용하십시오.,Lambda 별칭을 사용하여 트래픽 이동을 설정합니다.,Lambda 함수에서 환경 변수를 생성합니다.,,,0,,
udemy,DVA-02,199,"The current application deployment process of a company is tedious and is prone to errors. They asked a developer to set up CodeDeploy as their deployment service, which can automate their application deployments on their hybrid cloud architecture. Which of the following deployment types does CodeDeploy support? (Select TWO.)",BE,BE,Blue/green deployments to on-premises servers.,Blue/green deployments to ECS.,In-place deployments to AWS Lambda.,Rolling deployments to ECS.,In-place deployments to on-premises servers,,,현재 회사의 애플리케이션 배포 프로세스는 지루하고 오류가 발생하기 쉽습니다. 그들은 개발자에게 하이브리드 클라우드 아키텍처에서 애플리케이션 배포를 자동화할 수 있는 배포 서비스로 CodeDeploy를 설정하도록 요청했습니다.다음 중 CodeDeploy가 지원하는 배포 유형은 무엇입니까? (2개를 선택하세요.),온프레미스 서버에 블루/그린 배포.,ECS에 대한 블루/그린 배포.,AWS Lambda에 대한 현재 위치 배포.,ECS에 대한 롤링 배포.,온프레미스 서버에 대한 내부 배포,,0,,
udemy,DVA-02,200,"A developer has recently released a new Lambda function which calculates accruals, interests, and other financial data. This function must have a streamlined integration setup with API Gateway. The requirement is to pass the incoming request from the client as the input to the backend Lambda function, via HTTPS, in the following format:  {    ""resource"": ""Resource path"",    ""path"": ""Path parameter"",    ""httpMethod"": ""Incoming request's method name""    ""headers"": {String containing incoming request headers}    ""multiValueHeaders"": {List of strings containing incoming request headers}    ""queryStringParameters"": {query string parameters }    ""multiValueQueryStringParameters"": {List of query string parameters}    ""pathParameters"":  {path parameters}    ""stageVariables"": {Applicable stage variables}    ""requestContext"": {Request context, including authorizer-returned key-value pairs}    ""body"": ""A JSON string of the request payload.""    ""isBase64Encoded"": ""A boolean flag to indicate if the applicable request payload is Base64-encode""}Which of the following options is the MOST appropriate method to use to meet this requirement?",A,A,Lambda proxy integration,HTTP Proxy integration,Lambda custom integration,HTTP custom integration,,,,"한 개발자는 최근 발생액, 이자 및 기타 금융 데이터를 계산하는 새로운 Lambda 함수를 출시했습니다. 이 기능에는 API 게이트웨이와의 간소화된 통합 설정이 있어야 합니다. 요구 사항은 클라이언트에서 들어오는 요청을 HTTPS를 통해 다음 형식으로 백엔드 Lambda 함수에 대한 입력으로 전달하는 것입니다.  {    ""resource"" : ""리소스 경로"" ,     ""path"" : ""경로 매개변수"" ,     ""httpMethod"" : ""들어오는 요청의 메소드 이름""     ""headers"" : { 들어오는 요청 헤더가 포함된 문자열 }     ""multiValueHeaders"" : { 들어오는 요청 헤더가 포함된 문자열 목록 }     ""queryStringParameters"" : { 쿼리 문자열 매개변수 }     ""multiValueQueryStringParameters"" : { 쿼리 문자열 매개변수 목록 }     ""pathParameters"" : { 경로 매개변수 }      ""stageVariables"" : { 적용 가능한 단계 변수 }     ""requestContext"" : { 승인자 - 반환 된 키 - 값 쌍을 포함한 요청 컨텍스트 }     ""body"" : ""요청 페이로드의 JSON 문자열입니다.""     ""isBase64Encoded"" : ""적용 가능한 요청 페이로드가 Base64 인코딩인지 여부를 나타내는 부울 플래그"" }다음 옵션 중 이 요구 사항을 충족하는 데 사용하는 가장 적절한 방법은 무엇입니까?",Lambda 프록시 통합,HTTP 프록시 통합,Lambda 사용자 정의 통합,HTTP 사용자 정의 통합,,,0,,
udemy,DVA-02,201,"Your team is developing a serverless application, which is composed of multiple Lambda functions which process data from an SQS queue and stores the results to an RDS database. To comply with the strict IT policy of the company, you were instructed to configure these functions to share the same connection string that should be properly secured and encrypted. What should you do to protect, encrypt, and share your database credentials in AWS?",A,A,Use AWS Systems Manager Parameter Store as a Secure String Parameter.,Encrypt the database credentials and store them in an S3 bucket which the Lambda functions can fetch.,Use IAM DB Authentication in RDS to allow encrypted connections from each Lambda function.,Store the database credentials as environment variables with KMS encryption which will be shared by the Lambda functions.,,,,"귀하의 팀은 SQS 대기열의 데이터를 처리하고 결과를 RDS 데이터베이스에 저장하는 여러 Lambda 함수로 구성된 서버리스 애플리케이션을 개발 중입니다. 회사의 엄격한 IT 정책을 준수하기 위해 적절하게 보호되고 암호화되어야 하는 동일한 연결 문자열을 공유하도록 이러한 기능을 구성하라는 지시를 받았습니다.AWS에서 데이터베이스 자격 증명을 보호, 암호화 및 공유하려면 어떻게 해야 합니까?",AWS 시스템 관리자 매개변수 저장소를 보안 문자열 매개변수로 사용합니다.,데이터베이스 자격 증명을 암호화하여 Lambda 함수가 가져올 수 있는 S3 버킷에 저장합니다.,RDS에서 IAM DB 인증을 사용하여 각 Lambda 함수의 암호화된 연결을 허용합니다.,Lambda 함수에서 공유할 KMS 암호화를 사용하여 데이터베이스 자격 증명을 환경 변수로 저장합니다.,,,0,,
udemy,DVA-02,202,"You are designing an online medical appointment system that allows patients to book an appointment with their preferred doctor at medical centers all over the country. The DynamoDB Streams feature is enabled in your DynamoDB database, which allows you to capture information about every modification to data items in your table. A Lambda function integrated with CloudWatch Events is used to process the data stream every 36 hours then store the results in an S3 bucket. However, you noticed that there are a lot of updated data in DynamoDB which are not sent to your S3 bucket even though there are no errors in the logs. Which of the following is the MOST appropriate solution for this issue?",B,B,Set the value of StreamViewType parameter in DynamoDB Streams to NEW_AND_OLD_IMAGES.,Decrease the interval of running your function to 24 hours.,Increase the interval of running your function to 48 hours.,Set the value of StreamViewType parameter in DynamoDB Streams to NEW_IMAGE.,,,,귀하는 전국의 의료기관에서 환자가 선호하는 의사와 진료 예약을 할 수 있는 온라인 진료 예약 시스템을 설계하고 있습니다. DynamoDB 스트림 기능은 DynamoDB 데이터베이스에서 활성화되어 테이블의 데이터 항목에 대한 모든 수정 사항에 대한 정보를 캡처할 수 있습니다. CloudWatch 이벤트와 통합된 Lambda 함수는 36시간마다 데이터 스트림을 처리한 다음 결과를 S3 버킷에 저장하는 데 사용됩니다. 그러나 로그에 오류가 없음에도 불구하고 S3 버킷으로 전송되지 않은 업데이트된 데이터가 DynamoDB에 많이 있다는 사실을 발견했습니다.다음 중 이 문제에 대한 가장 적절한 해결책은 무엇입니까?,StreamViewTypeDynamoDB 스트림의 매개변수 값을 로 설정합니다 NEW_AND_OLD_IMAGES.,함수 실행 간격을 24시간으로 줄입니다.,함수 실행 간격을 48시간으로 늘립니다.,StreamViewTypeDynamoDB 스트림의 매개변수 값을 로 설정합니다 NEW_IMAGE.,,,0,,
udemy,DVA-02,203,You are developing an application that continuously collects data about player-game interactions and feeds the real-time data into your gaming platform. There is a requirement to make the system highly scalable to accommodate the sudden influx of gamers that will use the platform. Which AWS service will help you achieve this?,D,D,AWS Elastic Map Reduce,AWS DynamoDB,AWS Redshift,AWS Kinesis Data Stream,,,,플레이어-게임 상호 작용에 대한 데이터를 지속적으로 수집하고 실시간 데이터를 게임 플랫폼에 제공하는 애플리케이션을 개발 중입니다. 플랫폼을 사용하게 될 게이머의 갑작스러운 유입을 수용할 수 있도록 시스템의 확장성을 높여야 한다는 요구 사항이 있습니다.이를 달성하는 데 도움이 되는 AWS 서비스는 무엇입니까?,AWS Elastic Map 축소,AWS 다이나모DB,AWS 레드시프트,AWS Kinesis 데이터 스트림,,,0,,
udemy,DVA-02,204,"A medical technology company has a system hosted in AWS that manages their patients' high-resolution imaging records such as MRI, PET Positron Emission Tomography, CAT scan, and many others. For their archiving process, the records that are one year older are encrypted before they are archived to AWS Glacier. A doctor has a non-urgent request for a patient’s medical record from 2 years ago, which should be made available within 5 hours. Which of the following is the MOST cost-effective retrieval option to use in this scenario?",A,A,Standard Retrieval,Expedited Retrieval,Bulk Retrieval,Ranged Archive Retrievals,,,,"한 의료 기술 회사에는 MRI, PET 양전자 방출 단층촬영, CAT 스캔 등과 같은 환자의 고해상도 영상 기록을 관리하는 시스템이 AWS에 호스팅되어 있습니다. 보관 프로세스의 경우 1년이 지난 레코드는 AWS Glacier에 보관되기 전에 암호화됩니다. 의사가 환자의 2년 전 의료 기록에 대해 긴급하지 않은 요청을 했으며, 이 기록은 5시간 이내에 제공되어야 합니다.다음 중 이 시나리오에서 사용할 수 있는 가장 비용 효율적인 검색 옵션은 무엇입니까?",표준 검색,신속한 검색,대량 검색,범위별 아카이브 검색,,,0,,
udemy,DVA-02,205,A developer is using API Gateway Lambda Authorizer to securely authenticate the API requests to their web application. The authentication process should be implemented using a custom authorization scheme which accepts header and query string parameters from the API caller. Which of the following methods should the developer use to properly implement the above requirement?,D,D,Amazon Cognito User Pools Authorizer,Token-based Authorization,Cross-Account Lambda Authorizer,Request Parameter-based Authorization,,,,개발자는 API Gateway Lambda Authorizer를 사용하여 웹 애플리케이션에 대한 API 요청을 안전하게 인증하고 있습니다. 인증 프로세스는 API 호출자로부터 헤더 및 쿼리 문자열 매개변수를 허용하는 사용자 정의 인증 체계를 사용하여 구현되어야 합니다.위의 요구 사항을 올바르게 구현하기 위해 개발자는 다음 중 어떤 방법을 사용해야 합니까?,Amazon Cognito 사용자 풀 권한 부여자,토큰 기반 인증,교차 계정 Lambda 권한 부여자,매개변수 기반 승인 요청,,,0,,
udemy,DVA-02,206,You are using an AWS Lambda function to process records in an Amazon Kinesis Data Streams stream which has 100 active shards. The Lambda function takes an average of 10 seconds to process the data and the stream is receiving 50 new items per second. Which of the following statements are TRUE regarding this scenario?,D,D,The Lambda function has 500 concurrent executions.,The Kinesis shards must be merged to increase the data capacity of the stream as well as the concurrency execution of the Lambda function.,The Lambda function will throttle the incoming requests due to the excessive number of Kinesis shards.,There will be at most 100 Lambda function invocations running concurrently.,,,,AWS Lambda 함수를 사용하여 100개의 활성 샤드가 있는 Amazon Kinesis Data Streams 스트림의 레코드를 처리하고 있습니다. Lambda 함수는 데이터를 처리하는 데 평균 10초가 걸리며 스트림은 초당 50개의 새 항목을 수신합니다.이 시나리오와 관련하여 다음 설명 중 참인 것은 무엇입니까?,Lambda 함수에는 500개의 동시 실행이 있습니다.,스트림의 데이터 용량과 Lambda 함수의 동시 실행을 늘리려면 Kinesis 샤드를 병합해야 합니다.,Lambda 함수는 Kinesis 샤드 수가 너무 많아 수신 요청을 제한합니다.,동시에 실행되는 Lambda 함수 호출은 최대 100개입니다.,,,0,,
udemy,DVA-02,207,A company has a static website running in an Auto Scaling group of EC2 instances which they want to convert as a dynamic e-commerce web portal. One of the requirements is to use HTTPS to improve the security of their portal and also improve their search ranking as a reputable and secure site. A developer recently requested an SSL/TLS certificate from a third-party certificate authority (CA) which is ready to be imported to AWS.Which of the following services can the developer use to safely import the SSL/TLS certificate? (Select TWO.),AB,AB,AWS Certificate Manager,IAM certificate store,A private S3 bucket with versioning enabled,Amazon Cognito,CloudFront,,,회사에는 동적 전자 상거래 웹 포털로 변환하려는 EC2 인스턴스의 Auto Scaling 그룹에서 실행되는 정적 웹 사이트가 있습니다. 요구 사항 중 하나는 HTTPS를 사용하여 포털의 보안을 향상하고 평판이 좋고 안전한 사이트로서 검색 순위를 높이는 것입니다. 최근 한 개발자가 AWS로 가져올 준비가 된 타사 인증 기관(CA)에 SSL/TLS 인증서를 요청했습니다.다음 중 개발자가 SSL/TLS 인증서를 안전하게 가져오기 위해 사용할 수 있는 서비스는 무엇입니까? (2개를 선택하세요.),AWS 인증서 관리자,IAM 인증서 저장소,버전 관리가 활성화된 프라이빗 S3 버킷,아마존 코그니토,CloudFront,,0,,
udemy,DVA-02,208,"You are developing a Node.js application which uses a DynamoDB database. The architecture should be designed to allow you to query over a single partition of the table, as specified by the partition key value in the query. It should also support both eventually consistent and strongly consistent reads. What should you do to satisfy this requirement?",B,B,Add a global secondary index after the table has been created.,Add a local secondary index before the table is created.,Add a local secondary index after the table has been created.,Add a global secondary index before the table is created.,,,,DynamoDB 데이터베이스를 사용하는 Node.js 애플리케이션을 개발 중입니다. 쿼리의 파티션 키 값에 지정된 대로 테이블의 단일 파티션을 쿼리할 수 있도록 아키텍처를 설계해야 합니다. 또한 최종적으로 일관된 읽기와 강력한 일관된 읽기를 모두 지원해야 합니다.이 요구 사항을 충족하려면 어떻게 해야 합니까?,테이블이 생성된 후 글로벌 보조 인덱스를 추가합니다.,테이블이 생성되기 전에 로컬 보조 인덱스를 추가하세요.,테이블이 생성된 후 로컬 보조 인덱스를 추가합니다.,테이블이 생성되기 전에 글로벌 보조 인덱스를 추가하세요.,,,0,,
udemy,DVA-02,209,A developer is designing a multi-threaded e-commerce application that will be reading and writing data on a DynamoDB table. There will be a lot of people who will use the application to update the price of items in the table at the same time. The application should prevent an update operation from modifying an item if one of its attributes has a certain value. Which of the following is the most suitable solution that the developer should use in this application?,D,D,Use batch operations.,Use pessimistic locking and conditional writes.,Use atomic counters.,Use optimistic locking and conditional writes.,,,,개발자는 DynamoDB 테이블에서 데이터를 읽고 쓰는 멀티 스레드 전자 상거래 애플리케이션을 설계하고 있습니다. 테이블에 있는 품목의 가격을 동시에 업데이트하기 위해 애플리케이션을 사용하는 사람들이 많이 있을 것입니다. 애플리케이션은 해당 속성 중 하나에 특정 값이 있는 경우 업데이트 작업으로 인해 항목이 수정되지 않도록 해야 합니다.다음 중 개발자가 이 애플리케이션에 사용해야 하는 가장 적합한 솔루션은 무엇입니까?,일괄 작업을 사용합니다.,비관적 잠금 및 조건부 쓰기를 사용합니다.,원자 카운터를 사용하십시오.,낙관적 잠금 및 조건부 쓰기를 사용합니다.,,,0,,
udemy,DVA-02,210,"For application deployments, a company is using CloudFormation templates, which are regularly updated to map the latest AMI IDs. A developer was assigned to automate the process since the current set up takes a lot of time to execute on a regular basis. Which of the following is the MOST suitable solution that the developer should implement to satisfy this requirement?",D,D,Integrate AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments.,Integrate CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments.,Set up your Systems Manager State Manager to store the latest AMI IDs and integrate it with your CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template.,"Set up CloudFormation with Systems Manager Parameter Store to retrieve the latest AMI IDs for your template. Whenever you decide to update the EC2 instances, call the update-stack API in CloudFormation in your CloudFormation template.",,,,애플리케이션 배포의 경우 회사는 최신 AMI ID를 매핑하기 위해 정기적으로 업데이트되는 CloudFormation 템플릿을 사용하고 있습니다. 현재 설정을 정기적으로 실행하는 데 많은 시간이 걸리므로 프로세스를 자동화하도록 개발자가 지정되었습니다.다음 중 개발자가 이 요구 사항을 충족하기 위해 구현해야 하는 가장 적합한 솔루션은 무엇입니까?,AWS Service Catalog를 AWS Config와 통합하면 자동으로 최신 AMI를 가져와서 후속 배포에 사용할 수 있습니다.,CloudFormation을 AWS Service Catalog와 통합하여 최신 AMI ID를 가져와 자동으로 후속 배포에 사용합니다.,최신 AMI ID를 저장하고 이를 CloudFormation 템플릿과 통합하도록 Systems Manager State Manager를 설정하세요. CloudFormation 템플릿에서 EC2 인스턴스를 업데이트하기로 결정할 때마다 CloudFormation에서 업데이트 스택 API를 호출하세요.,템플릿에 대한 최신 AMI ID를 검색하려면 Systems Manager Parameter Store로 CloudFormation을 설정하세요. EC2 인스턴스를 업데이트하기로 결정할 때마다 CloudFormation 템플릿의 CloudFormation에서 업데이트 스택 API를 호출하세요.,,,0,,
udemy,DVA-02,211,"A developer is working on an online game based on a popular movie, which may have a few users on its first few weeks of release. However, it is expected to grow and reach millions of concurrent users, with terabytes or more of new data generated per day. The database must seamlessly handle hundreds of thousands of reads and writes per second.Which of the following would be the MOST ideal data store to choose for this application?",B,B,RDS,DynamoDB,Redshift,S3,,,,"개발자가 인기 영화를 기반으로 한 온라인 게임을 개발 중인데, 출시 후 처음 몇 주 동안 사용자 수가 적을 수 있습니다. 그러나 하루에 테라바이트 이상의 새로운 데이터가 생성되면서 수백만 명의 동시 사용자를 확보하고 성장할 것으로 예상됩니다. 데이터베이스는 초당 수십만 건의 읽기 및 쓰기를 원활하게 처리해야 합니다.다음 중 이 애플리케이션에 선택하기에 가장 이상적인 데이터 저장소는 무엇입니까?",RDS,DynamoDB,적색편이,S3,,,0,,
udemy,DVA-02,212,"A batch application is hosted in an Auto Scaling group of On-Demand EC2 instances which consumes and processes the messages from an SQS queue. The system works well but there are times that the consumers process the same message twice. Upon investigation, you found out that if the consumer takes a long time to process the message, that exact same message becomes available again to other consumers, which causes duplicate processing. Which of the following is the BEST solution that the developer should implement to meet this requirement?",C,C,Configure the queue to use short polling by setting the WaitTimeSeconds parameter of the ReceiveMessage request to 0.,Configure the queue to use long polling by setting the Receive Message Wait Time parameter to a value greater than 0.,Set the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue.,Postpone the delivery of new messages by using a delay queue.,,,,배치 애플리케이션은 SQS 대기열의 메시지를 사용하고 처리하는 온디맨드 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅됩니다. 시스템은 잘 작동하지만 소비자가 동일한 메시지를 두 번 처리하는 경우가 있습니다. 조사 결과 소비자가 메시지를 처리하는 데 오랜 시간이 걸리면 동일한 메시지가 다른 소비자에게 다시 제공되어 중복 처리가 발생한다는 사실을 발견했습니다.다음 중 개발자가 이 요구 사항을 충족하기 위해 구현해야 하는 가장 좋은 솔루션은 무엇입니까?,WaitTimeSeconds요청 매개변수를 0으로 설정하여 짧은 폴링을 사용하도록 대기열을 구성합니다 ReceiveMessage.,Receive Message Wait Time매개변수를 0보다 큰 값으로 설정하여 긴 폴링을 사용하도록 대기열을 구성합니다 .,애플리케이션이 대기열에서 메시지를 처리하고 삭제하는 데 걸리는 최대 시간으로 제한 시간 초과를 설정합니다.,지연 대기열을 사용하여 새 메시지 배달을 연기합니다.,,,0,,
udemy,DVA-02,213,"A mobile game has a serverless backend in AWS which is composed of Lambda, API Gateway, and DynamoDB. It writes 100 items per second to the DynamoDB table and the size is 1.5 KB per item. The table has a provisioned WCU of 100 but the write requests are still being throttled by DynamoDB. What is the MOST suitable solution in order to rectify this throttling issue?",C,C,Enable DynamoDB Accelerator (DAX).,Implement database caching with an ElastiCache cluster.,Increase the WCU to 200.,Use strong consistency in the write operations.,,,,"모바일 게임에는 AWS에 Lambda, API Gateway 및 DynamoDB로 구성된 서버리스 백엔드가 있습니다. DynamoDB 테이블에 초당 100개의 항목을 쓰고 크기는 항목당 1.5KB입니다. 테이블에는 100개의 프로비저닝된 WCU가 있지만 쓰기 요청은 여전히 ​​DynamoDB에 의해 조절되고 있습니다.이 조절 문제를 해결하기 위해 가장 적합한 솔루션은 무엇입니까?",DynamoDB Accelerator(DAX)를 활성화합니다.,ElastiCache 클러스터를 사용하여 데이터베이스 캐싱을 구현합니다.,WCU를 200으로 늘립니다.,쓰기 작업에 강력한 일관성을 사용합니다.,,,0,,
udemy,DVA-02,214,"An application architect manages several AWS accounts for staging, testing, and production environments, which are used by several development teams. For application deployments, the developers use the similar base CloudFormation template for their applications. Which of the following can allow the developer to effectively manage the updates on this template across all AWS accounts with minimal effort?",D,D,Upload the CloudFormation templates to CodeCommit and use a combination of CodeDeploy and CodePipeline to manage the deployment to multiple accounts.,Create and manage stacks on multiple AWS accounts using CloudFormation Change Sets.,Define and manage stack instances on multiple AWS Accounts using CloudFormation Stack Instances.,Update the stacks on multiple AWS accounts using CloudFormation StackSets.,,,,"애플리케이션 설계자는 여러 개발 팀에서 사용하는 스테이징, 테스트 및 프로덕션 환경을 위한 여러 AWS 계정을 관리합니다. 애플리케이션 배포의 경우 개발자는 애플리케이션에 유사한 기본 CloudFormation 템플릿을 사용합니다.다음 중 개발자가 최소한의 노력으로 모든 AWS 계정에서 이 템플릿의 업데이트를 효과적으로 관리할 수 있는 방법은 무엇입니까?",CloudFormation 템플릿을 CodeCommit에 업로드하고 CodeDeploy와 CodePipeline의 조합을 사용하여 여러 계정에 대한 배포를 관리합니다.,CloudFormation 변경 세트를 사용하여 여러 AWS 계정에서 스택을 생성하고 관리합니다.,CloudFormation 스택 인스턴스를 사용하여 여러 AWS 계정에서 스택 인스턴스를 정의하고 관리합니다.,CloudFormation StackSets를 사용하여 여러 AWS 계정의 스택을 업데이트합니다.,,,0,,
udemy,DVA-02,215,"An EBS-backed EC2 instance has been recently reported to contain a malware that could spread to your other instances. To fix this security vulnerability, you will need to attach its root EBS volume to a new EC2 instance which hosts a security program that can scan viruses, worms, Trojan horses, or spyware. What steps would you take to detach the root volume from the compromised EC2 instance?",C,C,"Unmount the volume, stop the instance, and then detach.",Unmount the volume from the OS and then detach.,Stop the instance then detach the volume.,Detach the volume from the AWS Console. AWS takes care of unmounting the volume for you.,,,,"EBS 지원 EC2 인스턴스에는 다른 인스턴스로 확산될 수 있는 악성 코드가 포함되어 있는 것으로 최근 보고되었습니다. 이 보안 취약점을 해결하려면 루트 EBS 볼륨을 바이러스, 웜, 트로이 목마 또는 스파이웨어를 검사할 수 있는 보안 프로그램을 호스팅하는 새 EC2 인스턴스에 연결해야 합니다.손상된 EC2 인스턴스에서 루트 볼륨을 분리하려면 어떤 단계를 수행해야 합니까?",볼륨을 마운트 해제하고 인스턴스를 중지한 후 분리합니다.,OS에서 볼륨을 마운트 해제한 다음 분리합니다.,인스턴스를 중지한 다음 볼륨을 분리합니다.,AWS 콘솔에서 볼륨을 분리합니다. AWS가 볼륨 마운트 해제를 자동으로 처리합니다.,,,0,,
udemy,DVA-02,216,"A developer is designing an application which will be hosted in ECS and uses an EC2 launch type. You need to group your container instances by certain attributes such as Availability Zone, instance type, or custom metadata. After you have defined a group of container instances, you will need to customize Amazon ECS to place tasks on container instances based on the group you specified. Which of the following ECS features provides you with expressions that you can use to group container instances by a specific attribute?",B,B,Task Placement Constraints,Cluster Query Language,Task Placement Strategies,Task Groups,,,,"개발자는 ECS에서 호스팅되고 EC2 시작 유형을 사용하는 애플리케이션을 설계하고 있습니다. 가용 영역, 인스턴스 유형 또는 사용자 지정 메타데이터와 같은 특정 속성을 기준으로 컨테이너 인스턴스를 그룹화해야 합니다. 컨테이너 인스턴스 그룹을 정의한 후에는 지정한 그룹을 기반으로 컨테이너 인스턴스에 작업을 배치하도록 Amazon ECS를 사용자 지정해야 합니다.다음 ECS 기능 중 특정 속성을 기준으로 컨테이너 인스턴스를 그룹화하는 데 사용할 수 있는 표현식을 제공하는 것은 무엇입니까?",작업 배치 제약,클러스터 쿼리 언어,작업 배치 전략,작업 그룹,,,0,,
udemy,DVA-02,217,"You are developing a new batch job for the enterprise application suite in your company, which is hosted in an Auto Scaling group of EC2 instances behind an ELB. The application is using an S3 bucket configured with Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS). The batch job must upload files to the bucket using the default AWS KMS key to protect the data at rest. What should you do to satisfy this requirement with the LEAST amount of configuration?",A,A,Include the x-amz-server-side-encryption header with a value of aws:kms in your upload request.,Include the x-amz-server-side-encryption header with a value of AES256 in your upload request.,"Include the x-amz-server-side​-encryption​-customer-algorithm, x-amz-server-side-encryption-customer-key, and x-amz-server-side-encryption-customer-key-MD5 headers with appropriate values in the upload request.",Include the x-amz-server-side-encryption header with a value of aws:kms as well as the x-amz-server-side-encryption-aws-kms-key-id header containing the ID of the default AWS KMS key in your upload request.,,,,회사에서 ELB 뒤에 있는 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅되는 엔터프라이즈 애플리케이션 제품군을 위한 새로운 배치 작업을 개발하고 있습니다. 애플리케이션은 AWS KMS 관리형 키(SSE-KMS)를 사용한 서버 측 암호화로 구성된 S3 버킷을 사용하고 있습니다. 배치 작업은 저장 데이터를 보호하기 위해 기본 AWS KMS 키를 사용하여 버킷에 파일을 업로드해야 합니다.최소한의 구성으로 이 요구 사항을 충족하려면 어떻게 해야 합니까?,업로드 요청에 x-amz-server-side-encryption헤더 값을 포함하세요 .aws:kms,업로드 요청에 x-amz-server-side-encryption헤더 값을 포함하세요 .AES256,"업로드 요청에 적절한 값과 함께 x-amz-server-side​-encryption​-customer-algorithm, x-amz-server-side-encryption-customer-key및 헤더를 포함합니다 .x-amz-server-side-encryption-customer-key-MD5",업로드 요청에 기본 AWS KMS 키의 ID가 포함된 헤더 와 x-amz-server-side-encryption값이 포함된 헤더 를 포함합니다 .aws:kmsx-amz-server-side-encryption-aws-kms-key-id,,,0,,
udemy,DVA-02,218,A single docker container environment is hosted in Elastic Beanstalk. Your manager instructed you to ensure that the compute resources maintain full capacity during deployments to avoid any degradation of the service or possible down time.Which of the following deployment methods should you use to satisfy the given requirement? (Select TWO.),BE,BE,Rolling,Immutable,All at once,Canary,Rolling with additional batch,,,단일 Docker 컨테이너 환경은 Elastic Beanstalk에서 호스팅됩니다. 관리자는 서비스 저하나 가동 중지 시간을 방지하기 위해 배포 중에 컴퓨팅 리소스가 전체 용량을 유지하도록 지시했습니다.주어진 요구 사항을 충족하려면 다음 배포 방법 중 어떤 것을 사용해야 합니까? (2개를 선택하세요.),구르는,불변,한꺼번에,카나리아,추가 배치로 롤링,,0,,
udemy,DVA-02,219,There is a requirement to postpone the delivery of new messages to an SQS queue for a number of seconds. You must configure the queue to ensure that any messages that you send remain invisible to consumers for a duration of time specified. Which of the following SQS feature should you use to meet this requirement?,C,C,Short Polling,Long Polling,Delay Queue,Visibility Timeouts,,,,몇 초 동안 SQS 대기열로의 새 메시지 전달을 연기해야 ​​한다는 요구 사항이 있습니다. 보내는 모든 메시지가 지정된 기간 동안 소비자에게 표시되지 않도록 큐를 구성해야 합니다.이 요구 사항을 충족하려면 다음 SQS 기능 중 무엇을 사용해야 합니까?,짧은 폴링,긴 폴링,지연 대기열,가시성 시간 초과,,,0,,
udemy,DVA-02,220,"A developer is designing a multitiered system which utilizes various AWS resources. The application will be hosted in Elastic Beanstalk, which uses an RDS database and an S3 bucket that is configured to use Server-Side Encryption with Customer-Provided Encryption Keys (SSE-C). In this configuration, Amazon S3 does not store the encryption key you provide but instead, stores a randomly salted hash-based message authentication code (HMAC) value of the encryption key in order to validate future requests. Which of the following is a valid consideration that the developer should keep in mind when implementing this architecture?",D,D,The salted HMAC value can be used to decrypt the contents of the encrypted object.,"If you lose the encryption key, the salted HMAC value can be used to decrypt the object.",The salted HMAC value can be used to derive the value of the encryption key.,"If you lose the encryption key, you lose the object.",,,,개발자는 다양한 AWS 리소스를 활용하는 다중 계층 시스템을 설계하고 있습니다. 애플리케이션은 SSE-C(고객 제공 암호화 키)를 사용한 서버 측 암호화를 사용하도록 구성된 RDS 데이터베이스와 S3 버킷을 사용하는 Elastic Beanstalk에서 호스팅됩니다. 이 구성에서 Amazon S3는 사용자가 제공한 암호화 키를 저장하지 않고 대신 향후 요청을 검증하기 위해 무작위로 솔트 처리된 암호화 키의 해시 기반 메시지 인증 코드(HMAC) 값을 저장합니다.다음 중 개발자가 이 아키텍처를 구현할 때 염두에 두어야 할 유효한 고려 사항은 무엇입니까?,솔트 처리된 HMAC 값은 암호화된 객체의 콘텐츠를 해독하는 데 사용될 수 있습니다.,암호화 키를 분실한 경우 솔트 처리된 HMAC 값을 사용하여 객체를 해독할 수 있습니다.,솔트 처리된 HMAC 값을 사용하여 암호화 키 값을 파생할 수 있습니다.,암호화 키를 분실하면 객체도 손실됩니다.,,,0,,
udemy,DVA-02,221,"A global financial company has hundreds of users from all over the world that regularly upload terabytes of transactional data to a centralized S3 bucket. You noticed that there are some users from different parts of the globe that take a lot of time to upload their data, which causes delays in the processing. You need to improve data throughput and ensure consistently fast data transfer to the S3 bucket regardless of the user's location. Which of the following features should you use to satisfy the above requirement?",A,A,Amazon S3 Transfer Acceleration,AWS Transfer for SFTP,CloudFront,AWS Direct Connect,,,,한 글로벌 금융 회사에는 전 세계 수백 명의 사용자가 정기적으로 테라바이트 규모의 거래 데이터를 중앙 집중식 S3 버킷에 업로드합니다. 전 세계 여러 지역의 일부 사용자가 데이터를 업로드하는 데 많은 시간이 걸려 처리가 지연되는 것을 발견했습니다. 데이터 처리량을 개선하고 사용자 위치에 관계없이 S3 버킷으로 지속적으로 빠른 데이터 전송을 보장해야 합니다.위의 요구 사항을 충족하려면 다음 중 어떤 기능을 사용해야 합니까?,Amazon S3 전송 가속화,SFTP를 위한 AWS 전송,CloudFront,AWS 다이렉트 커넥트,,,0,,
udemy,DVA-02,222,"A clickstream application uses Amazon Kinesis Data Stream for real-time processing. PutRecord API calls are being used by the producer to send data to the stream. However, there are cases where the producer intermittently restarted while doing the processing, which resulted in sending the same data twice to the stream. This inadvertently causes duplication of entries in the data stream, which affects the processing of the consumers.Which of the following should you implement to resolve this issue?",D,D,Add more shards.,Split shards of the data stream.,Merge shards of the data stream.,Embed a primary key within the record.,,,,클릭스트림 애플리케이션은 실시간 처리를 위해 Amazon Kinesis Data Stream을 사용합니다. PutRecordAPI 호출은 생산자가 스트림에 데이터를 보내는 데 사용됩니다. 그러나 처리를 수행하는 동안 생산자가 간헐적으로 다시 시작하여 동일한 데이터를 스트림에 두 번 보내는 경우가 있습니다. 이로 인해 실수로 데이터 스트림의 항목이 중복되어 소비자 처리에 영향을 줍니다.이 문제를 해결하려면 다음 중 무엇을 구현해야 합니까?,샤드를 더 추가하세요.,데이터 스트림의 분할을 분할합니다.,데이터 스트림의 샤드를 병합합니다.,레코드 내에 기본 키를 포함합니다.,,,0,,
udemy,DVA-02,223,"A developer runs a shell script that uses the AWS CLI to upload a large file to an S3 bucket, which includes an AWS KMS key. An Access Denied error always shows up whenever the developer uploads a file with a size of 100 GB or more. However, when he tried to upload a smaller file with the KMS key, the upload succeeds. Which of the following are possible reasons why this issue is happening? (Select TWO.)",AB,AB,The AWS CLI S3 commands perform a multipart upload when the file is large.,The developer does not have the kms:Decrypt permission.,The developer does not have the kms:Encrypt permission.,The maximum size that can be encrypted in KMS is only 100 GB.,The developer's IAM permission has an attached inline policy that restricts him from uploading a file to S3 with a size of 100 GB or more.,,,개발자는 AWS CLI를 사용하여 AWS KMS 키가 포함된 S3 버킷에 대용량 파일을 업로드하는 셸 스크립트를 실행합니다. Access Denied개발자가 100GB 이상의 파일을 업로드할 때마다 항상 오류가 표시됩니다 . 그러나 KMS 키를 사용하여 더 작은 파일을 업로드하려고 하면 업로드가 성공합니다.다음 중 이 문제가 발생하는 이유는 무엇입니까? (2개를 선택하세요.),AWS CLI S3 명령은 파일이 클 때 멀티파트 업로드를 수행합니다.,개발자에게 권한이 없습니다 kms:Decrypt.,개발자에게 권한이 없습니다 kms:Encrypt.,KMS에서 암호화할 수 있는 최대 크기는 100GB에 불과합니다.,개발자의 IAM 권한에는 100GB 이상의 파일을 S3에 업로드하는 것을 제한하는 인라인 정책이 첨부되어 있습니다.,,0,,
udemy,DVA-02,224,A developer wants to deploy a REST API using the CloudFormation template shown below:Which changes should be done so that the newly created API endpoint can be referenced to other stacks?,B,B,Add the AWS::Include transform in the original template to directly import the HelloWorldFunction resource to other templates.,Include the Export property in the original template's Outputs section. Then use the Fn::ImportValue function in other templates to retrieve the exported value.,Specify HelloWorldApias parameter when using the Fn::ImportValue function in other templates.,Include the Export property in the original template's Outputs section. Then use the Ref function in other templates to retrieve the exported value.,,,,개발자는 아래 표시된 CloudFormation 템플릿을 사용하여 REST API를 배포하려고 합니다.새로 생성된 API 엔드포인트를 다른 스택에서 참조할 수 있도록 하려면 어떤 변경을 수행해야 합니까?,AWS::Include리소스를 다른 템플릿으로 직접 가져오려면 원본 템플릿에 변환을 추가하세요 HelloWorldFunction.,Export원본 템플릿의 섹션에 속성을 포함합니다 Outputs. 그런 다음 Fn::ImportValue다른 템플릿의 함수를 사용하여 내보낸 값을 검색합니다.,다른 템플릿에서 HelloWorldApi해당 기능을 사용할 경우 매개변수로 지정하세요 .Fn::ImportValue,Export원본 템플릿의 섹션에 속성을 포함합니다 Outputs. 그런 다음 Ref다른 템플릿의 함수를 사용하여 내보낸 값을 검색합니다.,,,0,,
udemy,DVA-02,225,"An online role-playing video game requires cross-device syncing of application-related user data. It must synchronize the user profile data across mobile devices without requiring your own backend. When the device is online, it should synchronize data and notify other devices immediately that an update is available. Which of the following is the most suitable feature that you have to use to meet this requirement?",D,D,Amazon Cognito Identity Pools,AWS Device Farm,Amazon Cognito User Pools,Amazon Cognito Sync,,,,온라인 롤플레잉 비디오 게임에는 애플리케이션 관련 사용자 데이터의 장치 간 동기화가 필요합니다. 자체 백엔드 없이 모바일 장치 전체에서 사용자 프로필 데이터를 동기화해야 합니다. 장치가 온라인 상태이면 데이터를 동기화하고 업데이트가 가능하다는 사실을 다른 장치에 즉시 알려야 합니다.다음 중 이 요구 사항을 충족하기 위해 사용해야 하는 가장 적합한 기능은 무엇입니까?,Amazon Cognito 자격 증명 풀,AWS 디바이스 팜,Amazon Cognito 사용자 풀,Amazon 학습 동기화,,,0,,
udemy,DVA-02,226,"A developer is creating a React application whose source code is hosted in GitHub. To help ensure proper functionality and identify any UI issues before going live, the developer must perform end-to-end (E2E) testing using Cypress.Which combination of actions should the developer take? (Select Two)",CE,CE,Create an application in AWS Amplify Studio. Clone the application’s source code in a local environment and run amplify pull --appId APP_ID --envName ENV_NAME,Update the amplifyconfiguration.json with appropriate configuration settings for Cypress.,Connect the Github repository to AWS Amplify Hosting,Include the location of the Cypress configuration file in the aws-exports.js file.,Update the amplify.yml file with appropriate configuration settings for Cypress.,,,개발자가 GitHub에서 소스 코드가 호스팅되는 React 애플리케이션을 만들고 있습니다. 적절한 기능을 보장하고 실행하기 전에 UI 문제를 식별하려면 개발자는 Cypress를 사용하여 E2E(종단 간) 테스트를 수행해야 합니다.개발자는 어떤 작업 조합을 취해야 합니까? (2개 선택),AWS Amplify Studio에서 애플리케이션을 생성합니다. 로컬 환경에서 애플리케이션의 소스 코드를 복제하고 실행amplify pull --appId APP_ID --envName ENV_NAME,Cypress에 대한 적절한 구성 설정으로 업데이트합니다 amplifyconfiguration.json.,Github 리포지토리를 AWS Amplify 호스팅에 연결,파일 에 Cypress 구성 파일의 위치를 ​​포함합니다 aws-exports.js.,amplify.ymlCypress에 대한 적절한 구성 설정으로 파일을 업데이트합니다 .,,0,,
udemy,DVA-02,227,"An application is hosted in Elastic Beanstalk with an ElastiCache cluster that acts as a database cache layer for accessing its data in DynamoDB. It is currently configured to write the data to the cache only if there is a cache miss, which causes the data in the cache to become stale. A developer is instructed to ensure that the data in the cache is always current and to minimize wasted space in the cluster by automatically deleting the data that are never read. What is the BEST way to implement this to satisfy the given requirement?",D,D,Implement Lazy Loading in the application in conjunction with the Write Through caching strategy.,Use a Lazy Loading caching strategy.,Use a Write Through caching strategy.,Implement a Write Through caching strategy in the application and enable TTL in Elasticache.,,,,"애플리케이션은 DynamoDB의 데이터에 액세스하기 위한 데이터베이스 캐시 계층 역할을 하는 ElastiCache 클러스터를 통해 Elastic Beanstalk에서 호스팅됩니다. 현재는 캐시 누락이 있는 경우에만 캐시에 데이터를 쓰도록 구성되어 있으며, 이로 인해 캐시의 데이터가 오래되었습니다. 개발자는 캐시의 데이터를 항상 최신 상태로 유지하고 읽지 않은 데이터를 자동으로 삭제하여 클러스터에서 낭비되는 공간을 최소화하도록 지시받습니다.주어진 요구 사항을 충족시키기 위해 이를 구현하는 가장 좋은 방법은 무엇입니까?",Write Through 캐싱 전략과 함께 애플리케이션에서 지연 로딩을 구현합니다.,지연 로딩 캐싱 전략을 사용합니다.,Write Through 캐싱 전략을 사용합니다.,애플리케이션에서 Write Through 캐싱 전략을 구현하고  Elasticache에서 TTL을 활성화합니다.,,,0,,
udemy,DVA-02,228,A newly hired developer has been instructed to debug an application. She tried to access the X-Ray console to view service maps and segments but her current access is insufficient. Which of the following is the MOST appropriate managed policy that should be granted to the developer?,C,C,AWSXRayDaemonWriteAccess,AWSXrayFullAccess,AWSXrayReadOnlyAccess,AmazonS3ReadOnlyAccess,,,,새로 고용된 개발자는 애플리케이션을 디버깅하라는 지시를 받았습니다. 그녀는 서비스 맵과 구간을 보기 위해 X-Ray 콘솔에 액세스하려고 시도했지만 현재 액세스 권한이 충분하지 않습니다.다음 중 개발자에게 부여해야 하는 가장 적절한 관리형 정책은 무엇입니까?,AWSXRayDaemonWriteAccess,AWSXrayFullAccess,AWSXrayReadOnlyAccess,AmazonS3ReadOnlyAccess,,,0,,
udemy,DVA-02,229,A mobile game has a DynamoDB table named TutorialsDojoScores which keeps track of the users and their respective scores. Each item in the table is identified by the FighterId attribute as its partition key and the FightTitle attribute as the sort key. A developer needs to retrieve data from non-key attributes of the table named DojoTopScores and DojoDateTime attributes. Which type of index should the developer add in the table to speed up queries on non-key attributes?,D,D,Sparse Index,Local Secondary Index,Primary Index,Global Secondary Index,,,,모바일 게임에는 TutorialsDojoScores사용자와 해당 점수를 추적하는 DynamoDB 테이블이 있습니다. 테이블의 각 항목은 FighterId파티션 키인 속성과 FightTitle정렬 키인 속성으로 식별됩니다. 개발자는 이름이 지정된 테이블의 키가 아닌 속성 DojoTopScores과 DojoDateTime속성에서 데이터를 검색해야 합니다.키가 아닌 속성에 대한 쿼리 속도를 높이려면 개발자가 테이블에 어떤 유형의 인덱스를 추가해야 합니까?,희소 인덱스,로컬 보조 인덱스,기본 인덱스,글로벌 보조 지수,,,0,,
udemy,DVA-02,230,"An application is hosted in an On-Demand Linux EC2 instance which uses an RDS database. There have been a lot of complaints that the application often crashes, but the support team can't pinpoint the problem using CloudWatch. To properly troubleshoot the issue, the team wants to monitor the memory and swap usage of the instance and the number of idle and running processes as well.Which of the following is the MOST suitable solution to use in this scenario?",D,D,Install the AWS X-Ray daemon on the EC2 instance.,Use AWS Cloud9 to consolidate all metrics in a single dashboard.,Use detailed monitoring in CloudWatch.,Install the Amazon CloudWatch Logs agent to the EC2 instance.,,,,애플리케이션은 RDS 데이터베이스를 사용하는 온디맨드 Linux EC2 인스턴스에서 호스팅됩니다. 애플리케이션이 자주 충돌한다는 불만이 많았지만 지원팀에서는 CloudWatch를 사용하여 문제를 정확히 찾아낼 수 없습니다. 문제를 적절하게 해결하기 위해 팀에서는 인스턴스의 메모리 및 스왑 사용량과 유휴 및 실행 중인 프로세스 수를 모니터링하려고 합니다.다음 중 이 시나리오에 사용하기에 가장 적합한 솔루션은 무엇입니까?,EC2 인스턴스에 AWS X-Ray 데몬을 설치합니다.,AWS Cloud9을 사용하여 모든 지표를 단일 대시보드에 통합하세요.,CloudWatch에서 세부 모니터링을 사용하세요.,Amazon CloudWatch Logs 에이전트를 EC2 인스턴스에 설치합니다.,,,0,,
udemy,DVA-02,231,"A developer has been instructed to configure Cross-Region Replication (CRR) to their S3 bucket as part of the company's disaster recovery plan. She is using the put-bucket-replication AWS CLI to enable CRR on the bucket but it fails whenever she attempts to issue the command. However, the same command works for the other S3 buckets. Which of the following options is the MOST likely reason for this issue?",C,C,Amazon S3 Object Lock is enabled in the bucket.,The bucket should be configured as a static web hosting first.,Versioning is not enabled in the bucket.,S3 Transfer Acceleration is not enabled in the bucket.,,,,개발자는 회사 재해 복구 계획의 일부로 S3 버킷에 CRR(교차 지역 복제)을 구성하라는 지시를 받았습니다. 그녀는 put-bucket-replicationAWS CLI를 사용하여 버킷에서 CRR을 활성화하지만 명령을 실행하려고 할 때마다 실패합니다. 그러나 다른 S3 버킷에도 동일한 명령이 작동합니다.다음 옵션 중 이 문제의 가장 큰 원인은 무엇입니까?,버킷에서 Amazon S3 객체 잠금이 활성화되었습니다.,먼저 버킷을 정적 웹 호스팅으로 구성해야 합니다.,버킷에서 버전 관리가 활성화되어 있지 않습니다.,버킷에서 S3 Transfer Acceleration이 활성화되어 있지 않습니다.,,,0,,
udemy,DVA-02,232,"A cryptocurrency exchange portal has a key management service hosted in their on-premises data center, which stores encryption keys and uses an RSA asymmetric encryption algorithm. The company has recently implemented a hybrid cloud architecture in AWS and you were assigned to migrate the exchange portal to their cloud infrastructure. For security compliance, the keys should be stored in dedicated, third-party validated hardware security modules under your exclusive control. Which of the following is the BEST solution that you should implement to meet the above requirement?",D,D,Use AWS KMS to store and manage the encryption keys.,Import the encryption keys from your on-premises key management service to AWS Secrets Manager as Customer Master Keys (CMKs).,Develop a custom key management service using the AWS Encryption SDK.,Import the encryption keys from your on-premises key management service to AWS CloudHSM.,,,,암호화폐 거래소 포털에는 암호화 키를 저장하고 RSA 비대칭 암호화 알고리즘을 사용하는 온프레미스 데이터 센터에 호스팅되는 키 관리 서비스가 있습니다. 회사는 최근 AWS에 하이브리드 클라우드 아키텍처를 구현했으며 귀하는 Exchange 포털을 클라우드 인프라로 마이그레이션하도록 배정되었습니다. 보안 규정 준수를 위해 키는 귀하가 독점적으로 관리하는 타사의 검증된 전용 하드웨어 보안 모듈에 저장되어야 합니다.다음 중 위의 요구 사항을 충족하기 위해 구현해야 하는 가장 좋은 솔루션은 무엇입니까?,AWS KMS를 사용하여 암호화 키를 저장하고 관리합니다.,온프레미스 키 관리 서비스의 암호화 키를 고객 마스터 키(CMK)로 AWS Secrets Manager로 가져옵니다.,AWS 암호화 SDK를 사용하여 사용자 지정 키 관리 서비스를 개발합니다.,온프레미스 키 관리 서비스에서 AWS CloudHSM으로 암호화 키를 가져옵니다.,,,0,,
udemy,DVA-02,233,"A developer is currently building a scalable microservices architecture where complex applications are decomposed into smaller, independent services. Docker will be used as its application container to provide an optimal way of running small, decoupled services. The developer should also have fine-grained control over the custom application architecture. Which of the following services is the MOST suitable one to use?",D,D,Elastic Beanstalk,AWS SAM,EC2,ECS,,,,개발자는 현재 복잡한 애플리케이션을 더 작고 독립적인 서비스로 분해하는 확장 가능한 마이크로서비스 아키텍처를 구축하고 있습니다. Docker는 소규모의 분리된 서비스를 실행하는 최적의 방법을 제공하기 위해 애플리케이션 컨테이너로 사용됩니다. 또한 개발자는 사용자 정의 애플리케이션 아키텍처를 세밀하게 제어할 수 있어야 합니다.다음 중 사용하기에 가장 적합한 서비스는 무엇입니까?,엘라스틱 콩나무,AWS SAM,EC2,ECS,,,0,,
udemy,DVA-02,234,A developer will be building a game data feed application which will continuously collect data about player-game interactions and feed the data into your gaming platform. The application uses the Kinesis Client Library to process the data stream from the Amazon Kinesis Data Streams and stores the data to Amazon DynamoDB. It is required that the system should have enough shards and EC2 instances in order to handle failover and adequately process the amount of data coming in and out of the stream. Which of the following ratio of the number of Kinesis shards to EC2 worker instances should the developer implement to achieve the above requirement in the most cost-effective and highly available way?,A,A,4 shards : 2 instances,6 shards : 1 instance,1 shard : 6 instances,4 shards : 8 instances,,,,개발자는 플레이어-게임 상호 작용에 대한 데이터를 지속적으로 수집하고 게임 플랫폼에 데이터를 공급하는 게임 데이터 피드 애플리케이션을 구축할 것입니다. 애플리케이션은 Kinesis Client Library를 사용하여 Amazon Kinesis Data Streams의 데이터 스트림을 처리하고 해당 데이터를 Amazon DynamoDB에 저장합니다. 장애 조치를 처리하고 스트림에 들어오고 나가는 데이터 양을 적절하게 처리하려면 시스템에 충분한 샤드와 EC2 인스턴스가 있어야 합니다.개발자가 가장 비용 효율적이고 가용성이 높은 방식으로 위의 요구 사항을 달성하기 위해 다음 중 EC2 작업자 인스턴스에 대한 Kinesis 샤드 수의 비율을 구현해야 합니까?,샤드 4개: 인스턴스 2개,샤드 6개: 인스턴스 1개,샤드 1개 : 인스턴스 6개,샤드 4개: 인스턴스 8개,,,0,,
udemy,DVA-02,235,"A company is using OpenAPI, which is also known as Swagger, for the API specifications of their REST web services that are hosted on their on-premises data center. They want to migrate their system to AWS using Lambda and API Gateway. In line with this, you are instructed to create a new API and populate it with the resources and methods from their Swagger definition. Which of the following is the EASIEST way to accomplish this task?",A,A,Import their Swagger or OpenAPI definitions to API Gateway using the AWS Console.,Use AWS SAM to migrate and deploy the company's web services to API Gateway.,Create models and templates for request and response mappings based on the company's API definitions.,Use CodeDeploy to migrate and deploy the company's web services to API Gateway.,,,,한 회사는 온프레미스 데이터 센터에서 호스팅되는 REST 웹 서비스의 API 사양에 대해 Swagger라고도 알려진 OpenAPI를 사용하고 있습니다. 그들은 Lambda와 API Gateway를 사용하여 시스템을 AWS로 마이그레이션하려고 합니다. 이에 따라 새 API를 생성하고 Swagger 정의의 리소스 및 메서드로 채우라는 지시를 받습니다.다음 중 이 작업을 수행하는 가장 쉬운 방법은 무엇입니까?,AWS 콘솔을 사용하여 Swagger 또는 OpenAPI 정의를 API 게이트웨이로 가져옵니다.,AWS SAM을 사용하여 회사의 웹 서비스를 API 게이트웨이로 마이그레이션하고 배포합니다.,회사의 API 정의를 기반으로 요청 및 응답 매핑을 위한 모델과 템플릿을 만듭니다.,CodeDeploy를 사용하여 회사의 웹 서비스를 API Gateway로 마이그레이션하고 배포합니다.,,,0,,
udemy,DVA-02,236,A developer is building an online game in AWS which will be using a NoSQL database with DynamoDB. Each player data has an average size of 3.5 KB and it is expected that the game will send 150 eventually consistent read requests per second. How may Read Capacity Units (RCU) should the developer provision to the table?,C,C,600,150,75,300,,,,개발자는 DynamoDB와 함께 NoSQL 데이터베이스를 사용할 온라인 게임을 AWS에서 구축하고 있습니다. 각 플레이어 데이터의 평균 크기는 3.5KB이며 게임은 초당 150개의 일관된 읽기 요청을 보낼 것으로 예상됩니다.개발자는 RCU(읽기 용량 단위)를 테이블에 어떻게 프로비저닝해야 합니까?,600,150,75,300,,,0,,
udemy,DVA-02,237,"A company has an AWS account with an ID of 061218980612 and has a centralized Java web application hosted in AWS Elastic Beanstalk that is used by different departments. The developer used the iam create-account-alias --account-alias finance-dept AWS CLI command to create a user-friendly identifier for the finance department.For faster troubleshooting, the application must also be configured to easily trace all its downstream requests, such as Apache HTTP requests, AWS SDK requests, and SQL queries made using a JDBC driver. The ability to send traces to multiple different tracing backends without having to re-instrument the application code is required as well.Which of the following options is the MOST suitable solution that the developer implements?",B,B,Use the https://finance-dept.aws.amazon.com/console sign-in page URL for the AWS account. Install and configure the AWS X-Ray auto-instrumentation Java agent to trace all the downstream API calls.,Use the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the AWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.,Use the https://finance-dept.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the downstream API calls.,Use the https://061218980612.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and configure the Amazon CloudWatch Evidently to trace all the downstream API calls.,,,,"회사에는 ID가 061218980612인 AWS 계정이 있고 여러 부서에서 사용되는 AWS Elastic Beanstalk에서 호스팅되는 중앙 집중식 Java 웹 애플리케이션이 있습니다. 개발자는 iam create-account-alias --account-alias finance-deptAWS CLI 명령을 사용하여 재무 부서에 대한 사용자 친화적인 식별자를 생성했습니다.더 빠른 문제 해결을 위해서는 Apache HTTP 요청, AWS SDK 요청, JDBC 드라이버를 사용하여 생성된 SQL 쿼리 등 모든 다운스트림 요청을 쉽게 추적하도록 애플리케이션을 구성해야 합니다. 애플리케이션 코드를 다시 계측할 필요 없이 여러 다른 추적 백엔드로 추적을 보내는 기능도 필요합니다.다음 옵션 중 개발자가 구현하는 가장 적합한 솔루션은 무엇입니까?",https://finance-dept.aws.amazon.com/consoleAWS 계정의 로그인 페이지 URL을 사용합니다 . 모든 다운스트림 API 호출을 추적하도록 AWS X-Ray 자동 계측 Java 에이전트를 설치하고 구성합니다.,https://finance-dept.signin.aws.amazon.com/consoleAWS 계정의 로그인 페이지 URL을 사용합니다 . OpenTelemetry용 AWS Distro Collector를 설치하고 모든 다운스트림 API 호출을 추적하도록 OpenTelemetry용 AWS Distro를 설정합니다.,https://finance-dept.aws.signin.amazon.com/consoleAWS 계정의 로그인 페이지 URL을 사용합니다 . 모든 다운스트림 API 호출을 추적하기 위해 적절한 소스 ID 접두사를 사용하여 Elastic Beanstalk에서 IAM Roles Anywhere 신뢰 모델을 설정하고 구성합니다.,https://061218980612.aws.signin.amazon.com/consoleAWS 계정의 로그인 페이지 URL을 사용합니다 . 모든 다운스트림 API 호출을 추적하도록 Amazon CloudWatch를 설정하고 구성합니다.,,,0,,
udemy,DVA-02,238,"A developer has enabled API Caching on his application endpoints in Amazon API Gateway. For testing purposes, he wants to fetch the latest data, and not the cache data, whenever he sends a GET request with a Cache-Control: max-age=0 header to a specific resource. Which of the following policies will allow him to invalidate the cache for requests?",D,D,"{  ""Version"": ""2012-10-17"",  ""Statement"": [    {      ""Effect"": ""Allow"",      ""Action"": [        ""execute-api:Invoke""      ],      ""Resource"": [""arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier""      ]   } ]}","{ ""Version"": ""2012-10-17"", ""Statement"": [  {   ""Effect"": ""Deny"",   ""Action"": [    ""execute-api:*""   ],   ""Resource"": [    ""arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier""   ]  } ]}","{ ""Version"": ""2012-10-17"", ""Statement"": [  {   ""Effect"": ""Deny"",   ""Action"": [    ""execute-api:InvalidateCache""   ],   ""Resource"": [    ""arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier""   ]  } ]}","{ ""Version"": ""2012-10-17"", ""Statement"": [  {   ""Effect"": ""Allow"",   ""Action"": [    ""execute-api:InvalidateCache""   ],   ""Resource"": [    ""arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier""   ]  } ]}",,,,개발자가 Amazon API Gateway의 애플리케이션 엔드포인트에서 API 캐싱을 활성화했습니다. Cache-Control: max-age=0테스트 목적으로 그는 특정 리소스에 헤더가 포함된 GET 요청을 보낼 때마다 캐시 데이터가 아닌 최신 데이터를 가져오고 싶어합니다 .다음 중 요청에 대한 캐시를 무효화할 수 있는 정책은 무엇입니까?,"{  ""버전"" : ""2012- 10-17"" ,  ""진술"" : [     {      ""효과"" : ""허용"" ,       ""액션"" : [         ""실행하다-API:호출""      ],      ""리소스"" : [ ""arn:aws:실행-API:지역:계정 ID:API-아이디/스테이지-이름/GET/자원e-경로 지정자""      ]   } ]}","{ ""버전"" : ""2012-10-17"" ,  ""진술"" : [   {   ""효과"" : ""거부"" ,    ""액션"" : [     ""execute-api:*""   ],   ""리소스"" : [     ""arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier""   ]  } ]}","{ ""버전"" : ""2012-10-17"" ,  ""진술"" : [   {   ""효과"" : ""거부"" ,    ""액션"" : [     ""execute-api:InvalidateCache""   ],   ""리소스"" : [     ""arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier""   ]  } ]}","{ ""버전"" : ""2012-10-17"" ,  ""진술"" : [   {   ""효과"" : ""허용"" ,    ""액션"" : [     ""execute-api:InvalidateCache""   ],   ""리소스"" : [     ""arn:aws:execute-api:region:account-id:api-id/stage-name/GET/resource-path-specifier""   ]  } ]}",,,0,,
udemy,DVA-02,239,"Your development team is heavily relying on AWS managed solutions like CodeCommit, CodeBuild, CodePipeline, and CodeDeploy. As your project grows, you would like to further automate your CI/CD process. The management also requested to have a project dashboard to centrally monitor application activity and manage day-to-day development tasks.Which of the following AWS services will help you achieve this?",C,C,Amazon CodeGuru,AWS Elastic Beanstalk,AWS CodeStar,AWS Fault Injection Simulator,,,,"개발 팀은 CodeCommit, CodeBuild, CodePipeline 및 CodeDeploy와 같은 AWS 관리형 솔루션에 크게 의존하고 있습니다. 프로젝트가 성장함에 따라 CI/CD 프로세스를 더욱 자동화하고 싶습니다. 또한 경영진은 애플리케이션 활동을 중앙에서 모니터링하고 일상적인 개발 작업을 관리할 수 있는 프로젝트 대시보드를 요청했습니다.다음 중 어떤 AWS 서비스가 이를 달성하는 데 도움이 됩니까?",Amazon CodeGuru,AWS 엘라스틱 빈스토크,AWS 코드스타,AWS 오류 주입 시뮬레이터,,,0,,
udemy,DVA-02,240,"A startup has an urgent requirement to deploy their new NodeJS application to AWS. You were assigned to perform the deployment to a service where you don't need to worry about the underlying infrastructure that runs the application. The service must also automatically handle provisioning, load balancing, scaling, and application health monitoring. Which service will you use to easily deploy and manage the application?",A,A,AWS Elastic Beanstalk,AWS CloudFormation,AWS CodeDeploy,AWS SAM,,,,"스타트업에는 새로운 NodeJS 애플리케이션을 AWS에 배포해야 하는 긴급한 요구 사항이 있습니다. 귀하는 애플리케이션을 실행하는 기본 인프라에 대해 걱정할 필요가 없는 서비스에 대한 배포를 수행하도록 지정되었습니다. 또한 서비스는 프로비저닝, 로드 밸런싱, 크기 조정 및 애플리케이션 상태 모니터링을 자동으로 처리해야 합니다.애플리케이션을 쉽게 배포하고 관리하기 위해 어떤 서비스를 사용하시겠습니까?",AWS 엘라스틱 빈스토크,AWS 클라우드포메이션,AWS 코드배포,AWS SAM,,,0,,
udemy,DVA-02,241,"A company is developing a distributed system which will use a Lambda function that will be invoked asynchronously. In the event of failure, the function must be retried twice before sending the unprocessed events to an Amazon SQS queue through the use of Dead Letter Queue (DLQ). Which of the following is the correct way to implement a DLQ in Lambda?",B,B,Specify the AWS Service Namespace of the SQS Queue in the Lambda function's DeadLetterConfig parameter.,Specify the Amazon Resource Name of the SQS Queue in the Lambda function's DeadLetterConfig parameter.,Specify the Amazon Resource Name of the SQS Queue in the Transform section of the AWS SAM template that you'll use for deploying the function.,Specify the AWS Service Namespace of the SQS Queue in the AWS::Lambda::Function resource of the CloudFormation template that you'll use for deploying the function.,,,,한 회사는 비동기식으로 호출되는 Lambda 함수를 사용하는 분산 시스템을 개발하고 있습니다. 오류가 발생한 경우 DLQ(배달 못한 편지 대기열)를 사용하여 처리되지 않은 이벤트를 Amazon SQS 대기열로 보내기 전에 함수를 두 번 재시도해야 합니다.다음 중 Lambda에서 DLQ를 구현하는 올바른 방법은 무엇입니까?,Lambda 함수의 매개변수에 SQS 대기열의 AWS 서비스 네임스페이스를 지정합니다 DeadLetterConfig.,Lambda 함수의 매개변수에 SQS 대기열의 Amazon 리소스 이름을 지정합니다 DeadLetterConfig.,Transform함수 배포에 사용할 AWS SAM 템플릿 섹션 에서 SQS 대기열의 Amazon 리소스 이름을 지정합니다 .,AWS::Lambda::Function함수 배포에 사용할 CloudFormation 템플릿의 리소스 에 SQS 대기열의 AWS 서비스 네임스페이스를 지정합니다 .,,,0,,
udemy,DVA-02,242,"Both the read and write operations to your DynamoDB table are throttled, which are causing errors in your application. You checked the CloudWatch metrics but they indicate that the consumed capacity units haven't exceeded the provisioned capacity units. Upon further investigation, you found that the issue is caused by a ""hot partition"" in your table in which a certain partition is accessed by your downstream applications much more frequently than other partitions. What should you do to resolve this issue in your application with MINIMAL cost? (Select TWO.)",CD,CD,Increase the amount of read or write capacity for your table.,Implement read sharding to distribute workloads evenly.,Refactor your application to distribute your read and write operations as evenly as possible across your table.,Implement error retries and exponential backoff.,Use DynamoDB Accelerator (DAX).,,,"DynamoDB 테이블에 대한 읽기 및 쓰기 작업이 모두 제한되어 애플리케이션에 오류가 발생합니다. CloudWatch 지표를 확인했지만 소비된 용량 단위가 프로비저닝된 용량 단위를 초과하지 않았음을 나타냅니다. 추가 조사 결과, 다운스트림 애플리케이션이 다른 파티션보다 훨씬 더 자주 특정 파티션에 액세스하는 테이블의 ""핫 파티션""으로 인해 문제가 발생하는 것으로 나타났습니다.최소한의 비용으로 애플리케이션에서 이 문제를 해결하려면 어떻게 해야 합니까? (2개를 선택하세요.)",테이블의 읽기 또는 쓰기 용량을 늘리십시오.,워크로드를 균등하게 분산하기 위해 읽기 샤딩을 구현합니다.,읽기 및 쓰기 작업을 테이블 전체에 최대한 균등하게 분산하도록 애플리케이션을 리팩터링하세요.,오류 재시도 및 지수 백오프를 구현합니다.,DynamoDB Accelerator(DAX)를 사용합니다.,,0,,
udemy,DVA-02,243,There is a requirement to improve the performance of your serverless application in AWS by increasing the allocated CPU available for your Lambda functions. Which of the following is the MOST appropriate solution that you should implement to meet this requirement?,B,B,Manually configure the CPU settings of the Lambda function to the maximum value.,Increase the memory configuration of your function.,Configure the concurrency limit of your function to be the same as the account level concurrency limit.,Use Lambda layers to optimize the performance of the Lambda function.,,,,Lambda 함수에 사용할 수 있는 할당된 CPU를 늘려 AWS에서 서버리스 애플리케이션의 성능을 향상시켜야 한다는 요구 사항이 있습니다. 다음 중 이 요구 사항을 충족하기 위해 구현해야 하는 가장 적절한 솔루션은 무엇입니까?,Lambda 함수의 CPU 설정을 최대값으로 수동으로 구성합니다.,함수의 메모리 구성을 늘리십시오.,함수의 동시성 제한을 계정 수준 동시성 제한과 동일하게 구성하십시오.,Lambda 계층을 사용하여 Lambda 함수의 성능을 최적화합니다.,,,0,,
udemy,DVA-02,244,"You are managing an application which is composed of an SQS queue and an Auto Scaling group of EC2 instances. Recently, your customers are complaining that there are a lot of incidents where their orders are being erroneously sent twice. What should you do to rectify this problem?",A,A,Use a FIFO (First-In-First-Out) Queue and provide the Message Deduplication ID for each message.,Use a FIFO (First-In-First-Out) Queue by disabling the content-based deduplication.,Use a Standard Queue and provide the Message Group ID for each message.,Use a Standard Queue and provide the Message Deduplication ID for each message.,,,,SQS 대기열과 EC2 인스턴스의 Auto Scaling 그룹으로 구성된 애플리케이션을 관리하고 있습니다. 최근 귀하의 고객이 주문이 두 번 잘못 전송되는 사건이 많다고 불평하고 있습니다.이 문제를 해결하려면 어떻게 해야 합니까?,FIFO(선입선출) 대기열을 사용하고 각 메시지에 대해 메시지 중복 제거 ID를 제공합니다.,콘텐츠 기반 중복 제거를 비활성화하여 FIFO(선입선출) 대기열을 사용합니다.,표준 대기열을 사용하고 각 메시지에 대한 메시지 그룹 ID를 제공합니다.,표준 대기열을 사용하고 각 메시지에 대해 메시지 중복 제거 ID를 제공합니다.,,,0,,
udemy,DVA-02,245,"You were recently hired as a developer for a leading insurance firm in Asia which has a hybrid cloud architecture with AWS. The project that was assigned to you involves setting up a static website using Amazon S3 with a CORS configuration as shown below:<?xml version=""1.0"" encoding=""UTF-8""?><CORSConfiguration xmlns=""http://s3.amazonaws.com/doc/2006-03-01/""> <CORSRule>  <AllowedOrigin>https://tutorialsdojo.com</AllowedOrigin>  <AllowedMethod>GET</AllowedMethod>  <AllowedMethod>PUT</AllowedMethod>  <AllowedMethod>POST</AllowedMethod>  <AllowedMethod>DELETE</AllowedMethod>  <AllowedHeader>*</AllowedHeader>  <ExposeHeader>ETag</ExposeHeader>  <ExposeHeader>x-amz-meta-custom-header</ExposeHeader>  <MaxAgeSeconds>3600</MaxAgeSeconds> </CORSRule></CORSConfiguration>Which of the following statements are TRUE with regards to this S3 configuration? (Select TWO.)",CD,CD,The request will fail if the x-amz-meta-custom-header header is not included.,All HTTP Methods are allowed.,"It allows a user to view, add, remove or update objects inside the S3 bucket from the domain tutorialsdojo.com.",This will cause the browser to cache the response of the preflight OPTIONS request for 1 hour.,This configuration authorizes the user to perform actions on the S3 bucket.,,,"귀하는 최근 AWS와 하이브리드 클라우드 아키텍처를 갖춘 아시아 최고의 보험 회사의 개발자로 채용되었습니다. 귀하에게 할당된 프로젝트에는 아래와 같이 CORS 구성으로 Amazon S3를 사용하여 정적 웹 사이트를 설정하는 작업이 포함됩니다.<? xml 버전 = ""1.0"" 인코딩 = ""UTF-8"" ?><CORSConfiguration xmlns = ""http://s3.amazonaws.com/doc/2006-03-01/"" >  <CORS규칙>  <AllowedOrigin> https://tutorialsdojo.com </AllowedOrigin>  <허용메소드> GET </허용메소드>  <허용메소드> PUT </허용메소드>  <AllowedMethod> POST </AllowedMethod>  <허용메소드> 삭제 </허용메소드>  <허용헤더> * </허용헤더>  <ExposeHeader> ETag </ExposeHeader>  <ExposeHeader> x-amz-meta-custom-header </ExposeHeader>  <MaxAgeSeconds> 3600 </MaxAgeSeconds> </CORS규칙></CORS구성>이 S3 구성과 관련하여 다음 설명 중 참인 것은 무엇입니까? (2개를 선택하세요.)",헤더가 포함되지 않으면 요청이 실패합니다 x-amz-meta-custom-header.,모든 HTTP 메소드가 허용됩니다.,"이를 통해 사용자는 tutorialsdojo.com 도메인에서 S3 버킷 내부의 객체를 보고, 추가하고, 제거하거나 업데이트할 수 있습니다.",이로 인해 브라우저는 실행 전 OPTIONS 요청의 응답을 1시간 동안 캐시합니다.,이 구성은 사용자에게 S3 버킷에 대한 작업을 수행할 수 있는 권한을 부여합니다.,,0,,
udemy,DVA-02,246,A developer is working on an application that will process files encrypted with a data key generated from a KMS key. The application needs to decrypt the files locally before it can proceed with the processing of the files.Which of the following are valid and secure steps in decrypting data? (Select TWO.),AC,AC,"Use the plaintext data key to decrypt data locally, then erase the plaintext data key from memory.","Use the encrypted data key to decrypt data locally, then erase the encrypted data key from memory.",Use the Decrypt operation to decrypt the encrypted data key.,Use the Decrypt operation to decrypt the plaintext data key.,"Use the plaintext data key to decrypt data locally, then erase the encrypted data key from memory.",,,개발자는 KMS 키에서 생성된 데이터 키로 암호화된 파일을 처리하는 애플리케이션을 개발 중입니다. 애플리케이션은 파일 처리를 계속하기 전에 로컬에서 파일의 암호를 해독해야 합니다.다음 중 데이터를 해독하는 유효하고 안전한 단계는 무엇입니까? (2개를 선택하세요.),일반 텍스트 데이터 키를 사용하여 로컬로 데이터를 해독한 다음 메모리에서 일반 텍스트 데이터 키를 삭제합니다.,암호화된 데이터 키를 사용하여 로컬에서 데이터를 해독한 다음 암호화된 데이터 키를 메모리에서 삭제합니다.,Decrypt 작업을 사용하여 암호화된 데이터 키를 해독합니다.,Decrypt 작업을 사용하여 일반 텍스트 데이터 키를 해독합니다.,일반 텍스트 데이터 키를 사용하여 로컬로 데이터를 해독한 다음 암호화된 데이터 키를 메모리에서 삭제합니다.,,0,,
udemy,DVA-02,247,"An application experiences a sluggish response whenever there is a surge in requests involving read queries. The developer has already attempted to improve performance by optimizing the queries. However, the problem still persists even after applying the change. The application is hosted in an Amazon ECS Cluster and uses a MySQL database backed by Amazon RDS.Which of the following could the developer do to resolve the performance issue? (Select TWO.)",DE,DE,Cache the database response using Amazon CloudFront.,Replace the database with Amazon MemoryDB for Redis,Implement a Multi-AZ deployment configuration for the RDS DB instance.,Implement database caching using Amazon ElastiCache.,Set up read replicas for the RDS database instance and route read queries to these replicas.,,,읽기 쿼리와 관련된 요청이 급증할 때마다 애플리케이션의 응답이 느려집니다. 개발자는 이미 쿼리를 최적화하여 성능을 향상시키려고 시도했습니다. 그러나 변경 사항을 적용한 후에도 문제가 계속 발생합니다. 애플리케이션은 Amazon ECS 클러스터에서 호스팅되며 Amazon RDS가 지원하는 MySQL 데이터베이스를 사용합니다.다음 중 개발자가 성능 문제를 해결하기 위해 수행할 수 있는 작업은 무엇입니까? (2개를 선택하세요.),Amazon CloudFront를 사용하여 데이터베이스 응답을 캐시합니다.,데이터베이스를 Redis용 Amazon MemoryDB로 교체,RDS DB 인스턴스에 대한 다중 AZ 배포 구성을 구현합니다.,Amazon ElastiCache를 사용하여 데이터베이스 캐싱을 구현합니다.,RDS 데이터베이스 인스턴스에 대한 읽기 복제본을 설정하고 읽기 쿼리를 이러한 복제본으로 라우팅합니다.,,0,,
udemy,DVA-02,248,"A Lambda function is over 80 MB in size, which exceeds the deployment package size limit for direct uploads. You want to refactor the function to pull in additional code and other dependencies from another source, which will reduce the size of the deployment. Which feature of Lambda should you use in order to implement the above task?",D,D,Execution Context,Environment Variable,Alias,Layers,,,,Lambda 함수의 크기는 80MB를 초과하며 이는 직접 업로드에 대한 배포 패키지 크기 제한을 초과합니다. 다른 소스에서 추가 코드 및 기타 종속성을 가져오도록 함수를 리팩터링하려고 하면 배포 크기가 줄어듭니다.위 작업을 구현하려면 Lambda의 어떤 기능을 사용해야 합니까?,실행 컨텍스트,환경 변수,별명,레이어,,,0,,
udemy,DVA-02,249,"A leading technology company is building a serverless application in AWS using the C++ programming language. The application will use DynamoDB as its data store, Lambda as its compute service, and API Gateway as its API Proxy. You are tasked to handle the deployment of the compute resources to AWS. Which of the following steps should you implement to properly deploy the serverless application?",C,C,Create a Lambda function with the C++ code and directly upload it to AWS.,Use AWS Serverless Application Model (AWS SAM) to deploy the Lambda function.,Create a new layer which contains the Custom Runtime for C++ and then launch a Lambda function which uses that runtime.,Upload the deployment package to S3 and then use CloudFormation to deploy Lambda function with a reference to the S3 URL of the package.,,,,"한 선도적인 기술 회사는 C++ 프로그래밍 언어를 사용하여 AWS에서 서버리스 애플리케이션을 구축하고 있습니다. 애플리케이션은 DynamoDB를 데이터 저장소로, Lambda를 컴퓨팅 서비스로, API 게이트웨이를 API 프록시로 사용합니다. 귀하는 컴퓨팅 리소스를 AWS에 배포하는 작업을 처리해야 합니다.서버리스 애플리케이션을 올바르게 배포하려면 다음 중 어떤 단계를 구현해야 합니까?",C++ 코드로 Lambda 함수를 생성하고 AWS에 직접 업로드합니다.,AWS Serverless Application Model(AWS SAM)을 사용하여 Lambda 함수를 배포합니다.,C++용 사용자 지정 런타임이 포함된 새 계층을 생성한 다음 해당 런타임을 사용하는 Lambda 함수를 시작합니다.,배포 패키지를 S3에 업로드한 다음 CloudFormation을 사용하여 패키지의 S3 URL을 참조하여 Lambda 함수를 배포합니다.,,,0,,
udemy,DVA-02,250,A commercial bank is developing an online auction application with a DynamoDB database that will allow customers to bid for real estate properties from the comforts of their homes. The application should allow the minimum acceptable price established by the bank prior to the auction. The opening bid entered by the staff must be at least the minimum bid and the new bids submitted by the customers should be greater than the current bid. The application logic has already been implemented but the DynamoDB database calls should also be tailored to meet the requirements. Which of the following is the MOST effective solution that will satisfy the requirement in this scenario?,C,C,Enable DynamoDB Transactions to automatically check the minimum acceptable price as well as the current and new bid price.,Use an optimistic locking strategy in your database calls to ensure that the new bid submitted by the customer is greater than the current bid.,Configure the database calls of the application to use conditional updates and conditional writes with a condition expression that will check if the new bid submitted by the customer is greater than the current bid.,Use DynamoDB Streams and a Lambda function to track the current bid price and compare against all of the new bids submitted by the customers.,,,,"한 상업 은행은 고객이 집에서 편안하게 부동산에 입찰할 수 있도록 하는 DynamoDB 데이터베이스를 사용하여 온라인 경매 애플리케이션을 개발하고 있습니다. 신청서는 경매 이전에 은행이 설정한 최소 허용 가격을 허용해야 합니다. 직원이 입력한 최초 입찰가는 최소한 최소 입찰가 이상이어야 하며, 고객이 제출한 새 입찰가는 현재 입찰가보다 높아야 합니다. 애플리케이션 로직은 이미 구현되었지만 DynamoDB 데이터베이스 호출도 요구 사항에 맞게 조정되어야 합니다.다음 중 이 시나리오의 요구 사항을 충족하는 가장 효과적인 솔루션은 무엇입니까?",DynamoDB 트랜잭션을 활성화하면 최소 허용 가격은 물론 현재 및 신규 입찰 가격도 자동으로 확인할 수 있습니다.,고객이 제출한 새 입찰이 현재 입찰보다 큰지 확인하려면 데이터베이스 호출에 낙관적 잠금 전략을 사용하십시오.,고객이 제출한 새 입찰이 현재 입찰보다 큰지 확인하는 조건 표현식과 함께 조건부 업데이트 및 조건부 쓰기를 사용하도록 애플리케이션의 데이터베이스 호출을 구성합니다.,DynamoDB 스트림과 Lambda 함수를 사용하여 현재 입찰 가격을 추적하고 고객이 제출한 모든 새 입찰과 비교합니다.,,,0,,
udemy,DVA-02,251,A web application running in Amazon Elastic Beanstalk reads and writes a large number of related items in DynamoDB and processes each item one at a time. The network overhead of these transactions causes degradation in the application's performance. You were instructed by your manager to quickly refactor the application but without introducing major code changes such as implementing concurrency management or multithreading. Which of the following solutions is the EASIEST method to implement that will improve the application performance in a cost-effective manner?,B,B,Upgrade the EC2 instances to a higher instance type.,"Use DynamoDB Batch Operations API for GET, PUT, and DELETE operations.",Enable DynamoDB Streams.,Refactor the application to use DynamoDB transactional read and write APIs .,,,,Amazon Elastic Beanstalk에서 실행되는 웹 애플리케이션은 DynamoDB에서 많은 관련 항목을 읽고 쓰고 각 항목을 한 번에 하나씩 처리합니다. 이러한 트랜잭션의 네트워크 오버헤드로 인해 애플리케이션 성능이 저하됩니다. 동시성 관리 또는 멀티스레딩 구현과 같은 주요 코드 변경을 도입하지 않고 애플리케이션을 신속하게 리팩터링하라는 관리자의 지시를 받았습니다.다음 중 비용 효율적인 방식으로 애플리케이션 성능을 향상시키기 위해 구현하기 가장 쉬운 솔루션은 무엇입니까?,EC2 인스턴스를 더 높은 인스턴스 유형으로 업그레이드합니다.,"GET, PUT 및 DELETE 작업에는 DynamoDB 배치 작업 API를 사용합니다.",DynamoDB 스트림을 활성화합니다.,DynamoDB 트랜잭션 읽기 및 쓰기 API를 사용하도록 애플리케이션을 리팩터링합니다.,,,0,,
udemy,DVA-02,252,"A financial company has a cryptocurrency application that has been hosted in Elastic Beanstalk for a couple of months. Recently, the application's performance has been degrading, so you decided to check the CPU and memory utilization of the underlying EC2 instances in CloudWatch. You can see the CPU utilization of the instances but not the memory utilization.Which of the following is the MOST likely cause of this issue?",B,B,The .ebextensions/xray-daemon.config file in Elastic Beanstalk is missing.,CloudWatch does not track memory utilization by default.,X-Ray Daemon is not installed on the EC2 instances.,The detailed monitoring is not enabled in CloudWatch.,,,,금융 회사에는 Elastic Beanstalk에서 몇 달 동안 호스팅된 암호화폐 애플리케이션이 있습니다. 최근 애플리케이션 성능이 저하되어 CloudWatch에서 기본 EC2 인스턴스의 CPU 및 메모리 사용률을 확인하기로 결정했습니다. 인스턴스의 CPU 사용률은 볼 수 있지만 메모리 사용률은 볼 수 없습니다.다음 중 이 문제의 가장 큰 원인은 무엇입니까?,Elastic Beanstalk의 파일 .ebextensions/xray-daemon.config이 누락되었습니다.,CloudWatch는 기본적으로 메모리 사용률을 추적하지 않습니다.,X-Ray 데몬은 EC2 인스턴스에 설치되지 않습니다.,CloudWatch에서는 세부 모니터링이 활성화되지 않습니다.,,,0,,
udemy,DVA-02,253,You are using AWS Serverless Application Model (AWS SAM) to build and deploy applications in your serverless infrastructure. Your manager instructed you to create a CloudFormation template that includes your SAM script and other service configurations. This template will be used to launch a similar infrastructure in another region. What should you do in order to accomplish this task?,A,A,Add a Transform section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.,Add a Parameters section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.,Add a Resources section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.,Add a Mappings section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.,,,,AWS Serverless Application Model(AWS SAM)을 사용하여 서버리스 인프라에 애플리케이션을 구축하고 배포하고 있습니다. 관리자는 SAM 스크립트와 기타 서비스 구성이 포함된 CloudFormation 템플릿을 생성하라고 지시했습니다. 이 템플릿은 다른 지역에서 유사한 인프라를 시작하는 데 사용됩니다.이 임무를 완수하려면 무엇을 해야 합니까?,템플릿에 섹션을 추가하여 Transform사용할 AWS Serverless Application Model(AWS SAM) 버전을 지정합니다.,템플릿에 섹션을 추가하여 Parameters사용할 AWS Serverless Application Model(AWS SAM) 버전을 지정합니다.,템플릿에 섹션을 추가하여 Resources사용할 AWS Serverless Application Model(AWS SAM) 버전을 지정합니다.,템플릿에 섹션을 추가하여 Mappings사용할 AWS Serverless Application Model(AWS SAM) 버전을 지정합니다.,,,0,,
udemy,DVA-02,254,A development team has several developers where each of them has a corresponding IAM user. It is your primary responsibility to grant the developers access to CodeCommit to enable them to fully utilize the code repositories on their local computers. Which of the following should you implement to grant access to your developers? (Select TWO.),BD,BD,Generate Git credentials in Github.,Generate new SSH keys and associate the public SSH key to each of your developer's IAM user.,Generate new SSH keys and associate the private SSH key to each of your developer's IAM user.,Generate HTTPS Git credentials.,Enable Multi-Factor Authentication (MFA) for each IAM User.,,,개발 팀에는 여러 명의 개발자가 있으며 각 개발자에게는 해당 IAM 사용자가 있습니다. 개발자에게 CodeCommit에 대한 액세스 권한을 부여하여 로컬 컴퓨터의 코드 리포지토리를 완전히 활용할 수 있도록 하는 것은 귀하의 주요 책임입니다.다음 중 개발자에게 액세스 권한을 부여하려면 무엇을 구현해야 합니까? (2개를 선택하세요.),Github에서 Git 자격 증명을 생성합니다.,새 SSH 키를 생성하고 공개 SSH 키를 각 개발자의 IAM 사용자에 연결합니다.,새 SSH 키를 생성하고 개인 SSH 키를 각 개발자의 IAM 사용자에 연결합니다.,HTTPS Git 자격 증명을 생성합니다.,각 IAM 사용자에 대해 Multi-Factor Authentication(MFA)을 활성화합니다.,,0,,
udemy,DVA-02,255,"A software engineer is developing a serverless application which will use a DynamoDB database. One of the requirements is that each write request should return the total number of write capacity units consumed, with subtotals for the table and any secondary indexes that were affected by the operation. What should be done to accomplish this feature?",C,C,Add the ReturnConsumedCapacity parameter with a value of TOTAL in every write request.,Add the ReturnValues parameter with a value of INDEXES in every write request.,Add the ReturnConsumedCapacity parameter with a value of INDEXES in every write request.,Add the ReturnValues parameter with a value of TOTAL in every write request.,,,,소프트웨어 엔지니어가 DynamoDB 데이터베이스를 사용할 서버리스 애플리케이션을 개발하고 있습니다. 요구 사항 중 하나는 각 쓰기 요청이 소비된 총 쓰기 용량 단위 수와 작업의 영향을 받은 테이블 및 보조 인덱스의 소계를 반환해야 한다는 것입니다.이 기능을 수행하려면 어떻게 해야 합니까?,모든 쓰기 요청에 ReturnConsumedCapacity값이 있는 매개변수를 추가합니다 .TOTAL,모든 쓰기 요청에 ReturnValues값이 있는 매개변수를 추가합니다 .INDEXES,모든 쓰기 요청에 ReturnConsumedCapacity값이 있는 매개변수를 추가합니다 .INDEXES,모든 쓰기 요청에 ReturnValues값이 있는 매개변수를 추가합니다 .TOTAL,,,0,,
udemy,DVA-02,256,"The developer has built a real-time IoT device monitoring application that leverages Amazon Kinesis Data Stream to ingest data. The application uses several EC2 instances for processing. Recently, the developer has observed a steady increase in the rate of data flowing into the stream, indicating that the stream's capacity must be scaled up to sustain optimal performance.What should the developer do to increase the capacity of the stream?",C,C,Integrate Amazon Kinesis Data Firehose with the Amazon Kinesis Data Stream to increase the capacity of the stream.,Merge every shard in the stream.,Split every shard in the stream.,Upgrade the instance type of the EC2 instances.,,,,개발자는 Amazon Kinesis Data Stream을 활용하여 데이터를 수집하는 실시간 IoT 장치 모니터링 애플리케이션을 구축했습니다. 애플리케이션은 처리를 위해 여러 EC2 인스턴스를 사용합니다. 최근 개발자는 스트림으로 흐르는 데이터 속도가 꾸준히 증가하는 것을 관찰했습니다. 이는 최적의 성능을 유지하려면 스트림 용량을 확장해야 함을 나타냅니다.스트림 용량을 늘리려면 개발자는 무엇을 해야 합니까?,Amazon Kinesis Data Firehose를 Amazon Kinesis Data Stream과 통합하여 스트림 용량을 늘립니다.,스트림의 모든 샤드를 병합합니다.,스트림의 모든 샤드를 분할합니다.,EC2 인스턴스의 인스턴스 유형을 업그레이드합니다.,,,0,,
udemy,DVA-02,257,"A company has an application that is using CloudFront to serve their static contents to their users around the globe. They are receiving a number of bad reviews from their customers lately because it takes a lot of time to log into their website. Sometimes, their users are also getting HTTP 504 errors which is why the developer was instructed to fix this problem immediately. Which of the following combination of options should the developer use together to set up a cost-effective solution for this scenario? (Select TWO.)",AC,AC,Configure an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses.,Launch your application to multiple AWS regions to serve your global users. Use a Route 53 record with latency routing policy to route incoming traffic to the region with the best latency to the user.,"Customize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your Lambda functions to execute the authentication process in AWS locations closer to the users.",Launch your application to multiple and geographically disperse VPCs on various AWS regions then create a transit VPC to easily connect all your resources. Use several Lambda functions in each region using the AWS Serverless Application Model (SAM) service to improve the overall application performance.,Add a Cache-Control max-age directive to your objects in CloudFront and specify the longest practical value for max-age to increase the cache hit ratio of your CloudFront distribution.,,,"회사에는 CloudFront를 사용하여 전 세계 사용자에게 정적 콘텐츠를 제공하는 애플리케이션이 있습니다. 최근에는 웹사이트에 로그인하는 데 많은 시간이 걸리기 때문에 고객으로부터 나쁜 평가를 많이 받고 있습니다. 때때로 사용자에게 HTTP 504 오류가 발생하는 경우가 있는데, 이것이 바로 개발자가 이 문제를 즉시 해결하라는 지시를 받은 이유입니다.이 시나리오에 대한 비용 효율적인 솔루션을 설정하기 위해 개발자는 다음 중 어떤 옵션 조합을 함께 사용해야 합니까? (2개를 선택하세요.)",두 개의 오리진이 있는 오리진 그룹을 생성하여 오리진 장애 조치를 구성합니다. 하나는 기본 오리진으로 지정하고 다른 하나는 기본 오리진이 특정 HTTP 상태 코드 실패 응답을 반환할 때 CloudFront가 자동으로 전환하는 두 번째 오리진으로 지정합니다.,전 세계 사용자에게 서비스를 제공하기 위해 여러 AWS 리전에서 애플리케이션을 시작하세요. 지연 시간 라우팅 정책과 함께 Route 53 레코드를 사용하여 사용자에게 지연 시간이 가장 긴 지역으로 수신 트래픽을 라우팅합니다.,CloudFront 웹 배포가 Lambda@Edge를 사용하여 사용자에게 제공하는 콘텐츠를 사용자 지정합니다. 그러면 Lambda 함수가 사용자에게 더 가까운 AWS 위치에서 인증 프로세스를 실행할 수 있습니다.,다양한 AWS 리전에 지리적으로 분산된 여러 VPC에 애플리케이션을 시작한 다음 전송 VPC를 생성하여 모든 리소스를 쉽게 연결할 수 있습니다. AWS Serverless Application Model(SAM) 서비스를 사용하여 각 지역에서 여러 Lambda 함수를 사용하여 전반적인 애플리케이션 성능을 향상시킵니다.,Cache-Control max-ageCloudFront의 객체에 지시어를 추가 하고 max-ageCloudFront 배포의 캐시 적중률을 높이려면 가장 긴 실제 값을 지정합니다.,,0,,
udemy,DVA-02,258,"A company has recently adopted a hybrid cloud architecture to augment their on-premises data center with virtual private clouds (VPCs) in AWS. You were assigned to manage all of the company's cloud infrastructure including the security of their resources using IAM. In this scenario, which of the following are best practices in managing security in AWS? (Select TWO.)",AC,AC,Grant only the permissions required by the resource to perform a task.,Always keep your AWS account root user access key.,Delete root user access keys.,Grant all the permissions to the resource in order to perform the task without any issues.,Use IAM inline policies to delegate permissions.,,,한 회사는 최근 AWS의 가상 프라이빗 클라우드(VPC)로 온프레미스 데이터 센터를 강화하기 위해 하이브리드 클라우드 아키텍처를 채택했습니다. 귀하는 IAM을 사용하여 리소스 보안을 포함하여 회사의 모든 클라우드 인프라를 관리하도록 배정되었습니다.이 시나리오에서 다음 중 AWS의 보안 관리 모범 사례는 무엇입니까? (2개를 선택하세요.),작업을 수행하기 위해 리소스에 필요한 권한만 부여합니다.,항상 AWS 계정 루트 사용자 액세스 키를 보관하십시오.,루트 사용자 액세스 키를 삭제합니다.,문제 없이 작업을 수행하려면 리소스에 모든 권한을 부여하세요.,IAM 인라인 정책을 사용하여 권한을 위임합니다.,,0,,
udemy,DVA-02,259,"A serverless application, which uses a Lambda function integrated with API Gateway, provides data to a front-end application written in ReactJS. The users are complaining that they are getting HTTP 504 errors intermittently when they are using the application in peak times. The developer found no errors in the CloudWatch logs of the Lambda function.Which of the following is the MOST likely cause of this issue?",C,C,There is an authorization failure occurring between API Gateway and the Lambda function.,The API Gateway automatically enabled throttling in peak times which caused the HTTP 504 errors.,The underlying Lambda function has been running for more than 29 seconds causing the API Gateway request to time out.,The memory allocated for the Lambda function is insufficient,,,,API 게이트웨이와 통합된 Lambda 함수를 사용하는 서버리스 애플리케이션은 ReactJS로 작성된 프런트엔드 애플리케이션에 데이터를 제공합니다. 사용자들은 피크 타임에 애플리케이션을 사용할 때 간헐적으로 HTTP 504 오류가 발생한다고 불평하고 있습니다. 개발자는 Lambda 함수의 CloudWatch 로그에서 오류를 발견하지 못했습니다.다음 중 이 문제의 가장 큰 원인은 무엇입니까?,API Gateway와 Lambda 함수 사이에 인증 오류가 발생합니다.,API 게이트웨이는 피크 시간대에 자동으로 조절을 활성화하여 HTTP 504 오류를 발생시켰습니다.,기본 Lambda 함수가 29초 이상 실행되어 API Gateway 요청 시간이 초과되었습니다.,Lambda 함수에 할당된 메모리가 부족합니다.,,,0,,
udemy,DVA-02,260,"A company has a latency-sensitive service running on AWS Fargate, which is fronted by an Application Load Balancer (ALB). A CloudFront distribution uses the ALB as its origin and presents a custom domain for clients to access the service. The service authenticates requests by validating the JSON Web Token (JWT) obtained from the Authorization header sent by clients. Lately, there has been a significant influx of login attempts from unauthenticated users, which increases the CPU utilization of the Fargate tasks.Which solution would reduce the load on the Fargate tasks in the most operationally efficient manner?",B,B,Create a Lambda@Edge function for JWT validation. Attach it to the Origin Response event of the CloudFront distribution.,Create a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.,Create a Lambda function that performs JWT validation. Configure the ALB to route login requests to the Lambda function.,Enable auto-scaling on the Fargate tasks.,,,,회사에는 ALB(Application Load Balancer)가 앞에 있는 AWS Fargate에서 실행되는 지연 시간에 민감한 서비스가 있습니다. CloudFront 배포판은 ALB를 오리진으로 사용하고 클라이언트가 서비스에 액세스할 수 있도록 사용자 지정 도메인을 제공합니다. 서비스는 클라이언트가 보낸 Authorization 헤더에서 얻은 JWT(JSON 웹 토큰)의 유효성을 검사하여 요청을 인증합니다. 최근에는 인증되지 않은 사용자의 로그인 시도가 크게 증가하여 Fargate 작업의 CPU 사용률이 증가했습니다.운영상 가장 효율적인 방식으로 Fargate 작업의 부하를 줄이는 솔루션은 무엇입니까?,JWT 검증을 위한 Lambda@Edge 함수를 생성합니다. CloudFront 배포의 Origin Response 이벤트에 연결합니다.,JWT 검증을 위한 CloudFront 함수를 생성합니다. CloudFront 배포의 뷰어 요청 이벤트에 연결합니다.,JWT 검증을 수행하는 Lambda 함수를 생성합니다. 로그인 요청을 Lambda 함수로 라우팅하도록 ALB를 구성합니다.,Fargate 작업에서 자동 크기 조정을 활성화합니다.,,,0,,
udemy,DVA-02,261,A developer is writing a CloudFormation template which will be used to deploy a simple Lambda function to AWS. The function to be deployed is made in Python with just 3 lines of codes which can be written inline in the template. Which parameter of the AWS::Lambda::Function resource should the developer use to place the Python code in the template?,C,C,Code,Handler,ZipFile,CodeUri,,,,개발자는 간단한 Lambda 함수를 AWS에 배포하는 데 사용할 CloudFormation 템플릿을 작성하고 있습니다. 배포할 함수는 템플릿에 인라인으로 작성할 수 있는 단 3줄의 코드로 Python으로 작성됩니다.AWS::Lambda::Function개발자가 템플릿에 Python 코드를 배치하려면 리소스 의 어떤 매개변수를 사용해야 합니까?,Code,Handler,ZipFile,CodeUri,,,0,,
udemy,DVA-02,262,"A developer is creating a new global secondary index on a provisioned mode DynamoDB table. Since the application will store large quantities of data, the write capacity units must be specified for the expected workload on both the base table and its secondary index. Which of the following should the developer do to avoid any potential request throttling?",C,C,Ensure that the global secondary index's provisioned WCU is equal or less than the WCU of the base table.,Ensure that the global secondary index's provisioned RCU is equal or less than the RCU of the base table.,Ensure that the global secondary index's provisioned WCU is equal or greater than the WCU of the base table.,Ensure that the global secondary index's provisioned RCU is equal or greater than the RCU of the base table.,,,,개발자가 프로비저닝 모드 DynamoDB 테이블에 새로운 글로벌 보조 인덱스를 생성하고 있습니다. 애플리케이션은 많은 양의 데이터를 저장하므로 기본 테이블과 해당 보조 인덱스 모두에서 예상되는 워크로드에 대해 쓰기 용량 단위를 지정해야 합니다.다음 중 잠재적인 요청 제한을 방지하려면 개발자가 수행해야 하는 것은 무엇입니까?,글로벌 보조 인덱스의 프로비저닝된 WCU가 기본 테이블의 WCU보다 작거나 같은지 확인하세요.,글로벌 보조 인덱스의 프로비저닝된 RCU가 기본 테이블의 RCU보다 작거나 같은지 확인하세요.,글로벌 보조 인덱스의 프로비저닝된 WCU가 기본 테이블의 WCU보다 크거나 같은지 확인하세요.,글로벌 보조 인덱스의 프로비저닝된 RCU가 기본 테이블의 RCU보다 크거나 같은지 확인하세요.,,,0,,
udemy,DVA-02,263,"Your Lambda function initializes a lot of external dependencies such as database connections and HTTP endpoints, which are required for data processing. It also fetches static data with a size of 20 MB from a third-party provider over the Internet every time the function is invoked. This adds significant time in the total processing, which greatly affects the performance of their serverless application. Which of the following should you do to improve the performance of your function?",B,B,Use unreserved concurrency for your function.,Place the database and HTTP initialization logic outside the Lambda function handler and store the external files in the /tmp directory.,Increase the CPU allocation of the function by submitting a service limit increase ticket to AWS.,Allocate more memory to your function.,,,,Lambda 함수는 데이터 처리에 필요한 데이터베이스 연결 및 HTTP 엔드포인트와 같은 많은 외부 종속성을 초기화합니다. 또한 함수가 호출될 때마다 인터넷을 통해 타사 공급자로부터 20MB 크기의 정적 데이터를 가져옵니다. 이로 인해 전체 처리에 상당한 시간이 추가되어 서버리스 애플리케이션의 성능에 큰 영향을 미칩니다.다음 중 기능 성능을 향상하려면 무엇을 해야 합니까?,함수에 예약되지 않은 동시성을 사용합니다.,데이터베이스와 HTTP 초기화 로직을 Lambda 함수 핸들러 외부에 배치하고 외부 파일을 디렉터리에 저장합니다 /tmp.,AWS에 서비스 한도 증가 티켓을 제출하여 기능의 CPU 할당을 늘립니다.,함수에 더 많은 메모리를 할당하십시오.,,,0,,
udemy,DVA-02,264,A developer is planning to use the AWS Elastic Beanstalk console to run the AWS X-Ray daemon on the EC2 instances in her application environment. She will use X-Ray to construct a service map to help identify issues with her application and to provide insight on which application component to optimize. The environment is using a default Elastic Beanstalk instance profile. Which IAM managed policy does Elastic Beanstalk use for the X-Ray daemon to upload data to X-Ray?,B,B,AWSXrayReadOnlyAccess,AWSXRayDaemonWriteAccess,AWSXRayElasticBeanstalkWriteAccess,AWSXrayFullAccess,,,,개발자는 AWS Elastic Beanstalk 콘솔을 사용하여 애플리케이션 환경의 EC2 인스턴스에서 AWS X-Ray 데몬을 실행할 계획입니다. 그녀는 X-Ray를 사용하여 애플리케이션 문제를 식별하고 최적화할 애플리케이션 구성 요소에 대한 통찰력을 제공하는 서비스 맵을 구성할 것입니다. 환경은 기본 Elastic Beanstalk 인스턴스 프로필을 사용하고 있습니다.Elastic Beanstalk는 X-Ray 데몬이 X-Ray에 데이터를 업로드하는 데 어떤 IAM 관리형 정책을 사용합니까?,AWSXrayReadOnlyAccess,AWSXRayDaemonWriteAccess,AWSXRayElasticBeanstalkWriteAccess,AWSXrayFullAccess,,,0,,
udemy,DVA-02,265,"A company has a suite of web applications that is heavily using RDS database in Multi-AZ Deployments configuration with several Read Replicas. For improved security, you were instructed to ensure that all of their database credentials, API keys, and other secrets are encrypted and rotated on a regular basis. You should also configure your applications to use the latest version of the encrypted credentials when connecting to the RDS database. Which of the following is the MOST appropriate solution to secure the credentials?",C,C,Store the credentials in AWS KMS.,Store the credentials to Systems Manager Parameter Store with a SecureString data type.,Use AWS Secrets Manager to store and encrypt the credentials and enable automatic rotation.,Store the credentials to AWS ACM.,,,,"한 회사에는 여러 읽기 전용 복제본이 포함된 다중 AZ 배포 구성에서 RDS 데이터베이스를 많이 사용하는 웹 애플리케이션 제품군이 있습니다. 보안을 강화하기 위해 모든 데이터베이스 자격 증명, API 키 및 기타 비밀을 정기적으로 암호화하고 교체하도록 지시받았습니다. 또한 RDS 데이터베이스에 연결할 때 최신 버전의 암호화된 자격 증명을 사용하도록 애플리케이션을 구성해야 합니다.다음 중 자격 증명을 보호하는 데 가장 적합한 솔루션은 무엇입니까?",AWS KMS에 자격 증명을 저장합니다.,자격 증명을 데이터 유형으로 Systems Manager Parameter Store에 저장합니다 SecureString.,AWS Secrets Manager를 사용하여 자격 증명을 저장 및 암호화하고 자동 교체를 활성화합니다.,AWS ACM에 자격 증명을 저장합니다.,,,0,,
udemy,DVA-02,266,"A company is using a combination of CodeCommit, CodeBuild, CodePipeline and CodeDeploy services for its continuous integration and continuous delivery (CI/CD) pipeline on AWS. They want someone to perform a code review before a revision is allowed into the next stage of a pipeline. If the action is approved, the pipeline execution resumes but if it is not then the pipeline execution will not proceed. Which of the following is the MOST suitable solution to implement in this scenario?",B,B,Remodel the pipeline using AWS Serverless Application Model (AWS SAM),Implement a manual approval actions configuration in CodePipeline. Send the approval request to an SNS Topic.,Implement a manual approval actions configuration in CodePipeline. Send the approval request to an SQS Queue.,Split the processes into different Task states using Step Functions. Use a Wait state to set a timeout for approval.,,,,"한 회사는 AWS에서 CI/CD(지속적인 통합 및 지속적 전달) 파이프라인을 위해 CodeCommit, CodeBuild, CodePipeline 및 CodeDeploy 서비스의 조합을 사용하고 있습니다. 그들은 수정본이 파이프라인의 다음 단계로 넘어가기 전에 누군가가 코드 검토를 수행하기를 원합니다. 작업이 승인되면 파이프라인 실행이 재개되지만 승인되지 않으면 파이프라인 실행이 진행되지 않습니다.다음 중 이 시나리오에 구현하기에 가장 적합한 솔루션은 무엇입니까?",AWS Serverless Application Model(AWS SAM)을 사용하여 파이프라인 리모델링,CodePipeline에서 수동 승인 작업 구성을 구현합니다. SNS 주제에 승인 요청을 보냅니다.,CodePipeline에서 수동 승인 작업 구성을 구현합니다. SQS 대기열에 승인 요청을 보냅니다.,Step Functions를 사용하여 프로세스를 다양한 작업 상태로 분할합니다. 대기 상태를 사용하여 승인 시간 초과를 설정합니다.,,,0,,
udemy,DVA-02,267,"A company is transitioning their systems to AWS due to the limitations of their on-premises data center. As part of this project, a developer was assigned to build a brand new serverless architecture in AWS, which will be composed of AWS Lambda, API Gateway, and DynamoDB in a single stack. She needs a simple and reliable framework that will allow her to share configuration such as memory and timeouts between resources and deploy all related resources together as a single, versioned entity. Which of the following is the MOST appropriate service that the developer should use in this scenario?",A,A,AWS SAM,AWS OpsWorks,AWS CloudFormation,Serverless Application Framework,,,,"회사는 온프레미스 데이터 센터의 한계로 인해 시스템을 AWS로 전환하고 있습니다. 이 프로젝트의 일환으로 개발자는 AWS에서 단일 스택에 AWS Lambda, API Gateway 및 DynamoDB로 구성될 새로운 서버리스 아키텍처를 구축하도록 배정되었습니다. 그녀에게는 리소스 간에 메모리 및 시간 초과와 같은 구성을 공유하고 모든 관련 리소스를 버전이 지정된 단일 엔터티로 함께 배포할 수 있는 간단하고 안정적인 프레임워크가 필요합니다.다음 중 이 시나리오에서 개발자가 사용해야 하는 가장 적절한 서비스는 무엇입니까?",AWS SAM,AWS OpsWorks,AWS 클라우드포메이션,서버리스 애플리케이션 프레임워크,,,0,,
udemy,DVA-02,268,You are designing the DynamoDB table that will be used by your Node.js application. It will have to handle 10 writes per second and then 20 eventually consistent reads per second where all the items have a size of 2 KB for both operations. Which of the following are the most optimal WCU and RCU that you should provision to the table?,D,D,20 RCU and 20 WCU,40 RCU and 20 WCU,40 RCU and 40 WCU,10 RCU and 20 WCU,,,,Node.js 애플리케이션에서 사용할 DynamoDB 테이블을 설계하고 있습니다. 초당 10개의 쓰기를 처리한 다음 두 작업 모두에 대해 모든 항목의 크기가 2KB인 초당 20개의 최종 일관된 읽기를 처리해야 합니다.다음 중 테이블에 프로비저닝해야 하는 가장 최적의 WCU 및 RCU는 무엇입니까?,RCU 20개 및 WCU 20개,RCU 40개 및 WCU 20개,RCU 40개 및 WCU 40개,RCU 10개 및 WCU 20개,,,0,,
udemy,DVA-02,269,"A company is developing a serverless website that consists of images, videos, HTML pages and JavaScript files. There is also a requirement to serve the files with lowest possible latency to its global users. Which combination of services should be used in this scenario? (Select TWO.)",BE,BE,Amazon Glacier,Amazon S3,Amazon Elastic File System,AWS CodeStar,Amazon CloudFront,,,"한 회사가 이미지, 비디오, HTML 페이지 및 JavaScript 파일로 구성된 서버리스 웹 사이트를 개발하고 있습니다. 또한 글로벌 사용자에게 가능한 가장 짧은 대기 시간으로 파일을 제공해야 한다는 요구 사항도 있습니다.이 시나리오에서는 어떤 서비스 조합을 사용해야 합니까? (2개를 선택하세요.)",아마존 빙하,아마존 S3,Amazon 탄력적 파일 시스템,AWS 코드스타,아마존 클라우드프론트,,0,,
udemy,DVA-02,270,"A company has recently developed a containerized application that uses a multicontainer Docker platform which supports multiple containers per instance. They need a service that automatically handles tasks such as provisioning of the resources, load balancing, auto-scaling, monitoring, and placing the containers across the cluster. Which of the following services provides the EASIEST way to accomplish the above requirement?",C,C,EKS,ECS,Elastic Beanstalk,Lambda,,,,"한 회사는 최근 인스턴스당 여러 컨테이너를 지원하는 멀티컨테이너 Docker 플랫폼을 사용하는 컨테이너화된 애플리케이션을 개발했습니다. 리소스 프로비저닝, 로드 밸런싱, 자동 크기 조정, 모니터링, 클러스터 전체에 컨테이너 배치 등의 작업을 자동으로 처리하는 서비스가 필요합니다.다음 중 위의 요구 사항을 충족하는 가장 쉬운 방법을 제공하는 서비스는 무엇입니까?",전,ECS,엘라스틱 콩나무,람다,,,0,,
udemy,DVA-02,271,A product design firm has adopted a remote work policy and wants to provide employees with access to a suite of CAD software through EC2 Spot instances. These instances will be deployed using a CloudFormation template. The development team must be able to securely obtain software license keys in the template each time it is needed.Which solution meets this requirement while offering the most secure and cost-effective approach?,D,D,Pass the license key in the Parameter section of the CloudFormation template during stack creation. Enable the NoEcho attribute on the parameter.,Store the license key as a secret in AWS Secrets Manager. Use the secretsmanager dynamic reference to retrieve the secret in the CloudFormation template.,Embed the license keys in the Mapping section of the CloudFormation template. Let users choose the correct license key using the Parameter section. Enable the NoEcho attribute on the parameter.,Store the license key as a SecureString in AWS Systems Manager (SSM) Parameter Store. Use the ssm-secure dynamic reference to retrieve the secret in the CloudFormation template.,,,,제품 디자인 회사는 원격 작업 정책을 채택했으며 직원들에게 EC2 스팟 인스턴스를 통해 CAD 소프트웨어 제품군에 대한 액세스를 제공하려고 합니다. 이러한 인스턴스는 CloudFormation 템플릿을 사용하여 배포됩니다. 개발 팀은 필요할 때마다 템플릿에서 소프트웨어 라이센스 키를 안전하게 얻을 수 있어야 합니다.가장 안전하고 비용 효율적인 접근 방식을 제공하면서 이 요구 사항을 충족하는 솔루션은 무엇입니까?,Parameter스택 생성 중에 CloudFormation 템플릿 섹션 에 라이선스 키를 전달합니다 . NoEcho매개변수의 속성을 활성화합니다 .,라이선스 키를 AWS Secrets Manager에 비밀로 저장합니다. 동적 참조를 사용하여 secretsmanagerCloudFormation 템플릿에서 비밀을 검색합니다.,MappingCloudFormation 템플릿의 섹션 에 라이선스 키를 포함합니다 . 섹션 을 사용하여 사용자가 올바른 라이센스 키를 선택하도록 하십시오 Parameter. NoEcho매개변수의 속성을 활성화합니다 .,라이선스 키를 SecureStringAWS Systems Manager(SSM) Parameter Store에 저장합니다. 동적 참조를 사용하여 ssm-secureCloudFormation 템플릿에서 비밀을 검색합니다.,,,0,,
udemy,DVA-02,272,"A developer is planning to add a global secondary index in a DynamoDB table. This will allow the application to query a specific index that can span all of the data in the base table, across all partitions. Which of the following should the developer consider when using this type of index? (Select TWO.)",BE,BE,"When you query this index, you can choose either eventual consistency or strong consistency.",Queries on this index support eventual consistency only.,"For each partition key value, the total size of all indexed items must be 10 GB or less.",Queries or scans on this index consume read capacity units from the base table.,"Queries or scans on this index consume capacity units from the index, not from the base table.",,,개발자가 DynamoDB 테이블에 글로벌 보조 인덱스를 추가할 계획입니다. 이를 통해 애플리케이션은 모든 파티션에 걸쳐 기본 테이블의 모든 데이터를 포괄할 수 있는 특정 인덱스를 쿼리할 수 있습니다.이 유형의 인덱스를 사용할 때 개발자는 다음 중 무엇을 고려해야 합니까? (2개를 선택하세요.),이 인덱스를 쿼리할 때 최종 일관성 또는 강력한 일관성을 선택할 수 있습니다.,이 인덱스에 대한 쿼리는 최종 일관성만 지원합니다.,각 파티션 키 값에 대해 인덱싱된 모든 항목의 총 크기는 10GB 이하여야 합니다.,이 인덱스에 대한 쿼리 또는 스캔은 기본 테이블의 읽기 용량 단위를 사용합니다.,이 인덱스에 대한 쿼리 또는 스캔은 기본 테이블이 아닌 인덱스의 용량 단위를 사용합니다.,,0,,
udemy,DVA-02,273,"The operating cost of a serverless application is quite high and you are instructed to look for ways to lower the costs. As part of its processing, a Lambda function sends 320 strongly consistent read requests per second to a DynamoDB table which has a provisioned RCU of 5440. The average size of items stored in the database is 17 KB. Which of the following is the MOST suitable action that should you do to make the application more cost-effective while maintaining its performance?",C,C,Implement exponential backoff.,Switch the table from using provisioned mode to on-demand mode.,Set the provisioned RCU to 1600.,Decrease the provisioned RCU down to 800.,,,,서버리스 애플리케이션의 운영 비용은 상당히 높으며 비용을 낮추는 방법을 찾도록 지시받습니다. 처리의 일부로 Lambda 함수는 프로비저닝된 RCU가 5440인 DynamoDB 테이블에 초당 320개의 강력한 일관된 읽기 요청을 보냅니다. 데이터베이스에 저장된 항목의 평균 크기는 17KB입니다.다음 중 성능을 유지하면서 애플리케이션을 더욱 비용 효율적으로 만들기 위해 수행해야 하는 가장 적합한 조치는 무엇입니까?,지수 백오프를 구현합니다.,테이블을 프로비저닝 모드에서 온디맨드 모드로 전환합니다.,프로비저닝된 RCU를 1600으로 설정합니다.,프로비저닝된 RCU를 800으로 줄입니다.,,,0,,
udemy,DVA-02,274,You are planning to launch a Lambda function integrated with API Gateway. It is required to specify how the incoming request data is mapped to the integration request and how the resulting integration response data is mapped to the method response. Which of the following options is the MOST appropriate method use to meet this requirement?,B,B,HTTP Proxy integration,Lambda custom integration,HTTP custom integration,Lambda proxy integration,,,,API Gateway와 통합된 Lambda 함수를 시작할 계획입니다. 수신 요청 데이터가 통합 요청에 매핑되는 방식과 결과 통합 응답 데이터가 메서드 응답에 매핑되는 방식을 지정해야 합니다.다음 옵션 중 이 요구 사항을 충족하는 데 가장 적합한 방법은 무엇입니까?,HTTP 프록시 통합,Lambda 사용자 정의 통합,HTTP 사용자 정의 통합,Lambda 프록시 통합,,,0,,
udemy,DVA-02,275,"A developer recently deployed a serverless application, which consists of a Lambda function, API Gateway, and DynamoDB using the sam deploy CLI command. The Lambda function is invoked through the API Gateway and then processes and stores the data in a DynamoDB table with an average time of 20 minutes. However, the IT Support team noticed that there are several terminated Lambda invocations that happen every day, which is causing data discrepancies.Which of the following options is the MOST likely root cause of this problem?",A,A,The failed Lambda invocations have been running for over 15 minutes and reached the maximum execution time.,The serverless application should be deployed using the sam publish CLI command instead.,The concurrent execution limit has been reached.,The Lambda function contains a recursive code and has been running for over 15 minutes.,,,,"개발자는 최근 sam deployCLI 명령을 사용하여 Lambda 함수, API 게이트웨이 및 DynamoDB로 구성된 서버리스 애플리케이션을 배포했습니다. Lambda 함수는 API 게이트웨이를 통해 호출된 후 평균 20분 동안 DynamoDB 테이블의 데이터를 처리하고 저장합니다. 그러나 IT 지원 팀은 매일 발생하는 Lambda 호출이 여러 번 종료되어 데이터 불일치가 발생한다는 사실을 발견했습니다.다음 옵션 중 이 문제의 근본 원인일 가능성이 가장 높은 것은 무엇입니까?",실패한 Lambda 호출이 15분 이상 실행되어 최대 실행 시간에 도달했습니다.,대신 CLI 명령을 사용하여 서버리스 애플리케이션을 배포해야 합니다 sam publish.,동시 실행 제한에 도달했습니다.,Lambda 함수에는 재귀 코드가 포함되어 있으며 15분 이상 실행되었습니다.,,,0,,
udemy,DVA-02,276,"A developer has a Node.js function running in AWS Lambda. Currently, the code initializes a database connection to an Amazon RDS database every time the Lambda function is executed, and closes the connection before the function ends. What feature in AWS Lambda will allow the developer to reuse the already existing database connection instead of initializing it each time the function is run?",B,B,AWS Lambda is not capable of maintaining existing database connections due to its transient data store.,Execution context,Event source mapping,Environment variables,,,,개발자는 AWS Lambda에서 실행되는 Node.js 함수를 가지고 있습니다. 현재 코드는 Lambda 함수가 실행될 때마다 Amazon RDS 데이터베이스에 대한 데이터베이스 연결을 초기화하고 함수가 끝나기 전에 연결을 닫습니다.AWS Lambda의 어떤 기능을 통해 개발자는 함수가 실행될 때마다 초기화하는 대신 기존 데이터베이스 연결을 재사용할 수 있습니까?,AWS Lambda는 임시 데이터 저장소로 인해 기존 데이터베이스 연결을 유지할 수 없습니다.,실행 컨텍스트,이벤트 소스 매핑,환경 변수,,,0,,
udemy,DVA-02,277,"You are a developer for a global technology company, which heavily uses AWS with regional offices in San Francisco, Manila, and Bangalore. Most of the clients of your company are using serverless computing in which you are responsible for ensuring that their applications are working efficiently. Which of the following options are valid considerations in improving the performance of your Lambda function? (Select TWO.)",BC,BC,You have to install the X-Ray daemon in Lambda to enable active tracing.,The concurrent execution limit is enforced against the sum of the concurrent executions of all function.,An increase in memory size triggers an equivalent increase in CPU available to your function.,Lambda automatically creates Elastic IP's that enable your function to connect securely to other resources within your private VPC.,You can throttle all incoming executions and stop processing any invocations to your function by setting concurrency to false.,,,"귀하는 샌프란시스코, 마닐라 및 방갈로르에 지역 사무소를 두고 AWS를 많이 사용하는 글로벌 기술 회사의 개발자입니다. 회사의 클라이언트 대부분은 애플리케이션이 효율적으로 작동하는지 확인하는 책임이 있는 서버리스 컴퓨팅을 사용하고 있습니다.다음 중 Lambda 함수의 성능을 향상시킬 때 고려해야 할 유효한 옵션은 무엇입니까? (2개를 선택하세요.)",활성 추적을 활성화하려면 Lambda에 X-Ray 데몬을 설치해야 합니다.,동시 실행 제한은 모든 함수의 동시 실행 합계에 대해 적용됩니다.,메모리 크기가 증가하면 함수에 사용할 수 있는 CPU도 그에 상응하게 증가합니다.,Lambda는 함수가 프라이빗 VPC 내의 다른 리소스에 안전하게 연결할 수 있도록 하는 탄력적 IP를 자동으로 생성합니다.,동시성을 으로 설정하면 들어오는 모든 실행을 제한하고 함수 호출 처리를 중지할 수 있습니다 false.,,0,,
udemy,DVA-02,278,"An e-commerce application, which is hosted in an ECS Cluster, contains the connection string of an external database and other sensitive configuration files. Since the application accepts credit card payments, the company has to meet strict security compliance which requires that the database credentials are encrypted and periodically rotated. Which of the following should you do to comply to the requirements?",A,A,Store the database credentials in AWS Secrets Manager and enable rotation.,Store the database credentials as a secure string parameter in Systems Manager Parameter Store.,Store the database credentials in an encrypted ecs.config configuration file.,Store the database credentials in an encrypted dockerrun.aws.json configuration file.,,,,ECS 클러스터에서 호스팅되는 전자 상거래 애플리케이션에는 외부 데이터베이스의 연결 문자열과 기타 중요한 구성 파일이 포함되어 있습니다. 애플리케이션이 신용 카드 결제를 허용하므로 회사는 데이터베이스 자격 증명을 암호화하고 주기적으로 교체해야 하는 엄격한 보안 규정을 준수해야 합니다.요구사항을 준수하려면 다음 중 무엇을 해야 합니까?,AWS Secrets Manager에 데이터베이스 자격 증명을 저장하고 교체를 활성화합니다.,데이터베이스 자격 증명을 Systems Manager Parameter Store에 secure string 파라미터로 저장하십시오.,암호화된 구성 파일에 데이터베이스 자격 증명을 저장합니다 ecs.config.,암호화된 구성 파일에 데이터베이스 자격 증명을 저장합니다 dockerrun.aws.json.,,,0,,
udemy,DVA-02,279,"A company has assigned a developer to automate the patch management, data synchronization, and other recurring tasks in their department. The developer needs to have a service which can coordinate multiple AWS services into serverless workflows. Which of the following is the MOST cost-effective service that the developer should implement in this scenario?",C,C,AWS Batch,AWS Lambda,AWS Step Functions,SWF,,,,"한 회사에서는 부서에서 패치 관리, 데이터 동기화 및 기타 반복 작업을 자동화하도록 개발자를 지정했습니다. 개발자는 여러 AWS 서비스를 서버리스 워크플로로 조정할 수 있는 서비스가 필요합니다.다음 중 이 시나리오에서 개발자가 구현해야 하는 가장 비용 효율적인 서비스는 무엇입니까?",AWS 배치,AWS 람다,AWS 단계 함수,SWF,,,0,,
udemy,DVA-02,280,A developer uses AWS X-Ray to create a trace on an instrumented web application to identify any performance bottlenecks. The segment documents being sent by the application contain annotations that the developer wants to utilize in order to identify and filter out specific data from the trace.Which of the following should the developer do in order to satisfy this requirement with minimal configuration? (Select TWO.),AD,AD,Fetch the trace IDs and annotations using the GetTraceSummaries API.,Send trace results to an S3 bucket then query the trace output using Amazon Athena.,Configure Sampling Rules in the AWS X-Ray Console.,Use filter expressions via the X-Ray console.,Fetch the data using the BatchGetTraces API.,,,개발자는 AWS X-Ray를 사용하여 계측된 웹 애플리케이션에서 추적을 생성하여 성능 병목 현상을 식별합니다. 애플리케이션에서 전송되는 세그먼트 문서에는 추적에서 특정 데이터를 식별하고 필터링하기 위해 개발자가 활용하려는 주석이 포함되어 있습니다.최소한의 구성으로 이 요구 사항을 충족하려면 다음 중 개발자가 수행해야 하는 작업은 무엇입니까? (2개를 선택하세요.),API 를 사용하여 추적 ID와 주석을 가져옵니다 GetTraceSummaries.,추적 결과를 S3 버킷으로 보낸 다음 Amazon Athena를 사용하여 추적 출력을 쿼리합니다.,AWS X-Ray 콘솔에서 샘플링 규칙을 구성합니다.,X-Ray 콘솔을 통해 필터 표현식을 사용하세요.,API를 사용하여 데이터를 가져옵니다 BatchGetTraces.,,0,,
udemy,DVA-02,281,"A serverless application, which uses a DynamoDB database, is experiencing throttling issues during peak times. To troubleshoot the problem, you were instructed to get the total number of write capacity units consumed for the table and any secondary indexes whenever the UpdateItem operation is sent. In this scenario, what is the MOST appropriate value for the ReturnConsumedCapacity parameter that you should set in the update request?",C,C,TRUE,NONE,INDEXES,TOTAL,,,,DynamoDB 데이터베이스를 사용하는 서버리스 애플리케이션에서 피크 시간대에 조절 문제가 발생합니다. UpdateItem문제를 해결하기 위해 작업이 전송될 때마다 테이블 및 보조 인덱스에 사용된 총 쓰기 용량 단위 수를 가져오라는 지시를 받았습니다 .ReturnConsumedCapacity이 시나리오에서 업데이트 요청에 설정해야 하는 매개변수 에 가장 적합한 값은 무엇입니까 ?,TRUE,NONE,INDEXES,TOTAL,,,0,,
udemy,DVA-02,282,"A developer is managing an application hosted in EC2, which stores data in an S3 bucket. To comply with the new security policy, the developer must ensure that the data is encrypted at rest using an encryption key that is provided and managed by the company. The change should also provide AES-256 encryption to their data.Which of the following actions could the developer take to achieve this? (Select TWO.)",AE,AE,Encrypt the data on the client-side before sending to Amazon S3 using their own master key.,Implement Amazon S3 server-side encryption with AWS KMS-Managed Keys (SSE-KMS).,Implement Amazon S3 server-side encryption with Amazon S3-Managed Encryption Keys.,Use SSL to encrypt the data while in transit to Amazon S3.,Implement Amazon S3 server-side encryption with customer-provided keys (SSE-C).,,,개발자는 S3 버킷에 데이터를 저장하는 EC2에서 호스팅되는 애플리케이션을 관리하고 있습니다. 새로운 보안 정책을 준수하기 위해 개발자는 회사에서 제공하고 관리하는 암호화 키를 사용하여 저장 데이터가 암호화되었는지 확인해야 합니다. 또한 변경 사항은 데이터에 AES-256 암호화를 제공해야 합니다.이를 달성하기 위해 다음 중 개발자가 취할 수 있는 조치는 무엇입니까? (2개를 선택하세요.),자체 마스터 키를 사용하여 Amazon S3로 보내기 전에 클라이언트 측에서 데이터를 암호화합니다.,AWS KMS 관리형 키(SSE-KMS)를 사용하여 Amazon S3 서버 측 암호화를 구현합니다.,Amazon S3 관리형 암호화 키를 사용하여 Amazon S3 서버 측 암호화를 구현합니다.,SSL을 사용하여 Amazon S3로 전송되는 동안 데이터를 암호화합니다.,고객 제공 키(SSE-C)를 사용하여 Amazon S3 서버 측 암호화를 구현합니다.,,0,,
udemy,DVA-02,283,"A developer is managing a real-time fraud detection system that ingests a stream of data using Amazon Kinesis. The system works well with millisecond end-to-end latency, but the allocated shards are way underutilized based on the performance data in CloudWatch.Which of the following is the MOST suitable solution to reduce the cost and capacity of the stream?",A,A,Merge cold shards,Split hot shards,Merge hot shards,Split cold shards,,,,개발자는 Amazon Kinesis를 사용하여 데이터 스트림을 수집하는 실시간 사기 탐지 시스템을 관리하고 있습니다. 시스템은 밀리초 단위의 종단 간 지연 시간으로 잘 작동하지만 CloudWatch의 성능 데이터에 따르면 할당된 샤드의 활용도가 훨씬 낮습니다.다음 중 스트림의 비용과 용량을 줄이는 데 가장 적합한 솔루션은 무엇입니까?,콜드 샤드 병합,핫 샤드 분할,핫 샤드 병합,콜드 샤드 분할,,,0,,
udemy,DVA-02,284,A developer has recently launched a new API Gateway service which is integrated with AWS Lambda. He enabled API caching and per-key cache invalidation features in the API Gateway to comply with the requirement of the front-end development team which will use the API. The front-end team will have to invalidate an existing cache entry in some scenarios and fetch the latest data from the integration endpoint. Which of the following should the consumers of the API do to invalidate the cache in API Gateway?,B,B,Send a request with the Cache-Control: no-cache header.,Send a request with the Cache-Control: max-age=0 header.,Send a request with the Cache-Control: INVALIDATE_CACHE header.,Configure the front-end application to clear the browser cache before fetching data from API Gateway.,,,,한 개발자가 최근 AWS Lambda와 통합된 새로운 API 게이트웨이 서비스를 출시했습니다. 그는 API를 사용할 프런트 엔드 개발팀의 요구 사항을 준수하기 위해 API 게이트웨이에서 API 캐싱 및 키별 캐시 무효화 기능을 활성화했습니다. 프런트엔드 팀은 일부 시나리오에서 기존 캐시 항목을 무효화하고 통합 엔드포인트에서 최신 데이터를 가져와야 합니다.다음 중 API Gateway의 캐시를 무효화하려면 API 소비자가 수행해야 하는 작업은 무엇입니까?,헤더 와 함께 요청을 보냅니다 Cache-Control: no-cache.,헤더 와 함께 요청을 보냅니다 Cache-Control: max-age=0.,헤더 와 함께 요청을 보냅니다 Cache-Control: INVALIDATE_CACHE.,API Gateway에서 데이터를 가져오기 전에 브라우저 캐시를 지우도록 프런트엔드 애플리케이션을 구성합니다.,,,0,,
udemy,DVA-02,285,Your application is hosted on an Auto Scaling group of EC2 instances with a DynamoDB database. There were a lot of data discrepancy issues where the changes made by one user were always overwritten by another user. You noticed that this usually happens whenever there are a lot of people updating the same data. What should you do to solve this problem?,C,C,Use DynamoDB global tables and implement an optimistic locking strategy.,Implement a pessimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table.,Implement an optimistic locking strategy in your application source code by designating one property to store the version number in the mapping class for your table.,Use DynamoDB global tables and implement a pessimistic locking strategy.,,,,귀하의 애플리케이션은 DynamoDB 데이터베이스가 있는 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅됩니다. 한 사용자가 변경한 내용을 항상 다른 사용자가 덮어쓰는 데이터 불일치 문제가 많이 있었습니다. 동일한 데이터를 업데이트하는 사람이 많을 때마다 이런 일이 일반적으로 발생한다는 것을 알았습니다.이 문제를 해결하려면 어떻게 해야 합니까?,DynamoDB 전역 테이블을 사용하고 낙관적 잠금 전략을 구현합니다.,테이블의 매핑 클래스에 버전 번호를 저장하는 속성 하나를 지정하여 애플리케이션 소스 코드에서 비관적 잠금 전략을 구현합니다.,테이블의 매핑 클래스에 버전 번호를 저장하는 속성 하나를 지정하여 애플리케이션 소스 코드에서 낙관적 잠금 전략을 구현합니다.,DynamoDB 전역 테이블을 사용하고 비관적 잠금 전략을 구현합니다.,,,0,,
udemy,DVA-02,286,An Elastic Beanstalk application becomes inaccessible for several minutes whenever a failed deployment is rolled back. A developer should recommend a strategy that will have the least impact on the application's availability if the deployment fails. Teams must be able to revert changes quickly as well.Which deployment method should the developer suggest?,D,D,Rolling,All at Once,Rolling with Additional Batches,Blue/Green,,,,실패한 배포가 롤백될 때마다 몇 분 동안 Elastic Beanstalk 애플리케이션에 액세스할 수 없게 됩니다. 개발자는 배포가 실패할 경우 응용 프로그램의 가용성에 최소한의 영향을 미치는 전략을 권장해야 합니다. 팀은 변경 사항을 신속하게 되돌릴 수도 있어야 합니다.개발자는 어떤 배포 방법을 제안해야 합니까?,구르는,한꺼번에,추가 배치로 롤링,블루/그린,,,0,,
udemy,DVA-02,287,You are configuring the task definitions of your ECS Cluster in AWS to make sure that the tasks are scheduled on instances with enough resources to run them. It should also follow the constraints that you specified both implicitly or explicitly. Which of the following options should you implement to satisfy the requirement which requires the LEAST amount of configuration?,D,D,Use a spread task placement strategy which uses the instanceId and host attributes.,Use a binpack task placement strategy.,Use a spread task placement strategy with custom placement constraints.,Use a random task placement strategy.,,,,작업을 실행하기에 충분한 리소스가 있는 인스턴스에서 작업이 예약되도록 AWS에서 ECS 클러스터의 작업 정의를 구성하고 있습니다. 또한 암시적 또는 명시적으로 지정한 제약 조건을 따라야 합니다.최소한의 구성이 필요한 요구 사항을 충족하려면 다음 중 어떤 옵션을 구현해야 합니까?,및 속성을 spread사용하는 작업 배치 전략을 사용합니다 .instanceIdhost,작업 배치 전략을 사용하십시오 binpack.,사용자 지정 배치 제약 조건이 포함된 작업 배치 전략을 사용하세요 spread.,작업 배치 전략을 사용하십시오 random.,,,0,,
udemy,DVA-02,288,"A development team has recently completed building their serverless application and they are now ready to deploy it to AWS. They need to zip their code artifacts, upload them to Amazon S3, and produce the package template file for deployment.Which combination of commands is the MOST suitable for automating the deployment steps? (Select TWO.)",CE,CE,aws cloudformation package,sam publish,sam deploy,aws cloudformation deploy,sam package,,,개발 팀은 최근 서버리스 애플리케이션 구축을 완료했으며 이제 이를 AWS에 배포할 준비가 되었습니다. 코드 아티팩트를 압축하여 Amazon S3에 업로드하고 배포용 패키지 템플릿 파일을 생성해야 합니다.배포 단계를 자동화하는 데 가장 적합한 명령 조합은 무엇입니까? (2개를 선택하세요.),aws cloudformation package,sam publish,sam deploy,aws cloudformation deploy,sam package,,0,,
udemy,DVA-02,289,"A web application hosted in Elastic Beanstalk has a configuration file named .ebextensions/debugging.config which has the following content: option_settings:  aws:elasticbeanstalk:xray:   XRayEnabled: trueFor its database tier, it uses RDS with Multi-AZ deployments configuration and Read Replicas. There is a new requirement to record calls that your application makes to RDS and other internal or external HTTP web APIs. The tracing information should also include the actual SQL database queries sent by the application, which can be searched using the filter expressions in the X-Ray Console. Which of the following should you do to satisfy the above task?",B,B,Add metadata in the segment document.,Add annotations in the subsegment section of the segment document.,Add annotations in the segment document.,Add metadata in the subsegment section of the segment document.,,,,Elastic Beanstalk에서 호스팅되는 웹 애플리케이션에는 .ebextensions/debugging.config다음 콘텐츠가 포함된 구성 파일이 있습니다.옵션_설정 : aws : elasticbeanstalk : 엑스레이 :   XRayEnabled : 사실 데이터베이스 계층의 경우 다중 AZ 배포 구성 및 읽기 전용 복제본이 포함된 RDS를 사용합니다. 애플리케이션에서 RDS 및 기타 내부 또는 외부 HTTP 웹 API에 대한 호출을 기록해야 하는 새로운 요구 사항이 있습니다. 추적 정보에는 X-Ray 콘솔의 필터 표현식을 사용하여 검색할 수 있는 애플리케이션에서 보낸 실제 SQL 데이터베이스 쿼리도 포함되어야 합니다.위 작업을 충족하려면 다음 중 무엇을 해야 합니까?,세그먼트 문서에 메타데이터를 추가합니다.,세그먼트 문서의 하위 세그먼트 섹션에 주석을 추가합니다.,세그먼트 문서에 주석을 추가합니다.,세그먼트 문서의 하위 세그먼트 섹션에 메타데이터를 추가합니다.,,,0,,
udemy,DVA-02,290,A startup has recently launched their new mobile game and is gaining a lot of new users everyday. The founders plan to add a new feature which will enable cross-device syncing of user profile data across mobile devices to improve the user experience. Which of the following services should they use to meet this requirement?,A,A,Cognito Sync,AWS Amplify,Cognito Identity Pools,Cognito User Pools,,,,최근 한 스타트업이 새로운 모바일 게임을 출시했고 매일 많은 신규 사용자를 확보하고 있습니다. 창립자는 사용자 경험을 향상시키기 위해 모바일 장치 전반에 걸쳐 사용자 프로필 데이터의 장치 간 동기화를 가능하게 하는 새로운 기능을 추가할 계획입니다.이 요구 사항을 충족하려면 다음 중 어떤 서비스를 사용해야 합니까?,동기화 알아보기,AWS 증폭,Cognito 자격 증명 풀,Cognito 사용자 풀,,,0,,
udemy,DVA-02,291,A company uses AWS Systems Manager (SSM) Parameter Store to manage configuration details for multiple applications. The parameters are currently stored in the Standard tier. The company wants its operations team to be notified if there are sensitive parameters that haven’t been rotated within 90 days.Which must be done to meet the requirement?,D,D,Configure a NoChangeNotification policy with a value of 90 days. Use Amazon EventBridge to send a notification via Amazon SNS.,Set up an Amazon EventBridge event pattern that captures SSM Parameter-related events. Use Amazon SNS to send notifications.,Convert the sensitive parameters from Standard tier into Advanced tier. Set a ExpirationNotification policy with a value of 90 days. Use Amazon EventBridge to send a notification via Amazon SNS.,Convert the sensitive parameters from Standard tier into Advanced tier. Set a NoChangeNotification policy with a value of 90 days. Use Amazon EventBridge to send a notification via Amazon SNS.,,,,회사에서는 AWS Systems Manager(SSM) Parameter Store를 사용하여 여러 애플리케이션에 대한 구성 세부 정보를 관리합니다. 매개변수는 현재 표준 계층에 저장되어 있습니다. 회사는 90일 이내에 교체되지 않은 민감한 매개변수가 있는 경우 운영팀에 알림을 보내기를 원합니다.요구 사항을 충족하려면 어떤 작업을 수행해야 합니까?,NoChangeNotification값이 90일인 정책을 구성합니다 . Amazon EventBridge를 사용하여 Amazon SNS를 통해 알림을 보냅니다.,SSM 파라미터 관련 이벤트를 캡처하는 Amazon EventBridge 이벤트 패턴을 설정합니다. Amazon SNS를 사용하여 알림을 보냅니다.,민감한 매개변수를 표준 계층에서 고급 계층으로 변환합니다. ExpirationNotification90일 값으로 정책을 설정합니다 . Amazon EventBridge를 사용하여 Amazon SNS를 통해 알림을 보냅니다.,민감한 매개변수를 표준 계층에서 고급 계층으로 변환합니다. NoChangeNotification90일 값으로 정책을 설정합니다 . Amazon EventBridge를 사용하여 Amazon SNS를 통해 알림을 보냅니다.,,,0,,
udemy,DVA-02,292,"A leading insurance firm is hosting its customer portal in Elastic Beanstalk, which has an RDS database in AWS. The support team in your company discovered a lot of SQL injection attempts and cross-site scripting attacks on the portal, which is starting to affect the production environment.Which of the following services should you implement to mitigate this attack?",C,C,AWS Firewall Manager,Network Access Control List,AWS WAF,Amazon Guard​Duty,,,,"한 선도적인 보험 회사는 AWS에 RDS 데이터베이스가 있는 Elastic Beanstalk에서 고객 포털을 호스팅하고 있습니다. 귀사의 지원팀은 포털에서 많은 SQL 주입 시도와 크로스 사이트 스크립팅 공격을 발견했으며, 이는 프로덕션 환경에 영향을 미치기 시작했습니다.이 공격을 완화하려면 다음 중 어떤 서비스를 구현해야 합니까?",AWS 방화벽 관리자,네트워크 액세스 제어 목록,AWS WAF,아마존 경비 임무,,,0,,
udemy,DVA-02,293,"You are working as a software developer for an online training company, which is currently developing a learning portal that will use a DynamoDB table. One of the acceptance criteria requires you to ensure that there will be no hot partitions in the table which will result in throttling and inefficient use of your provisioned I/O capacity. The portal contains hundreds of thousands of online courses including the ones from their 3rd-party educational partners, which may or may not have the same Course ID. The table is structured as shown below: Which of the following is the MOST suitable partition key to use in this scenario?",B,B,Course Name,Item ID,Course Price,Course ID,,,,귀하는 현재 DynamoDB 테이블을 사용할 학습 포털을 개발 중인 온라인 교육 회사의 소프트웨어 개발자로 일하고 있습니다. 허용 기준 중 하나는 테이블에 프로비저닝된 I/O 용량의 조절 및 비효율적인 사용을 초래하는 핫 파티션이 없는지 확인하도록 요구합니다. 포털에는 동일한 코스 ID가 있을 수도 있고 없을 수도 있는 타사 교육 파트너의 코스를 포함하여 수십만 개의 온라인 코스가 포함되어 있습니다.테이블은 아래와 같이 구성됩니다.다음 중 이 시나리오에 사용하기에 가장 적합한 파티션 키는 무엇입니까?,코스명,아이템 ID,코스 가격,코스 ID,,,0,,
udemy,DVA-02,294,The company that you are working for recently decided to migrate and transform their monolithic application on-premises to a Lambda application. It is your responsibility to ensure that application works effectively in AWS. Which of the following are the best practices in developing Lambda functions? (Select TWO.),BE,BE,Use recursive code.,Take advantage of Execution Context reuse to improve the performance of your function.,Include the core logic in the Lambda handler.,Use Amazon Inspector for troubleshooting.,Use AWS Lambda Environment Variables to pass operational parameters to your function.,,,귀하가 근무하고 있는 회사는 최근 온프레미스의 모놀리식 애플리케이션을 Lambda 애플리케이션으로 마이그레이션하고 변환하기로 결정했습니다. 애플리케이션이 AWS에서 효과적으로 작동하는지 확인하는 것은 귀하의 책임입니다.다음 중 Lambda 함수 개발의 모범 사례는 무엇입니까? (2개를 선택하세요.),재귀 코드를 사용하십시오.,실행 컨텍스트 재사용을 활용하여 함수 성능을 향상하세요.,Lambda 핸들러에 핵심 논리를 포함합니다.,문제 해결을 위해 Amazon Inspector를 사용하십시오.,AWS Lambda 환경 변수를 사용하여 작동 매개변수를 함수에 전달합니다.,,0,,
udemy,DVA-02,295,A startup has recently launched a high-quality photo sharing portal using Amazon Lightsail and S3. They noticed that there are other external websites which are linking and using their photos without permission. This has caused an increase on their data transfer cost and potential revenue loss. Which of the following is the MOST effective method to solve this issue?,B,B,Enable cross-origin resource sharing (CORS) which allows cross-origin GET requests from all origins.,Configure the S3 bucket to remove public read access and use pre-signed URLs with expiry dates.,Use a CloudFront web distribution to serve the photos.,Block the IP addresses of the offending websites using Network Access Control List.,,,,한 스타트업은 최근 Amazon Lightsail 및 S3를 사용하여 고품질 사진 공유 포털을 출시했습니다. 그들은 다른 외부 웹사이트에서도 자신의 사진을 무단으로 링크해 사용하고 있다는 사실을 발견했습니다. 이로 인해 데이터 전송 비용이 증가하고 잠재적인 수익 손실이 발생했습니다.다음 중 이 문제를 해결하는 가장 효과적인 방법은 무엇입니까?,모든 원본의 원본 간 GET 요청을 허용하는 CORS(교차 원본 리소스 공유)를 활성화합니다.,공개 읽기 액세스를 제거하고 만료 날짜가 있는 미리 서명된 URL을 사용하도록 S3 버킷을 구성합니다.,CloudFront 웹 배포를 사용하여 사진을 제공합니다.,네트워크 액세스 제어 목록을 사용하여 문제가 되는 웹사이트의 IP 주소를 차단하세요.,,,0,,
udemy,DVA-02,296,"Your new web app is running on several on-demand EC2 instances behind a Classic Load Balancer. One of your EC2 instances has failed the health check and is no longer receiving traffic. After manually rebooting the instance, the application check becomes healthy again. Which of the following steps will you do next?",B,B,Enable Sticky Session on the ELB to rebalance traffic.,"Do nothing, the ELB will direct traffic to it after the health check threshold is passed.",Restart the classic load balancer to refresh the traffic flow.,You need to update the health check again to re-scan and see which instances are healthy.,,,,새 웹 앱은 Classic Load Balancer 뒤의 여러 온디맨드 EC2 인스턴스에서 실행되고 있습니다. EC2 인스턴스 중 하나가 상태 확인에 실패하여 더 이상 트래픽을 수신하지 않습니다. 인스턴스를 수동으로 재부팅한 후 애플리케이션 검사가 다시 정상 상태가 됩니다.다음 중 어떤 단계를 수행하시겠습니까?,트래픽 재조정을 위해 ELB에서 고정 세션을 활성화합니다.,아무것도 하지 마십시오. 상태 확인 임계값이 통과된 후 ELB가 트래픽을 ELB로 전달합니다.,트래픽 흐름을 새로 고치려면 클래식 로드 밸런서를 다시 시작하십시오.,다시 검사하여 어떤 인스턴스가 정상인지 확인하려면 상태 확인을 다시 업데이트해야 합니다.,,,0,,
udemy,DVA-02,297,A developer is launching a Lambda function which requires access to a MySQL RDS instance that is in a private subnet. Which of the following is the MOST secure way to achieve this?,C,C,Ensure that the Lambda function has a proper IAM permission to access RDS.,Expose an endpoint of your RDS to the Internet using an Elastic IP.,Configure the Lambda function to connect to your VPC.,Move your RDS instance to a public subnet.,,,,개발자가 프라이빗 서브넷에 있는 MySQL RDS 인스턴스에 액세스해야 하는 Lambda 함수를 시작하고 있습니다. 다음 중 이를 달성하는 가장 안전한 방법은 무엇입니까?,Lambda 함수에 RDS에 액세스할 수 있는 적절한 IAM 권한이 있는지 확인하십시오.,탄력적 IP를 사용하여 RDS의 엔드포인트를 인터넷에 노출합니다.,VPC에 연결하도록 Lambda 함수를 구성합니다.,RDS 인스턴스를 퍼블릭 서브넷으로 이동합니다.,,,0,,
udemy,DVA-02,298,"You have created an SWF workflow to coordinate the tasks of your media processing cluster, which processes the videos, and a separate media publishing cluster, which publishes the processed videos. Since the media processing cluster converts a single video multiple times, you need to record how many times a video is converted before another action is executed. Which of the following SWF options can be used to record such events?",B,B,Signals,Markers,Tags,Timers,,,,비디오를 처리하는 미디어 처리 클러스터와 처리된 비디오를 게시하는 별도의 미디어 게시 클러스터의 작업을 조정하기 위해 SWF 워크플로를 만들었습니다. 미디어 처리 클러스터는 단일 비디오를 여러 번 변환하므로 다른 작업이 실행되기 전에 비디오가 몇 번 변환되었는지 기록해야 합니다.다음 중 이러한 이벤트를 기록하는 데 사용할 수 있는 SWF 옵션은 무엇입니까?,신호,마커,태그,타이머,,,0,,
udemy,DVA-02,299,"A software engineer is building a serverless application in AWS consisting of Lambda, API Gateway, and DynamoDB. She needs to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML to determine the caller's identity. Which of the features of API Gateway is the MOST suitable one that she should use to build this feature?",D,D,Cross-Origin Resource Sharing (CORS),Cross-Account Lambda Authorizer,Resource Policy,Lambda Authorizers,,,,"소프트웨어 엔지니어가 AWS에서 Lambda, API Gateway 및 DynamoDB로 구성된 서버리스 애플리케이션을 구축하고 있습니다. 그녀는 호출자의 신원을 확인하기 위해 OAuth 또는 SAML과 같은 전달자 토큰 인증 전략을 사용하는 사용자 정의 인증 체계를 구현해야 합니다.API 게이트웨이의 기능 중 이 기능을 구축하는 데 사용해야 하는 가장 적합한 기능은 무엇입니까?",CORS(교차 원본 리소스 공유),교차 계정 Lambda 권한 부여자,자원 정책,Lambda 승인자,,,0,,
udemy,DVA-02,300,"A developer has recently deployed an application, which is hosted in an Auto Scaling group of EC2 instances and processes data from an Amazon Kinesis Data Stream. Each of the EC2 instances has exactly one KCL worker processing one Kinesis data stream which has 10 shards. Due to performance issues, the systems operations team has resharded the data stream to increase the number of open shards to 20.What is the maximum number of running EC2 instances that should ideally be kept to maintain application performance?",A,A,20,30,10,40,,,,개발자는 최근 EC2 인스턴스의 Auto Scaling 그룹에서 호스팅되고 Amazon Kinesis Data Stream의 데이터를 처리하는 애플리케이션을 배포했습니다. 각 EC2 인스턴스에는 10개의 샤드가 있는 하나의 Kinesis 데이터 스트림을 처리하는 정확히 한 명의 KCL 작업자가 있습니다. 성능 문제로 인해 시스템 운영 팀은 데이터 스트림을 다시 샤딩하여 공개 샤드 수를 20개로 늘렸습니다.애플리케이션 성능을 유지하기 위해 이상적으로 유지해야 하는 실행 중인 EC2 인스턴스의 최대 수는 몇 개입니까?,20,30,10,40,,,0,,
udemy,DVA-02,301,A developer must set up a caching layer in front of the tutorialsdojo database. The developer should come up with a function that ensures cached data is always up-to-date. Stale records in the cache must be automatically deleted as well to prevent the build-up of extra data.Which pseudocode best represents this caching strategy?,A,A,"save_item(item_id, item_value):    ttl = 500    tutorialsdojo.query(""UPDATE Customers WHERE id = {0}"", item_id, item_value)    cache.set(item_id, item_value, ttl)    return 'ok'","save_item(item_id, item_value):    ttl = 500    tutorialsdojo.query(""SELECT Customers WHERE id = {0}"", item_id, item_value)    cache.set(item_id, item_value, 500)    return 'ok'","save_item(item_id, item_value):    ttl = 500    cache.set(item_id, item_value, 500)    return 'ok'","save_item(item_id, item_value):    tutorialsdojo.query(""UPDATE Customers SET value = ? WHERE id = ?"", item_value, item_id)    cache.delete(item_id)    return 'ok'",,,,개발자는 데이터베이스 앞에 캐싱 계층을 설정해야 합니다 tutorialsdojo. 개발자는 캐시된 데이터가 항상 최신 상태인지 확인하는 기능을 마련해야 합니다. 캐시에 있는 오래된 레코드도 자동으로 삭제되어 추가 데이터가 쌓이는 것을 방지해야 합니다.이 캐싱 전략을 가장 잘 나타내는 의사 코드는 무엇입니까?,"저장_항목 ( 항목_ID , 항목_값 ):    TTL = 500     튜토리얼도조 . 쿼리 ( ""id = {0}인 곳에서 고객 업데이트"" , item_id , item_value )    캐시 . 세트 ( 항목_ID , 항목_값 , ttl )    '알았어' 로 돌아가","저장_항목 ( 항목_ID , 항목_값 ):    TTL = 500     튜토리얼도조 . 쿼리 ( ""ID = {0}인 곳에서 고객 선택"" , item_id , item_value )    캐시 . 세트 ( item_id , item_value , 500 )     '알았어' 로 돌아가","저장_항목 ( 항목_ID , 항목_값 ):    TTL = 500     캐시 . 세트 ( item_id , item_value , 500 )     '알았어' 로 돌아가","저장_항목 ( 항목_ID , 항목_값 ):    튜토리얼도조 . 쿼리 ( ""고객 설정 값 업데이트 = ? WHERE id = ?"" , item_value , item_id )    캐시 . 삭제 ( item_id )    '알았어' 로 돌아가",,,0,,
udemy,DVA-02,302,"In the next financial year, a company has decided to develop a completely new version of its legacy application that will utilize Node.js and GraphQL. The new architecture aims to offer an end-to-end view of requests as they traverse the application and display a map of the underlying components.To achieve this, the application will be hosted in an Auto Scaling group (ASG) of Linux EC2 instances behind an Application Load Balancer (ALB) and must be instrumented to send trace data to the AWS X-Ray.Which of the following options is the MOST suitable way to satisfy this requirement?",C,C,Refactor your application to send segment documents directly to X-Ray by using the PutTraceSegments API.,Enable AWS X-Ray tracing on the ASG’s launch template.,Use a user data script to install the X-Ray daemon.,Enable AWS Web Application Firewall (WAF) on the ALB to monitor web requests.,,,,다음 회계연도에 한 회사는 Node.js와 GraphQL을 활용하는 완전히 새로운 버전의 레거시 애플리케이션을 개발하기로 결정했습니다. 새로운 아키텍처는 요청이 애플리케이션을 통과하고 기본 구성 요소의 맵을 표시할 때 요청에 대한 엔드투엔드 보기를 제공하는 것을 목표로 합니다.이를 달성하기 위해 애플리케이션은 ALB(Application Load Balancer) 뒤에 있는 Linux EC2 인스턴스의 Auto Scaling 그룹(ASG)에서 호스팅되며 추적 데이터를 AWS X-Ray로 보내도록 계측되어야 합니다.다음 옵션 중 이 요구 사항을 충족하는 데 가장 적합한 방법은 무엇입니까?,API 를 사용하여 세그먼트 문서를 X-Ray로 직접 보내도록 애플리케이션을 리팩터링하세요 PutTraceSegments.,ASG의 시작 템플릿에서 AWS X-Ray 추적을 활성화합니다.,사용자 데이터 스크립트를 사용하여 X-Ray 데몬을 설치합니다.,ALB에서 AWS 웹 애플리케이션 방화벽(WAF)을 활성화하여 웹 요청을 모니터링합니다.,,,0,,
udemy,DVA-02,303,"You are a newly hired developer in a leading investment bank which uses AWS as its cloud infrastructure. One of your tasks is to develop an application that will store financial data to an already existing S3 bucket, which has the following bucket policy:{ ""Version"": ""2012-10-17"", ""Id"": ""PutObjPolicy"", ""Statement"": [  {   ""Sid"": ""AllowUploadCheck"",   ""Effect"": ""Deny"",   ""Principal"": ""*"",   ""Action"": ""s3:PutObject"",   ""Resource"": ""arn:aws:s3:::tutorialsdojo/*"",   ""Condition"": {    ""StringNotEquals"": {     ""s3:x-amz-server-side-encryption"": ""AES256""    }  }}, {  ""Sid"": ""AllowNullCheck"",  ""Effect"": ""Deny"",  ""Principal"": ""*"",  ""Action"": ""s3:PutObject"",  ""Resource"": ""arn:aws:s3:::tutorialsdojo/*"",  ""Condition"": {   ""Null"": {    ""s3:x-amz-server-side-encryption"": ""true""    }   }  } ]}Which of the following statements is true about uploading data to this S3 bucket?",C,C,The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of true.,The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of Null.,The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of AES256.,The bucket will deny object uploads unless the request includes the x-amz-server-side-encryption header with a value of aws:kms.,,,,"당신은 AWS를 클라우드 인프라로 사용하는 선도적인 투자 은행에 새로 채용된 개발자입니다. 작업 중 하나는 다음과 같은 버킷 정책이 있는 기존 S3 버킷에 금융 데이터를 저장할 애플리케이션을 개발하는 것입니다.{ ""버전"" : ""2012-10-17"" ,  ""ID"" : ""PutObjPolicy"" ,  ""진술"" : [   {   ""시드"" : ""AllowUploadCheck"" ,    ""효과"" : ""거부"" ,    ""주체"" : ""*"" ,    ""작업"" : ""s3:PutObject"" ,    ""리소스"" : ""arn:aws:s3:::tutorialsdojo/*"" ,    ""조건"" : {     ""StringNotEquals"" : {      ""s3:x-amz-서버측 암호화"" : ""AES256""     }  }}, {  ""시드"" : ""AllowNullCheck"" ,   ""효과"" : ""거부"" ,   ""주체"" : ""*"" ,   ""작업"" : ""s3:PutObject"" ,   ""리소스"" : ""arn:aws:s3:::tutorialsdojo/*"" ,   ""조건"" : {    ""널"" : {     ""s3:x-amz-server-side-encryption"" : ""true""     }   }  } ]}다음 중 이 S3 버킷에 데이터를 업로드하는 것과 관련하여 올바른 설명은 무엇입니까?",요청에 x-amz-server-side-encryption값이 있는 헤더가 포함되어 있지 않으면 버킷은 객체 업로드를 거부합니다 true.,요청에 x-amz-server-side-encryption값이 있는 헤더가 포함되어 있지 않으면 버킷은 객체 업로드를 거부합니다 Null.,요청에 x-amz-server-side-encryption값이 있는 헤더가 포함되어 있지 않으면 버킷은 객체 업로드를 거부합니다 AES256.,요청에 x-amz-server-side-encryption값이 있는 헤더가 포함되어 있지 않으면 버킷은 객체 업로드를 거부합니다 aws:kms.,,,0,,
udemy,DVA-02,304,"You are a software developer for a multinational investment bank which has a hybrid cloud architecture with AWS. To improve the security of their applications, they decided to use AWS Key Management Service (KMS) to create and manage their encryption keys across a wide range of AWS services. You were given the responsibility to integrate AWS KMS with the financial applications of the company. Which of the following are the recommended steps to locally encrypt data using AWS KMS that you should follow? (Select TWO.)",BE,BE,Use the GenerateDataKeyWithoutPlaintext operation to get a data encryption key then use the plaintext data key in the response to encrypt data locally.,Erase the plaintext data key from memory and store the encrypted data key alongside the locally encrypted data.,Erase the encrypted data key from memory and store the plaintext data key alongside the locally encrypted data.,Encrypt data locally using the Encrypt operation.,Use the GenerateDataKey operation to get a data encryption key then use the plaintext data key in the response to encrypt data locally.,,,귀하는 AWS를 사용한 하이브리드 클라우드 아키텍처를 갖춘 다국적 투자 은행의 소프트웨어 개발자입니다. 애플리케이션의 보안을 강화하기 위해 그들은 AWS Key Management Service(KMS)를 사용하여 광범위한 AWS 서비스에 걸쳐 암호화 키를 생성하고 관리하기로 결정했습니다. 귀하에게는 AWS KMS를 회사의 금융 애플리케이션과 통합할 책임이 주어졌습니다.다음 중 AWS KMS를 사용하여 데이터를 로컬로 암호화하기 위해 따라야 하는 권장 단계는 무엇입니까? (2개를 선택하세요.),작업을 사용하여 GenerateDataKeyWithoutPlaintext데이터 암호화 키를 얻은 다음 응답으로 일반 텍스트 데이터 키를 사용하여 데이터를 로컬로 암호화합니다.,일반 텍스트 데이터 키를 메모리에서 지우고 암호화된 데이터 키를 로컬로 암호화된 데이터와 함께 저장합니다.,암호화된 데이터 키를 메모리에서 지우고 일반 텍스트 데이터 키를 로컬로 암호화된 데이터와 함께 저장합니다.,작업을 사용하여 로컬로 데이터를 암호화합니다 Encrypt.,작업을 사용하여 GenerateDataKey데이터 암호화 키를 얻은 다음 응답으로 일반 텍스트 데이터 키를 사용하여 데이터를 로컬로 암호화합니다.,,0,,
udemy,DVA-02,305,"A developer wants to perform additional processing on newly inserted items in Amazon DynamoDB using AWS Lambda. In order to implement this requirement, the developer will have to use DynamoDB Streams to automatically send the new items in the table to a Lambda function for processing. Given the scenario, what steps should be performed by the developer to integrate his/her DynamoDB to his/her Lambda functions? (Select TWO.)",AE,AE,Select AWSLambdaDynamoDBExecutionRole managed policy as the function's execution role.,Create a trigger for a Kinesis Data Firehose delivery stream that uses a Lambda function for data processing.,Create an SNS topic to capture new records from DynamoDB.,Select AWSLambdaBasicExecutionRole managed policy as the function's execution role.,Create an event source mapping in Lambda to send records from your stream to a Lambda function.,,,개발자는 AWS Lambda를 사용하여 Amazon DynamoDB에 새로 삽입된 항목에 대해 추가 처리를 수행하려고 합니다. 이 요구 사항을 구현하려면 개발자는 DynamoDB Streams를 사용하여 테이블의 새 항목을 Lambda 함수로 자동 전송하여 처리해야 합니다.주어진 시나리오에서 개발자는 DynamoDB를 Lambda 함수에 통합하기 위해 어떤 단계를 수행해야 합니까? (2개를 선택하세요.),AWSLambdaDynamoDBExecutionRole함수의 실행 역할로 관리형 정책을 선택합니다 .,데이터 처리를 위해 Lambda 함수를 사용하는 Kinesis Data Firehose 전송 스트림에 대한 트리거를 생성합니다.,DynamoDB에서 새 레코드를 캡처하려면 SNS 주제를 생성하세요.,AWSLambdaBasicExecutionRole함수의 실행 역할로 관리형 정책을 선택합니다 .,Lambda에서 이벤트 소스 매핑을 생성하여 스트림의 레코드를 Lambda 함수로 보냅니다.,,0,,
udemy,DVA-02,306,A company is currently in the process of integrating their on-premises data center to their cloud infrastructure in AWS. One of the requirements is to integrate the on-premises Lightweight Directory Access Protocol (LDAP) directory service to their AWS VPC using IAM. Which of the following provides the MOST suitable solution to implement if the identity store that they are using is not compatible with SAML?,D,D,Implement the AWS Single Sign-On (SSO) service to enable single sign-on between AWS and your LDAP.,Create IAM roles to rotate the IAM credentials whenever LDAP credentials are updated.,Set up an IAM policy that references the LDAP identifiers and AWS credentials.,Create a custom identity broker application in your on-premises data center and use STS to issue short-lived AWS credentials.,,,,한 회사는 현재 온프레미스 데이터 센터를 AWS의 클라우드 인프라에 통합하는 과정을 진행 중입니다. 요구 사항 중 하나는 IAM을 사용하여 온프레미스 LDAP(Lightweight Directory Access Protocol) 디렉터리 서비스를 AWS VPC에 통합하는 것입니다.다음 중 사용 중인 ID 저장소가 SAML과 호환되지 않는 경우 구현하기에 가장 적합한 솔루션을 제공하는 것은 무엇입니까?,AWS Single Sign-On(SSO) 서비스를 구현하여 AWS와 LDAP 간의 Single Sign-On을 활성화합니다.,LDAP 자격 증명이 업데이트될 때마다 IAM 자격 증명을 교체하는 IAM 역할을 생성합니다.,LDAP 식별자와 AWS 자격 증명을 참조하는 IAM 정책을 설정합니다.,온프레미스 데이터 센터에서 사용자 지정 자격 증명 브로커 애플리케이션을 생성하고 STS를 사용하여 단기 AWS 자격 증명을 발급합니다.,,,0,,
udemy,DVA-02,307,"An aerospace engineering company has recently migrated to AWS for their cloud architecture. They are using CloudFormation and AWS SAM as deployment services for both of their monolithic and serverless applications. There is a new requirement where you have to dynamically install packages, create files, and start services on your EC2 instances upon the deployment of the application stack using CloudFormation. Which of the following helper scripts should you use in this scenario?",C,C,cfn-hup,cfn-signal,cfn-init,cfn-get-metadata,,,,"한 항공우주 엔지니어링 회사는 최근 클라우드 아키텍처를 위해 AWS로 마이그레이션했습니다. 그들은 모놀리식 애플리케이션과 서버리스 애플리케이션 모두에 대한 배포 서비스로 CloudFormation과 AWS SAM을 사용하고 있습니다. CloudFormation을 사용하여 애플리케이션 스택을 배포할 때 EC2 인스턴스에서 동적으로 패키지를 설치하고, 파일을 생성하고, 서비스를 시작해야 하는 새로운 요구 사항이 있습니다.이 시나리오에서는 다음 도우미 스크립트 중 어떤 것을 사용해야 합니까?",cfn-hup,cfn-신호,cfn-init,cfn - 가져오기 - 메타데이터,,,0,,
udemy,DVA-02,308,"You currently have an IAM user for working in the development environment using shell scripts that call the AWS CLI. The EC2 instance that you are using already contains the access key credential set and an IAM role, which are used to run the CLI and access the development environment. You were given a new set of access key credentials with another IAM role that allows you to access and manage the production environment. Which of the following is the EASIEST way to switch from one role to another?",A,A,"Create a new profile for the role in the AWS CLI configuration file then append the --profile parameter, along with the new profile name, whenever you run the CLI command.",Store the production access key credentials set in the instance metadata and call this whenever you need to access the production environment.,"Create a new instance profile in the AWS CLI configuration file then append the --profile parameter, along with the new profile name, whenever you run the CLI command.",Store the production access key credentials set in the user data of the instance and call this whenever you need to access the production environment.,,,,현재 AWS CLI를 호출하는 셸 스크립트를 사용하여 개발 환경에서 작업하는 IAM 사용자가 있습니다. 사용 중인 EC2 인스턴스에는 CLI를 실행하고 개발 환경에 액세스하는 데 사용되는 액세스 키 자격 증명 세트와 IAM 역할이 이미 포함되어 있습니다. 프로덕션 환경에 액세스하고 관리할 수 있는 다른 IAM 역할과 함께 새로운 액세스 키 자격 증명 세트가 제공되었습니다.다음 중 한 역할에서 다른 역할로 전환하는 가장 쉬운 방법은 무엇입니까?,AWS CLI 구성 파일에서 역할에 대한 새 프로필을 생성한 다음 --profileCLI 명령을 실행할 때마다 새 프로필 이름과 함께 파라미터를 추가합니다.,인스턴스 메타데이터에 설정된 프로덕션 액세스 키 자격 증명을 저장하고 프로덕션 환경에 액세스해야 할 때마다 이를 호출합니다.,AWS CLI 구성 파일에서 새 인스턴스 프로필을 생성한 다음 --profileCLI 명령을 실행할 때마다 새 프로필 이름과 함께 파라미터를 추가합니다.,인스턴스의 사용자 데이터에 설정된 프로덕션 액세스 키 자격 증명을 저장하고 프로덕션 환경에 액세스해야 할 때마다 이를 호출합니다.,,,0,,
udemy,DVA-02,309,"A developer is utilizing AWS X-Ray to generate a visual representation of the requests flowing through their enterprise web application. Since the application interacts with multiple services, all requests must be traced in X-Ray, including any downstream calls made to AWS resources.Which of the following actions should the developer implement for this scenario?",A,A,"Use X-Ray SDK to generate segment documents with subsegments and send them to the X-Ray daemon, which will buffer them and upload to the X-Ray API in batches.",Install AWS X-Ray on the different services that communicate with the application including the AWS resources that the application calls.,Use AWS X-Ray SDK to upload a trace segment by executing PutTraceSegments API.,Pass multiple trace segments as a parameter of PutTraceSegments API.,,,,개발자는 AWS X-Ray를 활용하여 엔터프라이즈 웹 애플리케이션을 통해 흐르는 요청을 시각적으로 표현하고 있습니다. 애플리케이션은 여러 서비스와 상호 작용하므로 AWS 리소스에 대한 다운스트림 호출을 포함하여 모든 요청을 X-Ray에서 추적해야 합니다.이 시나리오에서 개발자는 다음 중 어떤 작업을 구현해야 합니까?,X-Ray SDK를 사용하여 하위 세그먼트가 포함된 세그먼트 문서를 생성하고 X-Ray 데몬으로 보내면 X-Ray 데몬이 이를 버퍼링하고 X-Ray API에 일괄 업로드합니다.,애플리케이션이 호출하는 AWS 리소스를 포함하여 애플리케이션과 통신하는 다양한 서비스에 AWS X-Ray를 설치합니다.,API 를 실행하여 추적 세그먼트를 업로드하려면 AWS X-Ray SDK를 사용하십시오 PutTraceSegments.,여러 추적 세그먼트를 API의 매개변수로 전달합니다 PutTraceSegments.,,,0,,
udemy,DVA-02,310,"A web application is currently using an on-premises Microsoft SQL Server 2019 Enterprise Edition database. Your manager instructed you to migrate the application to Elastic Beanstalk and the database to RDS. For additional security, you must configure your database to automatically encrypt data before it is written to storage, and automatically decrypt data when the data is read from storage.Which of the following services will you use to achieve this?",B,B,Use Microsoft SQL Server Windows Authentication.,Enable Transparent Data Encryption (TDE).,Enable RDS Encryption.,Use IAM DB Authentication.,,,,"웹 애플리케이션은 현재 온프레미스 Microsoft SQL Server 2019 Enterprise Edition 데이터베이스를 사용하고 있습니다. 관리자는 애플리케이션을 Elastic Beanstalk로, 데이터베이스를 RDS로 마이그레이션하라고 지시했습니다. 보안을 강화하려면 데이터를 스토리지에 쓰기 전에 자동으로 암호화하고 스토리지에서 데이터를 읽을 때 데이터를 자동으로 해독하도록 데이터베이스를 구성해야 합니다.이를 달성하기 위해 다음 중 어떤 서비스를 사용하시겠습니까?",Microsoft SQL Server Windows 인증을 사용합니다.,TDE(투명한 데이터 암호화)를 활성화합니다.,RDS 암호화를 활성화합니다.,IAM DB 인증을 사용합니다.,,,0,,
udemy,DVA-02,311,"The infrastructure of an application is designed such that a producer sends data to a consumer via HTTPS. The consumer may sometimes take a while to process the messages, which can result in unexpected timeouts and cause newer messages not to be acknowledged immediately. To resolve this issue, a developer decided to introduce an Amazon SQS standard queue into the system. However, duplicate messages are still not being handled properly.What should the developer do to ensure that messages are durably delivered and to prevent duplicate messages? (Select TWO.)",BC,BC,Use a delay queue.,Create a FIFO queue as a replacement for the standard queue.,Configure the producer to set deduplication IDs for the messages.,Increase the timeout for the acknowledgement response.,Increase the number of consumers polling from your standard queue.,,,애플리케이션의 인프라는 생산자가 HTTPS를 통해 소비자에게 데이터를 보내도록 설계되었습니다. 소비자가 메시지를 처리하는 데 시간이 걸릴 수 있으며 이로 인해 예기치 않은 시간 초과가 발생하고 새 메시지가 즉시 승인되지 않을 수 있습니다. 이 문제를 해결하기 위해 개발자는 Amazon SQS 표준 대기열을 시스템에 도입하기로 결정했습니다. 그러나 중복된 메시지는 여전히 제대로 처리되지 않고 있습니다.메시지가 지속적으로 전달되도록 하고 중복 메시지를 방지하려면 개발자는 무엇을 해야 합니까? (2개를 선택하세요.),지연 대기열을 사용합니다.,표준 대기열 대신 FIFO 대기열을 생성합니다.,메시지에 대한 중복 제거 ID를 설정하도록 생산자를 구성합니다.,승인 응답에 대한 시간 제한을 늘립니다.,표준 대기열에서 폴링하는 소비자 수를 늘립니다.,,0,,
udemy,DVA-02,312,"You were recently hired by a media company that is planning to build a news portal using Elastic Beanstalk and DynamoDB database, which already contains a few data. There is already an existing DynamoDB Table that has an attribute of ArticleName which acts as the partition key and a Category attribute as its sort key. You are instructed to develop a feature that will query the ArticleName attribute but will use a different sort key other than the existing one. The feature also requires strong read consistency to fetch the most up-to-date data.Which of the following solutions should you implement?",A,A,Create a new DynamoDB table with a Local Secondary Index that uses the ArticleName attribute with a different sort key. Migrate the data from the existing table to the new table.,Create a Local Secondary Index that uses the ArticleName attribute and a different sort key.,Create a Global Secondary Index that uses the ArticleName attribute and a different sort key.,Create a Global Secondary Index which uses the ArticleName attribute and your alternative sort key as projected attributes.,,,,귀하는 이미 몇 가지 데이터가 포함된 Elastic Beanstalk 및 DynamoDB 데이터베이스를 사용하여 뉴스 포털을 구축하려는 미디어 회사에 최근 고용되었습니다. ArticleName파티션 키 역할을 하는 속성과 Category정렬 키 역할을 하는 속성을 가진 기존 DynamoDB 테이블이 이미 있습니다 . ArticleName속성을 쿼리하지만 기존 정렬 키와 다른 정렬 키를 사용하는 기능을 개발하라는 지시를 받았습니다 . 또한 이 기능은 최신 데이터를 가져오기 위해 강력한 읽기 일관성이 필요합니다.다음 중 어떤 솔루션을 구현해야 합니까?,ArticleName다른 정렬 키가 있는 속성을 사용하는 로컬 보조 인덱스가 있는 새 DynamoDB 테이블을 생성합니다 . 기존 테이블의 데이터를 새 테이블로 마이그레이션합니다.,ArticleName해당 속성과 다른 정렬 키를 사용하는 로컬 보조 인덱스를 생성합니다 .,ArticleName속성과 다른 정렬 키를 사용하는 글로벌 보조 인덱스를 생성합니다 .,ArticleName속성과 대체 정렬 키를 예상 속성으로 사용하는 글로벌 보조 인덱스를 생성합니다 .,,,0,,
udemy,DVA-02,313,Your request to increase your account's concurrent execution limit to 2000 has been recently approved by AWS. There are 10 Lambda functions running in your account and you already specified a concurrency execution limit on one function at 400 and on another function at 200. Which of the following statements are TRUE in this scenario? (Select TWO.),BC,BC,The unreserved concurrency pool is 600.,The remaining 1400 concurrent executions will be shared among the other 8 functions.,You can still set a concurrency execution limit of 1300 to a third Lambda function.,The combined allocated 600 concurrent execution will be shared among the 2 functions.,You can still set a concurrency execution limit of 1400 to a third Lambda function.,,,계정의 동시 실행 한도를 2000으로 늘리려는 귀하의 요청이 최근 AWS에서 승인되었습니다. 계정에서 10개의 Lambda 함수가 실행 중이고 이미 한 함수에 대해 동시 실행 제한을 400으로 지정했고 다른 함수에는 200으로 동시 실행 제한을 지정했습니다.이 시나리오에서 다음 설명 중 참인 것은 무엇입니까? (2개를 선택하세요.),예약되지 않은 동시성 풀은 600입니다.,나머지 1400개의 동시 실행은 다른 8개 기능에서 공유됩니다.,세 번째 Lambda 함수에 동시 실행 제한을 1300으로 설정할 수 있습니다.,결합된 할당된 600개의 동시 실행은 두 기능 간에 공유됩니다.,세 번째 Lambda 함수에 동시 실행 제한을 1400으로 설정할 수 있습니다.,,0,,
udemy,DVA-02,314,A company has an application hosted in an On-Demand EC2 instance in your VPC. The developer has been instructed to create a shell script that fetches the instance's associated public and private IP addresses. What should the developer do to complete this task?,B,B,Get the public and private IP addresses from the instance user data service using the http://169.254.169.254/latest/userdata/ endpoint.,Get the public and private IP addresses from the instance metadata service using the http://169.254.169.254/latest/meta-data/ endpoint.,Get the public and private IP addresses from Amazon CloudWatch.,Get the public and private IP addresses from AWS CloudTrail.,,,,회사에는 VPC의 온디맨드 EC2 인스턴스에 호스팅되는 애플리케이션이 있습니다. 개발자는 인스턴스의 연결된 공용 및 개인 IP 주소를 가져오는 셸 스크립트를 생성하라는 지시를 받았습니다.이 작업을 완료하려면 개발자가 무엇을 해야 합니까?,엔드포인트 를 사용하여 인스턴스 사용자 데이터 서비스에서 퍼블릭 및 프라이빗 IP 주소를 가져옵니다 http://169.254.169.254/latest/userdata/.,엔드포인트 를 사용하여 인스턴스 메타데이터 서비스에서 퍼블릭 및 프라이빗 IP 주소를 가져옵니다 http://169.254.169.254/latest/meta-data/.,Amazon CloudWatch에서 퍼블릭 및 프라이빗 IP 주소를 가져옵니다.,AWS CloudTrail에서 퍼블릭 및 프라이빗 IP 주소를 가져옵니다.,,,0,,
udemy,DVA-02,315,"You are working for a software development company that uses AWS CodePipeline as their CI/CD platform to build, test, and push their deployments to their production environment. You have created a Lambda function that will push the build details on a separate DynamoDB table. You want to trigger this function after a successful build on your Pipeline.Which of the following services will help you achieve this?",A,A,Amazon EventBridge,AWS OpsWorks,AWS CodeBuild,AWS CloudTrail Events,,,,"귀하는 AWS CodePipeline을 CI/CD 플랫폼으로 사용하여 배포를 구축, 테스트하고 프로덕션 환경에 푸시하는 소프트웨어 개발 회사에서 일하고 있습니다. 별도의 DynamoDB 테이블에 빌드 세부 정보를 푸시하는 Lambda 함수를 생성했습니다. 파이프라인에서 성공적인 빌드 후에 이 함수를 트리거하려고 합니다.다음 중 어떤 서비스가 이를 달성하는 데 도움이 됩니까?",아마존 이벤트브리지,AWS OpsWorks,AWS 코드빌드,AWS CloudTrail 이벤트,,,0,,
udemy,DVA-02,316,"A company has an application hosted in an ECS Cluster that heavily uses an RDS database. A developer needs to closely monitor how the different processes on a DB instance use the CPU, such as the percentage of the CPU bandwidth or the total memory consumed by each process to ensure application performance.Which of the following is the MOST suitable solution that the developer should implement?",D,D,Use CloudWatch to track the CPU Utilization of your database.,Develop a shell script that collects and publishes custom metrics to CloudWatch which tracks the real-time CPU Utilization of the RDS instance.,Track the CPU% and MEM% metrics which are readily available in the Amazon RDS console.,Use Enhanced Monitoring in RDS.,,,,한 회사에 RDS 데이터베이스를 많이 사용하는 ECS 클러스터에 호스팅되는 애플리케이션이 있습니다. 개발자는 애플리케이션 성능을 보장하기 위해 CPU 대역폭 비율이나 각 프로세스에서 소비하는 총 메모리 등 DB 인스턴스의 다양한 프로세스가 CPU를 사용하는 방식을 면밀히 모니터링해야 합니다.다음 중 개발자가 구현해야 하는 가장 적합한 솔루션은 무엇입니까?,CloudWatch를 사용하여 데이터베이스의 CPU 사용률을 추적하십시오.,RDS 인스턴스의 실시간 CPU 사용률을 추적하는 CloudWatch에 사용자 지정 지표를 수집하고 게시하는 셸 스크립트를 개발합니다.,Amazon RDS 콘솔에서 쉽게 사용할 수 있는 지표를 CPU%추적 합니다 .MEM%,RDS에서 향상된 모니터링을 사용하세요.,,,0,,
udemy,DVA-02,317,"A developer is planning to build a serverless Rust application in AWS using AWS Lambda and Amazon DynamoDB. Much to his disappointment, AWS Lambda does not natively support the Rust programming language. Can the developer still proceed with creating serverless Rust applications in AWS given the situation above?",B,B,No. The developer will have to wait for a new support release in AWS Lambda.,Yes. The developer can create a custom runtime for hist Rust applications and bootstrap it to an AWS Lambda function.,Yes. The developer will just have to use AWS Fargate instead of AWS Lambda.,Yes. The developer can submit a request ticket to AWS so that they can provide him a Lambda runtime environment that supports Rust.,,,,개발자는 AWS Lambda 및 Amazon DynamoDB를 사용하여 AWS에서 서버리스 Rust 애플리케이션을 구축할 계획입니다. 실망스럽게도 AWS Lambda는 기본적으로 Rust 프로그래밍 언어를 지원하지 않습니다.위의 상황에서 개발자가 AWS에서 서버리스 Rust 애플리케이션 생성을 계속 진행할 수 있습니까?,아니요. 개발자는 AWS Lambda의 새로운 지원 릴리스를 기다려야 합니다.,예. 개발자는 자신의 Rust 애플리케이션에 대한 사용자 지정 런타임을 생성하고 이를 AWS Lambda 함수로 부트스트랩할 수 있습니다.,예. 개발자는 AWS Lambda 대신 AWS Fargate를 사용하면 됩니다.,예. 개발자는 AWS에 요청 티켓을 제출하여 Rust를 지원하는 Lambda 런타임 환경을 제공할 수 있습니다.,,,0,,
udemy,DVA-02,318,"A developer is building an AI-based traffic monitoring application using Lambda in AWS. Due to the complexity of the application, the developer must do certain modifications such as the way Lambda runs the function's setup code and how the invocation events are read from the Lambda runtime API. In this scenario, which feature of Lambda should you take advantage of to meet the above requirement?",A,A,Custom Runtime,Layers,Lambda@Edge,DLQ,,,,"개발자는 AWS에서 Lambda를 사용하여 AI 기반 트래픽 모니터링 애플리케이션을 구축하고 있습니다. 애플리케이션의 복잡성으로 인해 개발자는 Lambda가 함수의 설정 코드를 실행하는 방식, Lambda 런타임 API에서 호출 이벤트를 읽는 방식 등 특정 수정을 수행해야 합니다.이 시나리오에서 위 요구 사항을 충족하려면 Lambda의 어떤 기능을 활용해야 합니까?",커스텀 런타임,레이어,람다@에지,DLQ,,,0,,
udemy,DVA-02,319,"You have two users concurrently accessing a DynamoDB table and submitting updates. If a user will modify a specific item in the table, she needs to make sure that the operation will not affect another user's attempt to modify the same item. You have to ensure that your update operations will only succeed if the item attributes meet one or more expected conditions. Which of the following DynamoDB features should you use in this scenario?",A,A,Conditional writes,Projection Expressions,Batch Operations,Update Expressions,,,,DynamoDB 테이블에 동시에 액세스하고 업데이트를 제출하는 두 명의 사용자가 있습니다. 사용자가 테이블의 특정 항목을 수정하는 경우 해당 작업이 동일한 항목을 수정하려는 다른 사용자의 시도에 영향을 미치지 않는지 확인해야 합니다. 항목 속성이 하나 이상의 예상 조건을 충족하는 경우에만 업데이트 작업이 성공하도록 해야 합니다.이 시나리오에서는 다음 DynamoDB 기능 중 어떤 기능을 사용해야 합니까?,조건부 쓰기,투영식,일괄 작업,표현식 업데이트,,,0,,
udemy,DVA-02,320,"You work for a software development company where the teams are divided into distinct projects. The management wants to have separation on their AWS resources, which will have a detailed report on the costs of each project. Which of the following options is the recommended way to implement this?",A,A,Create separate AWS accounts for each project and use consolidated billing.,Tag resources by projects and use Detailed Billing Reports to show costing per tag.,Tag resources by IAM group assigned for each project and use Detailed Billing reports to show costing.,Create separate AWS accounts for each project and generate Detailed Billing for each account.,,,,당신은 팀이 서로 다른 프로젝트로 나누어져 있는 소프트웨어 개발 회사에서 일하고 있습니다. 경영진은 AWS 리소스를 분리하여 각 프로젝트 비용에 대한 자세한 보고서를 원합니다.다음 옵션 중 이를 구현하는 데 권장되는 방법은 무엇입니까?,프로젝트마다 별도의 AWS 계정을 생성하고 통합 결제를 사용하세요.,프로젝트별로 리소스에 태그를 지정하고 세부 청구 보고서를 사용하여 태그당 비용을 표시합니다.,각 프로젝트에 할당된 IAM 그룹별로 리소스에 태그를 지정하고 세부 결제 보고서를 사용하여 비용을 표시합니다.,각 프로젝트에 대해 별도의 AWS 계정을 생성하고 각 계정에 대한 세부 결제를 생성합니다.,,,0,,
udemy,DVA-02,321,"A financial mobile application has a serverless backend API which consists of DynamoDB, Lambda, and Cognito. Due to the confidential financial transactions handled by the mobile application, there is a new requirement provided by the company to add a second authentication method that doesn't rely solely on user name and password. Which of the following is the MOST suitable solution that the developer should implement?",C,C,Use Cognito with SNS to allow additional authentication via SMS.,Create a custom application that integrates with Amazon Cognito which implements the second layer of authentication.,Integrate multi-factor authentication (MFA) to a user pool in Cognito to protect the identity of your users.,Use a new IAM policy to a user pool in Cognito.,,,,"금융 모바일 애플리케이션에는 DynamoDB, Lambda 및 Cognito로 구성된 서버리스 백엔드 API가 있습니다. 모바일 애플리케이션에서 처리되는 기밀 금융 거래로 인해 회사에서는 사용자 이름과 비밀번호에만 의존하지 않는 두 번째 인증 방법을 추가하라는 새로운 요구 사항을 제공합니다.다음 중 개발자가 구현해야 하는 가장 적합한 솔루션은 무엇입니까?",SMS를 통한 추가 인증을 허용하려면 SNS와 함께 Cognito를 사용하세요.,두 번째 인증 계층을 구현하는 Amazon Cognito와 통합되는 사용자 지정 애플리케이션을 생성합니다.,Cognito의 사용자 풀에 다단계 인증(MFA)을 통합하여 사용자의 신원을 보호하세요.,Cognito의 사용자 풀에 새로운 IAM 정책을 사용합니다.,,,0,,
udemy,DVA-02,322,You are developing an online game where the app preferences and game state of the player must be synchronized across devices. It should also allow multiple users to synchronize and collaborate shared data in real time. Which of the following is the MOST appropriate solution that you should implement in this scenario?,A,A,Integrate AWS AppSync to your mobile app.,Integrate Amazon Cognito Sync to your mobile app.,Integrate AWS Amplify to your mobile app.,Integrate Amazon Pinpoint to your mobile app.,,,,플레이어의 앱 기본 설정과 게임 상태가 여러 장치에서 동기화되어야 하는 온라인 게임을 개발 중입니다. 또한 여러 사용자가 공유 데이터를 실시간으로 동기화하고 공동 작업할 수 있어야 합니다.다음 중 이 시나리오에 구현해야 하는 가장 적절한 솔루션은 무엇입니까?,AWS AppSync를 모바일 앱에 통합합니다.,Amazon Cognito Sync를 모바일 앱에 통합하세요.,AWS Amplify를 모바일 앱에 통합하세요.,Amazon Pinpoint를 모바일 앱에 통합하세요.,,,0,,
udemy,DVA-02,323,"You are working as an IT Consultant for a top investment bank in Europe which uses several serverless applications in their AWS account. They just launched a new API Gateway service with a Lambda proxy integration and you were instructed to test out the new API. However, you are getting a Connection refused error whenever you use this Invoke URL http://779protaw8.execute-api.us-east-1.amazonaws.com/tutorialsdojo/ of the API Gateway. Which of the following is the MOST likely cause of this issue?",D,D,You are not using FTP in invoking the API.,You are not using HTTP/2 in invoking the API.,You are not using WebSocket in invoking the API.,You are not using HTTPS in invoking the API.,,,,귀하는 AWS 계정에서 여러 서버리스 애플리케이션을 사용하는 유럽 최고의 투자 은행에서 IT 컨설턴트로 일하고 있습니다. 방금 Lambda 프록시 통합을 통해 새로운 API 게이트웨이 서비스를 시작했으며 새로운 API를 테스트하라는 지시를 받았습니다. 그러나 이 API 게이트웨이 Connection refused호출 URL을 사용할 때마다 오류가 발생합니다 .http://779protaw8.execute-api.us-east-1.amazonaws.com/tutorialsdojo/다음 중 이 문제의 가장 큰 원인은 무엇입니까?,API를 호출할 때 FTP를 사용하고 있지 않습니다.,API 호출에 HTTP/2를 사용하지 않습니다.,API 호출에 WebSocket을 사용하고 있지 않습니다.,API 호출에 HTTPS를 사용하지 않습니다.,,,0,,
udemy,DVA-02,324,"An application running in an EC2 instance is regularly fetching and processing a lot of data from an S3 bucket which resulted in a significant increase in your operating costs. You want to lower the latency of retrieving data from S3 and bring the operating costs down. To improve the system, you need to use simple structured query language (SQL) statements to filter the contents of Amazon S3 objects and retrieve just the subset of data that you need. Which is the MOST suitable service that will help you accomplish this requirement?",C,C,Athena,AWS Step Functions,S3 Select,Redshift Spectrum,,,,EC2 인스턴스에서 실행되는 애플리케이션은 정기적으로 S3 버킷에서 많은 데이터를 가져오고 처리하므로 운영 비용이 크게 증가합니다. S3에서 데이터를 검색하는 대기 시간을 줄이고 운영 비용을 낮추고 싶습니다. 시스템을 개선하려면 간단한 구조적 쿼리 언어(SQL) 문을 사용하여 Amazon S3 객체의 콘텐츠를 필터링하고 필요한 데이터 하위 집합만 검색해야 합니다.이 요구 사항을 달성하는 데 도움이 되는 가장 적합한 서비스는 무엇입니까?,아테나,AWS 단계 함수,S3 선택,적색편이 스펙트럼,,,0,,
udemy,DVA-02,325,"A leading financial company has recently deployed its application to AWS using Lambda and API Gateway. However, they noticed that all metrics are being populated in their CloudWatch dashboard except for CacheHitCount and CacheMissCount.What could be the MOST likely cause of this issue?",A,A,API Caching is not enabled in API Gateway.,The provided IAM role to their API Gateway only has read access but no write privileges to CloudWatch.,API Gateway Private Integrations has not been configured yet.,They have not provided an IAM role to their API Gateway yet.,,,,한 선도적인 금융 회사는 최근 Lambda와 API Gateway를 사용하여 자사 애플리케이션을 AWS에 배포했습니다. CacheHitCount그러나 그들은 및를 제외한 모든 지표가 CloudWatch 대시보드에 채워지는 것을 확인했습니다 CacheMissCount.이 문제의 가장 가능성 있는 원인은 무엇입니까?,API Gateway에서는 API 캐싱이 활성화되어 있지 않습니다.,API 게이트웨이에 제공된 IAM 역할에는 읽기 액세스 권한만 있고 CloudWatch에 대한 쓰기 권한은 없습니다.,API Gateway 프라이빗 통합이 아직 구성되지 않았습니다.,아직 API 게이트웨이에 IAM 역할을 제공하지 않았습니다.,,,0,,
udemy,DVA-02,326,A developer needs to use IAM roles to list all EC2 instances that belong to the development environment in an AWS account.Which methods could be done to verify IAM access to describe instances? (Select TWO.),BE,BE,Run the get-role command.,Run the describe-instances command with the --dry-run parameter.,Validate the IAM role’s permission by querying the in-line policies within the EC2 instance metadata.,Run the get-group-policy command.,Use the IAM Policy Simulator to validate the permission for the IAM role.,,,개발자는 IAM 역할을 사용하여 AWS 계정의 개발 환경에 속하는 모든 EC2 인스턴스를 나열해야 합니다.인스턴스를 설명하기 위해 IAM 액세스를 확인하려면 어떤 방법을 수행할 수 있습니까? (2개를 선택하세요.),명령을 실행하십시오 get-role.,describe-instances매개변수를 사용하여 명령을 실행합니다 --dry-run.,EC2 인스턴스 메타데이터 내에서 인라인 정책을 쿼리하여 IAM 역할의 권한을 확인합니다.,명령을 실행하십시오 get-group-policy.,IAM 정책 시뮬레이터를 사용하여 IAM 역할에 대한 권한을 검증합니다.,,0,,
udemy,DVA-02,327,An AWS Site-to-Site VPN connection that uses Border Gateway Protocol (BGP) is used to establish a connection between an on-premises server and multiple EC2 instances in a VPC. A Developer cannot connect to an instance in subnet A but can access an instance in subnet B.Which action should the developer do as the first step in troubleshooting?,A,A,Check the VPC Flow Logs if the traffic is reaching subnet A.,Check the VPN Logs if the traffic is reaching subnet A.,Check the AWS CloudTrail Logs if the traffic is reaching subnet A.,Check the BGP Logs if the traffic is reaching subnet A.,,,,BGP(Border Gateway Protocol)를 사용하는 AWS Site-to-Site VPN 연결은 온프레미스 서버와 VPC의 여러 EC2 인스턴스 간의 연결을 설정하는 데 사용됩니다. 개발자는 서브넷 A의 인스턴스에 연결할 수 없지만 서브넷 B의 인스턴스에 액세스할 수 있습니다.문제 해결의 첫 번째 단계로 개발자는 어떤 조치를 취해야 합니까?,트래픽이 서브넷 A에 도달하는 경우 VPC 흐름 로그를 확인하세요.,트래픽이 서브넷 A에 도달하는 경우 VPN 로그를 확인하세요.,트래픽이 서브넷 A에 도달하는 경우 AWS CloudTrail 로그를 확인하십시오.,트래픽이 서브넷 A에 도달하는 경우 BGP 로그를 확인하세요.,,,0,,
udemy,DVA-02,328,"A company wants to know how its monolithic application will perform on a microservice architecture. The Lead Developer has deployed the application on Amazon ECS using the EC2 launch type. He terminated the container instance after testing; however, the container instance still appears as a resource in the ECS cluster.What is the possible cause of this?",C,C,"After terminating the container instance in the stopped state, the container instance must be manually deregistered in the Amazon EC2 Console since it was launched using the EC2 launch type.","When a container instance is terminated in the running state, the container instance is not automatically deregistered from the cluster.","When a container instance is terminated in the stopped state, the container instance is not automatically deregistered from the cluster.","After terminating the container instance in the running state, the container instance must be manually deregistered in the Amazon ECS Console.",,,,회사는 모놀리식 애플리케이션이 마이크로서비스 아키텍처에서 어떻게 작동하는지 알고 싶어합니다. 수석 개발자는 EC2 시작 유형을 사용하여 Amazon ECS에 애플리케이션을 배포했습니다. 그는 테스트 후 컨테이너 인스턴스를 종료했습니다. 그러나 컨테이너 인스턴스는 여전히 ECS 클러스터의 리소스로 표시됩니다.이것의 가능한 원인은 무엇입니까?,컨테이너 인스턴스는 중지된 상태에서 종료한 후 EC2 시작 유형을 사용하여 시작되었으므로 Amazon EC2 콘솔에서 수동으로 등록을 취소해야 합니다.,실행 중인 상태에서 컨테이너 인스턴스가 종료되면 해당 컨테이너 인스턴스는 클러스터에서 자동으로 등록 취소되지 않습니다.,컨테이너 인스턴스가 중지된 상태에서 종료되면 해당 컨테이너 인스턴스는 클러스터에서 자동으로 등록 취소되지 않습니다.,실행 중인 상태에서 컨테이너 인스턴스를 종료한 후 Amazon ECS 콘솔에서 컨테이너 인스턴스를 수동으로 등록 취소해야 합니다.,,,0,,
udemy,DVA-02,329,"A developer is debugging an issue in an AWS Lambda-based application. To save time searching through logs, the developer wants the function to return the corresponding log location of an invocation request.Which approach should the developer take with the least amount of effort?",D,D,Extract the log stream name from the Event object of the handler function.,Extract the invocation request id from the Event object of the handler. Call the FilterLogEvents API and use the request id to filter results.,"Extract the invocation request id from the Context object of the handler function. Then, call the FilterLogEvents API and pass the request id to filter results.",Extract the log stream name from the Context object of the handler function.,,,,개발자가 AWS Lambda 기반 애플리케이션의 문제를 디버깅하고 있습니다. 로그를 검색하는 시간을 절약하기 위해 개발자는 함수가 호출 요청의 해당 로그 위치를 반환하기를 원합니다.개발자는 최소한의 노력으로 어떤 접근 방식을 취해야 합니까?,Event핸들러 함수의 객체 에서 로그 스트림 이름을 추출합니다 .,Event핸들러 객체 에서 호출 요청 ID를 추출합니다 . API를 호출 FilterLogEvents하고 요청 ID를 사용하여 결과를 필터링합니다.,Context핸들러 함수의 객체 에서 호출 요청 ID를 추출합니다 . 그런 다음 FilterLogEventsAPI를 호출하고 요청 ID를 전달하여 결과를 필터링합니다.,Context핸들러 함수의 객체 에서 로그 스트림 이름을 추출합니다 .,,,0,,
udemy,DVA-02,330,The Customer and Payment service components of a microservices application have two separate DynamoDB tables. New items inserted into the Customer service table must be dynamically updated in the Payment service table.How can the Payment service get near real-time updates?,D,D,Create a Kinesis Data Firehose delivery stream to stream all the changes from the Customer service table and trigger a Lambda function to update the Payment service table.,Use a Kinesis data stream to stream all the changes from the Customer service database directly into the Payment service table.,Create a scheduled Amazon EventBridge rule that invokes a Lambda function every minute to update the changes from the Customer service table into the Payment service table.,Enable DynamoDB Streams to stream all the changes from the Customer service table and trigger a Lambda function to update the Payment service table.,,,,마이크로서비스 애플리케이션의 고객 및 결제 서비스 구성 요소에는 두 개의 별도 DynamoDB 테이블이 있습니다. 고객 서비스 테이블에 삽입된 새 항목은 결제 서비스 테이블에서 동적으로 업데이트되어야 합니다.결제 서비스는 어떻게 실시간 업데이트를 받을 수 있나요?,Kinesis Data Firehose 전송 스트림을 생성하여 고객 서비스 테이블의 모든 변경 사항을 스트리밍하고 Lambda 함수를 트리거하여 결제 서비스 테이블을 업데이트합니다.,Kinesis 데이터 스트림을 사용하여 고객 서비스 데이터베이스의 모든 변경 사항을 결제 서비스 테이블로 직접 스트리밍합니다.,매분마다 Lambda 함수를 호출하여 고객 서비스 테이블의 변경 사항을 결제 서비스 테이블로 업데이트하는 예약된 Amazon EventBridge 규칙을 생성합니다.,DynamoDB 스트림을 활성화하여 고객 서비스 테이블의 모든 변경 사항을 스트리밍하고 Lambda 함수를 트리거하여 결제 서비스 테이블을 업데이트합니다.,,,0,,
udemy,DVA-02,331,A team of developers needs permission to launch EC2 instances with an instance role that will allow them to update items in a DynamoDB table. Each developer has access to IAM users that belongs in the same IAM group.Which of the following steps must be done to implement the solution?,C,C,Create an IAM role with an IAM policy that will allow access to the EC2 instances. Add the DynamoDB service to the trust policy of the role. Create a custom policy with the iam:GetRole permission. Attach the policy to the IAM group.,Create an IAM role with an IAM policy that will allow access to the EC2 instances. Add the DynamoDB service to the trust policy of the role. Create a custom policy with the iam:PassRole permission. Attach the policy to the IAM group.,Create an IAM role with an IAM policy that will allow access to the DynamoDB table. Add the EC2 service to the trust policy of the role. Create a custom policy with the iam:PassRole permission. Attach the policy to the IAM group.,Create an IAM role with an IAM policy that will allow access to the DynamoDB table. Add the EC2 service to the trust policy of the role. Create a custom policy with iam:GetRolePolicy and iam:PutRolePolicy permissions. Attach the policy to the IAM group.,,,,개발자 팀에는 DynamoDB 테이블의 항목을 업데이트할 수 있는 인스턴스 역할을 사용하여 EC2 인스턴스를 시작할 수 있는 권한이 필요합니다. 각 개발자는 동일한 IAM 그룹에 속한 IAM 사용자에 액세스할 수 있습니다.솔루션을 구현하려면 다음 중 어떤 단계를 수행해야 합니까?,EC2 인스턴스에 대한 액세스를 허용하는 IAM 정책을 사용하여 IAM 역할을 생성합니다. 역할의 신뢰 정책에 DynamoDB 서비스를 추가합니다. 권한이 있는 사용자 지정 정책을 만듭니다 iam:GetRole. IAM 그룹에 정책을 연결합니다.,EC2 인스턴스에 대한 액세스를 허용하는 IAM 정책을 사용하여 IAM 역할을 생성합니다. 역할의 신뢰 정책에 DynamoDB 서비스를 추가합니다. 권한이 있는 사용자 지정 정책을 만듭니다 iam:PassRole. IAM 그룹에 정책을 연결합니다.,DynamoDB 테이블에 대한 액세스를 허용하는 IAM 정책을 사용하여 IAM 역할을 생성합니다. 역할의 신뢰 정책에 EC2 서비스를 추가합니다. 권한이 있는 사용자 지정 정책을 만듭니다 iam:PassRole. IAM 그룹에 정책을 연결합니다.,DynamoDB 테이블에 대한 액세스를 허용하는 IAM 정책을 사용하여 IAM 역할을 생성합니다. 역할의 신뢰 정책에 EC2 서비스를 추가합니다. iam:GetRolePolicy및 권한을 사용하여 사용자 지정 정책을 만듭니다 iam:PutRolePolicy. IAM 그룹에 정책을 연결합니다.,,,0,,
udemy,DVA-02,332,A developer has an application that stores sensitive data to an Amazon DynamoDB table. AWS KMS must be used to encrypt the data before sending it to the table and to manage the encryption keys.Which of the following features are supported when using AWS KMS? (Select TWO.),BE,BE,Importing a custom key material to an asymmetric KMS key,Creation of symmetric and asymmetric KMS keys,Using AWS Certificate Manager as a custom key store,Automatic key rotation for KMS keys in custom key stores,Re-enabling disabled keys,,,개발자는 중요한 데이터를 Amazon DynamoDB 테이블에 저장하는 애플리케이션을 가지고 있습니다. 데이터를 테이블로 보내기 전에 암호화하고 암호화 키를 관리하려면 AWS KMS를 사용해야 합니다.다음 중 AWS KMS를 사용할 때 지원되는 기능은 무엇입니까? (2개를 선택하세요.),사용자 지정 키 자료를 비대칭 KMS 키로 가져오기,대칭 및 비대칭 KMS 키 생성,AWS Certificate Manager를 사용자 지정 키 스토어로 사용,사용자 지정 키 스토어의 KMS 키에 대한 자동 키 순환,비활성화된 키를 다시 활성화,,0,,
udemy,DVA-02,333,"A developer is building a ReactJS application that will be hosted on Amazon S3. Amazon Cognito handles the registration and signing of users using the AWS Software Development Kit (SDK) for JavaScript. The JSON Web Token (JWT) received upon authentication will be stored on the browser's local storage. After signing in, the application will use the JWT as an authorizer to access an API Gateway endpoint.What are the steps needed to implement the scenario above? (Select THREE.)",BCD,BCD,Choose and set the authentication provider for your website.,Create an Amazon Cognito User Pool.,Set the name of the header that will be used from the request to the Cognito User Pool as a token source for authorization.,"On the API Gateway Console, create an authorizer using the Cognito User Pool ID.",Set the name of the header that will be used from the request to the Cognito Identity Pool as a token source for authorization.,Create an Amazon Cognito Identity Pool.,,개발자가 Amazon S3에서 호스팅될 ReactJS 애플리케이션을 구축하고 있습니다. Amazon Cognito는 JavaScript용 AWS 소프트웨어 개발 키트(SDK)를 사용하여 사용자 등록 및 서명을 처리합니다. 인증 시 수신된 JWT(JSON Web Token)는 브라우저의 로컬 저장소에 저장됩니다. 로그인한 후 애플리케이션은 JWT를 승인자로 사용하여 API 게이트웨이 엔드포인트에 액세스합니다.위의 시나리오를 구현하는 데 필요한 단계는 무엇입니까? (3개를 선택하세요.),귀하의 웹사이트에 대한 인증 공급자를 선택하고 설정하세요.,Amazon Cognito 사용자 풀을 생성합니다.,인증을 위한 토큰 소스로 Cognito 사용자 풀에 대한 요청에서 사용될 헤더의 이름을 설정합니다.,API Gateway 콘솔에서 Cognito 사용자 풀 ID를 사용하여 권한 부여자를 생성합니다.,인증을 위한 토큰 소스로 Cognito 자격 증명 풀에 대한 요청에서 사용될 헤더의 이름을 설정합니다.,,0,,Amazon Cognito 자격 증명 풀을 생성합니다.
udemy,DVA-02,334,"A developer has enabled the lifecycle policy of an application deployed in Elastic Beanstalk. The lifecycle is set to limit the application version to 15 versions. The developer wants to keep the source code in an S3 bucket, yet, it gets deleted.What change should the developer do?",B,B,Modify the value of the Set application versions limit by the total count option to zero.,Configure the Retention setting to retain the source bundle in the S3 bucket.,Trigger a Lambda function to copy the source code to another S3 bucket.,Modify the value of the Set the application versions limit by age option to zero.,,,,개발자가 Elastic Beanstalk에 배포된 애플리케이션의 수명 주기 정책을 활성화했습니다. 수명주기는 애플리케이션 버전을 15개 버전으로 제한하도록 설정됩니다. 개발자는 소스 코드를 S3 버킷에 보관하려고 하지만 삭제됩니다.개발자는 어떤 변화를 주어야 할까요?,옵션 값을 Set application versions limit by the total count0으로 수정합니다.,RetentionS3 버킷에 소스 번들을 유지하도록 설정을 구성합니다 .,Lambda 함수를 트리거하여 소스 코드를 다른 S3 버킷에 복사합니다.,옵션 값을 Set the application versions limit by age0으로 수정합니다.,,,0,,
udemy,DVA-02,335,A development team has migrated an existing Git repository to a CodeCommit repository. One of the developers was given an HTTPS clone URL of their new repository. The developer must be able to clone the repository using his access key credentials.What must the developer do before he can proceed?,C,C,Generate an HTTPS Git credential for AWS CodeCommit. Configure the Git credential helper with the AWS credential profile.,Import an SSL/TLS certificate into the AWS Certificate Manager.,Configure the Git credential helper with the AWS credential profile.,Generate an RSA key pair to use with AWS CodeCommit using AWS KMS.,,,,개발 팀이 기존 Git 리포지토리를 CodeCommit 리포지토리로 마이그레이션했습니다. 개발자 중 한 명에게 새 저장소의 HTTPS 복제 URL이 제공되었습니다. 개발자는 액세스 키 자격 증명을 사용하여 저장소를 복제할 수 있어야 합니다.계속 진행하기 전에 개발자는 무엇을 해야 합니까?,AWS CodeCommit에 대한 HTTPS Git 자격 증명을 생성합니다. AWS 자격 증명 프로필을 사용하여 Git 자격 증명 도우미를 구성합니다.,SSL/TLS 인증서를 AWS Certificate Manager로 가져옵니다.,AWS 자격 증명 프로필을 사용하여 Git 자격 증명 도우미를 구성합니다.,AWS KMS를 사용하여 AWS CodeCommit에 사용할 RSA 키 페어를 생성합니다.,,,0,,
udemy,DVA-02,336,"A startup is integrating an event-driven alerting tool with a third-party platform. The platform requires a publicly accessible HTTPS endpoint to receive webhook requests, which will be processed by a Lambda function.Given that the platform signs each request with a secret key and includes it in the headers, the developer must ensure that the Lambda function executes the domain logic only when a webhook request comes from a valid user.Which action would satisfy the requirement with the least amount of development effort?",B,B,Configure API Gateway to connect with the Lambda function using a Lambda proxy integration. Create a Lambda function authorizer to validate incoming requests based on a signature provided in the HTTP headers.,"Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the ""lambda:FunctionUrlAuthType"": ""NONE"" condition is present. Write a custom authorization logic based on a signature provided in the HTTP headers.","Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the ""lambda:FunctionUrlAuthType"": ""AWS_IAM"" condition is present.","Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the ""lambda:CodeSigningConfigArn"": ""arn:aws:lambda:<AWS_REGION>:<ACCOUNT_NUMBER>:code-signing-config:csc-<SIGNING_SECRET>"" condition is present.",,,,한 스타트업에서는 이벤트 기반 알림 도구를 타사 플랫폼과 통합하고 있습니다. 플랫폼에는 Lambda 함수에 의해 처리되는 웹후크 요청을 수신하기 위해 공개적으로 액세스 가능한 HTTPS 엔드포인트가 필요합니다.플랫폼이 비밀 키로 각 ​​요청에 서명하고 이를 헤더에 포함한다는 점을 감안할 때 개발자는 웹훅 요청이 유효한 사용자로부터 오는 경우에만 Lambda 함수가 도메인 로직을 실행하는지 확인해야 합니다.최소한의 개발 노력으로 요구 사항을 충족하는 작업은 무엇입니까?,Lambda 프록시 통합을 사용하여 Lambda 함수와 연결하도록 API 게이트웨이를 구성합니다. HTTP 헤더에 제공된 서명을 기반으로 수신 요청을 검증하는 Lambda 함수 권한 부여자를 생성합니다.,"Lambda 함수 URL을 생성합니다. ""lambda:FunctionUrlAuthType"": ""NONE""조건이 있는 경우에만 누구든지 호출할 수 있도록 함수에 리소스 기반 정책을 연결합니다 . HTTP 헤더에 제공된 서명을 기반으로 사용자 정의 인증 논리를 작성합니다.","Lambda 함수 URL을 생성합니다. ""lambda:FunctionUrlAuthType"": ""AWS_IAM""조건이 있는 경우에만 누구든지 호출할 수 있도록 함수에 리소스 기반 정책을 연결합니다 .","Lambda 함수 URL을 생성합니다. ""lambda:CodeSigningConfigArn"": ""arn:aws:lambda:<AWS_REGION>:<ACCOUNT_NUMBER>:code-signing-config:csc-<SIGNING_SECRET>""조건이 있는 경우에만 누구든지 호출할 수 있도록 함수에 리소스 기반 정책을 연결합니다 .",,,0,,
udemy,DVA-02,337,"A Lamba function has multiple sub-functions that are chained together to process large data synchronously. When invoked, the function tends to exceed its maximum timeout limit. This has prompted the developer to break the Lambda function into manageable coordinated states using Step Functions, enabling each sub-function to run in separate processes.Which of the following type of states should the developer use to run processes?",B,B,Parallel State,Task State,Wait State,Pass State,,,,Lamba 함수에는 대규모 데이터를 동기식으로 처리하기 위해 함께 연결되는 여러 하위 함수가 있습니다. 호출되면 함수가 최대 시간 초과 제한을 초과하는 경향이 있습니다. 이로 인해 개발자는 Step Functions를 사용하여 Lambda 기능을 관리 가능한 조정 상태로 분리하여 각 하위 기능이 별도의 프로세스에서 실행될 수 있도록 했습니다.다음 중 개발자가 프로세스를 실행하기 위해 사용해야 하는 상태 유형은 무엇입니까?,병렬 상태,작업 상태,대기 상태,통과 상태,,,0,,
udemy,DVA-02,338,A developer needs to build a queueing mechanism for an application that will run on AWS. The application is expected to consume SQS messages that are larger than 256 KB and up to 1 GB in size.How should the developer manage the SQS messages?,D,D,Use Amazon S3 and the Amazon SQS Console,Use Amazon S3 and the Amazon SQS CLI,Use Amazon S3 and the Amazon SQS HTTP API,Use Amazon S3 and the Amazon SQS Extended Client Library for Java,,,,개발자는 AWS에서 실행될 애플리케이션에 대한 대기열 메커니즘을 구축해야 합니다. 애플리케이션은 256KB보다 크고 크기는 최대 1GB인 SQS 메시지를 사용할 것으로 예상됩니다.개발자는 SQS 메시지를 어떻게 관리해야 합니까?,Amazon S3 및 Amazon SQS 콘솔 사용,Amazon S3 및 Amazon SQS CLI 사용,Amazon S3 및 Amazon SQS HTTP API 사용,Amazon S3 및 Java용 Amazon SQS 확장 클라이언트 라이브러리 사용,,,0,,
udemy,DVA-02,339,A development team wants to move its continuous integration (CI) system into AWS. The system is built using Github as the code repository. Each code push triggers a webhook that causes the CI software to compile the source code and runs a test to check if the changes broke anything before deploying.What AWS services can the development team use?,A,A,"Replace Github, webhook, and the CI software with AWS CodeCommit, AWS Lambda, and AWS CodeBuild respectively.","Replace Github, webhook, and the CI software with AWS CodeCommit, AWS Lambda, and AWS CodeDeploy respectively.","Replace Github, webhook, and the CI software with AWS CodeBuild, AWS Lambda, and AWS CodeCommit respectively.","Replace Github, webhook, and the CI software with AWS CodeArtifact, AWS Lambda, and AWS CodeDeploy respectively.",,,,개발 팀은 CI(지속적 통합) 시스템을 AWS로 이전하려고 합니다. 시스템은 Github를 코드 저장소로 사용하여 구축되었습니다. 각 코드 푸시는 CI 소프트웨어가 소스 코드를 컴파일하고 테스트를 실행하여 배포하기 전에 변경 사항이 중단되었는지 확인하는 웹후크를 트리거합니다.개발팀은 어떤 AWS 서비스를 사용할 수 있습니까?,"Github, webhook 및 CI 소프트웨어를 각각 AWS CodeCommit, AWS Lambda 및 AWS CodeBuild로 교체합니다.","Github, webhook 및 CI 소프트웨어를 각각 AWS CodeCommit, AWS Lambda 및 AWS CodeDeploy로 교체합니다.","Github, webhook 및 CI 소프트웨어를 각각 AWS CodeBuild, AWS Lambda 및 AWS CodeCommit으로 교체합니다.","Github, webhook 및 CI 소프트웨어를 각각 AWS CodeArtifact, AWS Lambda 및 AWS CodeDeploy로 교체합니다.",,,0,,
udemy,DVA-02,340,"A company is looking to run a distributed application across hundreds of containers using Amazon Elastic Container Service. Due to the nature and size of the application, the Chief Technology Officer (CTO) is worried about how the developers can analyze and debug the application conveniently and efficiently. The developers need to have an end-to-end view of how the application runs and performs across the resources and services so they can make adjustments if needed.Which AWS service should the CTO use?",A,A,AWS X-Ray,AWS CloudTrail,Amazon CloudWatch,Amazon EventBridge,,,,한 회사는 Amazon Elastic Container Service를 사용하여 수백 개의 컨테이너에 걸쳐 분산 애플리케이션을 실행하려고 합니다. 애플리케이션의 특성과 규모로 인해 CTO(최고 기술 책임자)는 개발자가 애플리케이션을 편리하고 효율적으로 분석하고 디버깅할 수 있는 방법에 대해 고민합니다. 개발자는 필요한 경우 조정할 수 있도록 리소스와 서비스 전반에 걸쳐 애플리케이션이 어떻게 실행되고 수행되는지에 대한 엔드투엔드 보기가 필요합니다.CTO는 어떤 AWS 서비스를 사용해야 합니까?,AWS 엑스레이,AWS 클라우드트레일,아마존 클라우드워치,아마존 이벤트브리지,,,0,,
udemy,DVA-02,341,"An application is hosted in the us-east-1 region. The app needs to be recreated on the us-east-2, ap-northeast-1, and ap-southeast-1 region using the same Amazon Machine Image (AMI). As the developer, you have to use AWS CloudFormation to rebuild the application using a template.Which of the following actions is the most suitable way to configure the CloudFormation template for the scenario?",B,B,"Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::ImportValue function to retrieve the desired Image Id from the region key.","Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::FindInMap function to retrieve the desired Image Id from the region key.","Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Fn::GetAtt function to retrieve the desired Image Id from the region key.","Copy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then, add a Parameters section wherein you will define the different Image Id for the three regions. Use the region name as the key in mapping to its correct Image Id. Lastly, use the Ref function to retrieve the desired Image Id from the region key.",,,,"애플리케이션은 us-east-1 지역에서 호스팅됩니다. 동일한 Amazon Machine Image(AMI)를 사용하여 us-east-2, ap-northeast-1 및 ap-southeast-1 지역에서 앱을 다시 생성해야 합니다. 개발자는 템플릿을 사용하여 애플리케이션을 재구축하려면 AWS CloudFormation을 사용해야 합니다.다음 중 시나리오에 맞게 CloudFormation 템플릿을 구성하는 가장 적합한 방법은 무엇입니까?","us-east-1 지역의 인스턴스 AMI를 us-east-2, ap-northeast-1 및 ap-southeast-1 지역으로 복사합니다. 그런 다음 세 지역에 대해 서로 다른 이미지 ID를 정의할 매핑 섹션을 추가합니다. 올바른 이미지 ID에 매핑할 때 지역 이름을 키로 사용하세요. 마지막으로 Fn::ImportValue기능을 사용하여 지역 키에서 원하는 이미지 ID를 검색합니다.","us-east-1 지역의 인스턴스 AMI를 us-east-2, ap-northeast-1 및 ap-southeast-1 지역으로 복사합니다. 그런 다음 세 지역에 대해 서로 다른 이미지 ID를 정의할 매핑 섹션을 추가합니다. 올바른 이미지 ID에 매핑할 때 지역 이름을 키로 사용하세요. 마지막으로 Fn::FindInMap기능을 사용하여 지역 키에서 원하는 이미지 ID를 검색합니다.","us-east-1 지역의 인스턴스 AMI를 us-east-2, ap-northeast-1 및 ap-southeast-1 지역으로 복사합니다. 그런 다음 세 지역에 대해 서로 다른 이미지 ID를 정의할 매핑 섹션을 추가합니다. 올바른 이미지 ID에 매핑할 때 지역 이름을 키로 사용하세요. 마지막으로 Fn::GetAtt기능을 사용하여 지역 키에서 원하는 이미지 ID를 검색합니다.","us-east-1 지역의 인스턴스 AMI를 us-east-2, ap-northeast-1 및 ap-southeast-1 지역으로 복사합니다. 그런 다음 세 지역에 대해 서로 다른 이미지 ID를 정의할 매개변수 섹션을 추가합니다. 올바른 이미지 ID에 매핑할 때 지역 이름을 키로 사용하세요. 마지막으로 Ref기능을 사용하여 지역 키에서 원하는 이미지 ID를 검색합니다.",,,0,,
udemy,DVA-02,342,"A developer plans to launch an EC2 instance, with Amazon Linux 2 as its AMI, using the AWS Console. A security group with port 80 that is open to public access will be associated with the instance. He wants to quickly build and test an Apache webserver with an index.html displaying a hello world message.Which of the following should the developer do?",C,C,Connect to the instance via port 22. Run the commands that will install and create the Apache webserver.,Connect to the instance via port 80. Run the commands that will install and create the Apache webserver.,Configure the user data at the creation of the EC2 instance to run a script that will install and create the Apache webserver after the instance starts.,Configure the metadata at the creation of the EC2 instance to run a script that will install the Apache webserver after the instance starts.,,,,개발자는 AWS 콘솔을 사용하여 Amazon Linux 2를 AMI로 사용하여 EC2 인스턴스를 시작할 계획입니다. 공개 액세스에 개방된 포트 80이 있는 보안 그룹이 인스턴스와 연결됩니다. index.html그는 hello world 메시지를 표시하는 Apache 웹 서버를 빠르게 구축하고 테스트하려고 합니다 .다음 중 개발자는 무엇을 해야 합니까?,포트 22를 통해 인스턴스에 연결합니다. Apache 웹 서버를 설치하고 생성하는 명령을 실행합니다.,포트 80을 통해 인스턴스에 연결합니다. Apache 웹 서버를 설치하고 생성하는 명령을 실행합니다.,인스턴스가 시작된 후 Apache 웹 서버를 설치하고 생성하는 스크립트를 실행하려면 EC2 인스턴스 생성 시 사용자 데이터를 구성합니다.,인스턴스가 시작된 후 Apache 웹 서버를 설치하는 스크립트를 실행하도록 EC2 인스턴스 생성 시 메타데이터를 구성합니다.,,,0,,
udemy,DVA-02,343,A development team is building a website that displays an analytics dashboard. The team uses AWS CodeBuild to compile the website from a source code residing on Github. A member was instructed to configure CodeBuild to run with a proxy server for privacy and security reasons. A RequestError timeout error appears on CloudWatch whenever CodeBuild is accessed.Which is a possible solution to resolve the issue?,D,D,Modify the proxy element of the AppSpec.yml file on the source code root directory.,Modify the artifacts element of the buildspec.yml file on the source code root directory.,Modify the phases element of the AppSpec.yml file on the source code root directory.,Modify the proxy element of the buildspec.yml file on the source code root directory.,,,,개발팀은 분석 대시보드를 표시하는 웹사이트를 구축하고 있습니다. 팀은 AWS CodeBuild를 사용하여 Github에 있는 소스 코드에서 웹 사이트를 컴파일합니다. 회원에게는 개인 정보 보호 및 보안상의 이유로 프록시 서버와 함께 실행되도록 CodeBuild를 구성하라는 지시가 있었습니다. RequestErrorCodeBuild에 액세스할 때마다 CloudWatch에 시간 초과 오류가 나타납니다 .문제를 해결하기 위한 가능한 해결책은 무엇입니까?,소스 코드 루트 디렉터리에 있는 파일 proxy의 요소를 수정합니다 .AppSpec.yml,소스 코드 루트 디렉터리에 있는 파일 artifacts의 요소를 수정합니다 .buildspec.yml,소스 코드 루트 디렉터리에 있는 파일 phases의 요소를 수정합니다 .AppSpec.yml,소스 코드 루트 디렉터리에 있는 파일 proxy의 요소를 수정합니다 .buildspec.yml,,,0,,
udemy,DVA-02,344,A developer uses Amazon ECS to orchestrate two Docker containers. He needs to configure ECS to allow the two containers to share log data.Which configuration should the developer do?,C,C,Specify the containers in a single pod specification and configure EFS as its volume type.,Use two task definitions for each container and mount an EFS volume between the tasks.,Specify the containers in a single task definition and configure EFS as its volume type.,Use two pod specifications for each container and mount an EFS volume between the pods.,,,,개발자는 Amazon ECS를 사용하여 두 개의 Docker 컨테이너를 오케스트레이션합니다. 그는 두 컨테이너가 로그 데이터를 공유할 수 있도록 ECS를 구성해야 합니다.개발자는 어떤 구성을 해야 합니까?,단일 포드 사양에 컨테이너를 지정하고 EFS를 볼륨 유형으로 구성합니다.,각 컨테이너에 대해 두 개의 작업 정의를 사용하고 작업 사이에 EFS 볼륨을 탑재합니다.,단일 작업 정의에서 컨테이너를 지정하고 EFS를 볼륨 유형으로 구성합니다.,각 컨테이너에 대해 두 개의 포드 사양을 사용하고 포드 사이에 EFS 볼륨을 탑재합니다.,,,0,,
udemy,DVA-02,345,A developer has been instructed to automate the creation of the snapshot of an existing Amazon EC2 instance. The engineer created a script that uses the AWS Command Line Interface (CLI) to run the necessary API call. He is getting an InvalidInstanceID.NotFound error whenever the script is run.What is the most likely cause of the error?,A,A,The AWS Region name used to configure the AWS CLI does not match the region where the instance lives.,"The AWS Region, where the programmatic access for the AWS CLI is created, does not match the region where the instance lives.",The AWS Access Key Id used to configure the AWS CLI is invalid.,The Image Id used in running the command for creating a snapshot is incorrect.,,,,개발자는 기존 Amazon EC2 인스턴스의 스냅샷 생성을 자동화하라는 지시를 받았습니다. 엔지니어는 AWS 명령줄 인터페이스(CLI)를 사용하여 필요한 API 호출을 실행하는 스크립트를 생성했습니다. InvalidInstanceID.NotFound스크립트가 실행될 때마다 오류 가 발생합니다 .오류의 가장 가능성 있는 원인은 무엇입니까?,AWS CLI를 구성하는 데 사용된 AWS 지역 이름이 인스턴스가 있는 지역과 일치하지 않습니다.,AWS CLI에 대한 프로그래밍 방식의 액세스가 생성되는 AWS 리전이 인스턴스가 있는 리전과 일치하지 않습니다.,AWS CLI를 구성하는 데 사용된 AWS 액세스 키 ID가 유효하지 않습니다.,스냅샷 생성 명령을 실행하는 데 사용된 이미지 ID가 올바르지 않습니다.,,,0,,
udemy,DVA-02,346,A developer wants to cut down the execution time of the scan operation on a DynamoDB table during periods of low demand without interfering with typical workloads. The operation consumes half of the strongly consistent read capacity units within regular operating hours.How can the developer improve this scan operation?,B,B,Use a parallel scan operation.,Perform a rate-limited parallel scan operation.,Perform a rate-limited sequential scan operation.,Use eventually consistent reads for the scan operation instead of strongly consistent reads.,,,,scan개발자는 일반적인 워크로드를 방해하지 않고 수요가 적은 기간 동안 DynamoDB 테이블에서 작업 실행 시간을 단축하려고 합니다 . 이 작업은 정규 운영 시간 내에 Strongly Constance 읽기 용량 단위의 절반을 소비합니다.개발자는 이 scan작업을 어떻게 개선할 수 있습니까?,병렬 scan작업을 사용하십시오.,속도가 제한된 병렬 scan작업을 수행합니다.,속도가 제한된 순차 scan작업을 수행합니다.,scan강력한 일관된 읽기 대신 최종적으로 일관된 읽기를 작업에 사용합니다 .,,,0,,
udemy,DVA-02,347,"A developer is hosting a static website from an S3 bucket. The website makes requests to an API Gateway endpoint integrated with a Lambda function (non-proxy). The developer noticed that the requests were failing. Upon debugging, he found a: ""No 'Access-Control-Allow-Origin' header is present on the requested resource"" error message.What should the developer do to resolve this issue?",D,D,Set the value of the Access-Control-Allow-Credentials header to true.,Set the value of the Access-Control-Max-Age header to 0.,"In the Amazon S3 Console, enable cross-origin resource sharing (CORS) on the S3 bucket where the website is hosted.","In the API Gateway Console, enable cross-origin resource sharing (CORS) for the method in the specified resource.",,,,"개발자가 S3 버킷에서 정적 웹 사이트를 호스팅하고 있습니다. 웹사이트는 Lambda 함수(비프록시)와 통합된 API Gateway 엔드포인트에 요청합니다. 개발자는 요청이 실패하고 있음을 발견했습니다. 디버깅 중에 그는 ""No 'Access-Control-Allow-Origin' header is present on the requested resource""오류 메시지를 발견했습니다.이 문제를 해결하려면 개발자는 어떻게 해야 합니까?",헤더 값을 Access-Control-Allow-Credentials으로 설정합니다 true.,헤더 값을 Access-Control-Max-Age0으로 설정합니다.,Amazon S3 콘솔에서 웹 사이트가 호스팅되는 S3 버킷에 대해 CORS(교차 원본 리소스 공유)를 활성화합니다.,API Gateway 콘솔에서 지정된 리소스의 메서드에 대해 CORS(교차 원본 리소스 공유)를 활성화합니다.,,,0,,
udemy,DVA-02,348,A developer is building a new feature for an application deployed on an EC2 instance in the N. Virginia region. A co-developer suggests to upload the code on Amazon S3 and use CodeDeploy to deploy the new version of the application. The deployment fails during the DownloadBundle deployment lifecycle event with the UnknownError: not opened for reading error.What is the possible cause of this?,D,D,Versioning is not enabled on the Amazon S3 Bucket where the application code resides.,The DownloadBundle deployment lifecycle event is not supported in the N. Virginia region.,Wrong configuration of the DownloadBundle lifecycle event in the AppSec file.,The EC2 instance’s IAM profile does not have the permissions to access the application code in Amazon S3.,,,,개발자가 버지니아 북부 지역의 EC2 인스턴스에 배포된 애플리케이션을 위한 새로운 기능을 구축하고 있습니다. 공동 개발자는 Amazon S3에 코드를 업로드하고 CodeDeploy를 사용하여 애플리케이션의 새 버전을 배포할 것을 제안합니다. . DownloadBundle_ UnknownError: not opened for reading error_이것의 가능한 원인은 무엇입니까?,애플리케이션 코드가 있는 Amazon S3 버킷에서는 버전 관리가 활성화되지 않습니다.,DownloadBundle버지니아 북부 지역에서는 배포 수명 주기 이벤트가 지원되지 않습니다 .,DownloadBundleAppSec 파일의 수명 주기 이벤트 구성이 잘못되었습니다 .,EC2 인스턴스의 IAM 프로필에는 Amazon S3의 애플리케이션 코드에 액세스할 수 있는 권한이 없습니다.,,,0,,
udemy,DVA-02,349,"A company is running an Artificial Intelligence (AI) software for its automotive clients using the AWS Cloud. The software is used for identifying road obstructions for autonomous driving and predicting failure on vehicle components. The company wants to extend its usage and access based on different levels (students, professionals, and hobbyist developers) by exposing an API through API Gateway. The company should regulate access to the API and monetize it by charging based on usage.What should the company do?",D,D,Create three Authorizers to control API access.,"Create three stages and enable CloudWatch metrics. Set up an alarm for each stage according to the ApiName, Method, Resource, and Stage dimensions.",Create three stages. Specify a quota and throttle requests according to the level of access.,Create three Usage Plans. Specify a quota and throttle requests according to the level of access.,,,,"한 회사는 AWS 클라우드를 사용하여 자동차 고객을 위한 인공 지능(AI) 소프트웨어를 실행하고 있습니다. 이 소프트웨어는 자율주행을 위한 도로 장애물을 식별하고 차량 부품의 고장을 예측하는 데 사용됩니다. 회사는 API Gateway를 통해 API를 노출함으로써 다양한 수준(학생, 전문가, 취미 개발자)에 따라 사용 및 액세스를 확장하려고 합니다. 회사는 API에 대한 접근을 규제하고 사용량에 따라 요금을 부과하여 수익을 창출해야 합니다.회사는 어떻게 해야 할까요?",API 액세스를 제어하기 위해 세 개의 승인자를 만듭니다.,"세 가지 단계를 생성하고 CloudWatch 지표를 활성화합니다. ApiName, Method, Resource 및 Stage 차원에 따라 각 단계에 대한 경보를 설정합니다.",3개의 스테이지를 만듭니다. 액세스 수준에 따라 할당량 및 제한 요청을 지정합니다.,세 가지 사용 계획을 만듭니다. 액세스 수준에 따라 할당량 및 제한 요청을 지정합니다.,,,0,,
udemy,DVA-02,350,A developer is building a serverless workflow using Step Functions. The developer has to implement a solution that will help the State Machine handle and recover from State exception errors.Which of the following actions should the developer do?,C,C,Use Catch and Retry fields inside the application code.,"Use a try and catch logic inside the application code to capture the exception error. Then, use a Retry field in the state definition.",Use Catch and Retry fields in the state machine definition.,"Use a Catch field inside the application code to capture the exception error. Then, use a Retry field in the state definition.",,,,개발자가 Step Functions를 사용하여 서버리스 워크플로를 구축하고 있습니다. 개발자는 상태 머신이 예외 오류를 처리하고 복구하는 데 도움이 되는 솔루션을 구현해야 합니다 State.다음 중 개발자는 어떤 조치를 취해야 합니까?,애플리케이션 코드 내에서 Catch 및 Retry 필드를 사용하세요.,애플리케이션 코드 내에서 try and catch 논리를 사용하여 예외 오류를 캡처합니다. 그런 다음 상태 정의에서 Retry 필드를 사용합니다.,상태 머신 정의에서 Catch 및 Retry 필드를 사용합니다.,예외 오류를 캡처하려면 애플리케이션 코드 내의 Catch 필드를 사용하세요. 그런 다음 상태 정의에서 Retry 필드를 사용합니다.,,,0,,
udemy,DVA-02,351,A startup has recently opened an AWS account to develop a cloud-native web application. The CEO wants to improve the security of the account by implementing the best practices in managing access keys in AWS.Which actions follow the security best practices in IAM? (Select TWO.),AB,AB,Use IAM roles for applications that need access to AWS services.,Delete any access keys to your AWS account root user.,Maintain at least one access key for your AWS account root user,Regularly rotate the credentials for all the account users except for the administrator user for tracking purposes.,Save the access key in your application code for convenience.,,,한 스타트업은 최근 클라우드 네이티브 웹 애플리케이션을 개발하기 위해 AWS 계정을 개설했습니다. CEO는 AWS에서 액세스 키 관리에 대한 모범 사례를 구현하여 계정 보안을 강화하려고 합니다.IAM의 보안 모범 사례를 따르는 작업은 무엇입니까? (2개를 선택하세요.),AWS 서비스에 액세스해야 하는 애플리케이션에 IAM 역할을 사용합니다.,AWS 계정 루트 사용자에 대한 액세스 키를 삭제합니다.,AWS 계정 루트 사용자에 대한 액세스 키를 하나 이상 유지하십시오.,추적 목적으로 관리자 사용자를 제외한 모든 계정 사용자의 자격 증명을 정기적으로 교체합니다.,편의를 위해 애플리케이션 코드에 액세스 키를 저장하세요.,,0,,
udemy,DVA-02,352,A developer plans to use AWS Elastic Beanstalk to deploy a microservice application. The application will be implemented in a multi-container Docker environment.How should the developer configure the container definitions in the environment?,C,C,Configure the container definitions in the Dockerrun.aws.json.config and put it inside the .ebextensions folder.,Configure the container definitions in the Amazon ECS Console when building the Docker environment.,Configure the container definitions in the Dockerrun.aws.json file.,Use the eb config command to configure the container definitions.,,,,개발자는 AWS Elastic Beanstalk를 사용하여 마이크로서비스 애플리케이션을 배포할 계획입니다. 애플리케이션은 다중 컨테이너 Docker 환경에서 구현됩니다.개발자는 환경에서 컨테이너 정의를 어떻게 구성해야 합니까?,컨테이너 정의를 구성하고 Dockerrun.aws.json.config.ebextensions 폴더 안에 넣습니다.,Docker 환경을 구축할 때 Amazon ECS 콘솔에서 컨테이너 정의를 구성합니다.,파일 에서 컨테이너 정의를 구성합니다 Dockerrun.aws.json.,명령을 사용하여 eb config컨테이너 정의를 구성합니다.,,,0,,
udemy,DVA-02,353,"A company is running an e-commerce application on an Amazon EC2 instance. A newly hired developer has been tasked to monitor and handle the necessary updates on the EC2 instance every Saturday. The developer is working from home and needs remote access to the webserver. As the system administrator, you’re looking to use the AWS STS API to give the developer temporary credentials and enforce Multi-factor Authentication (MFA) to protect specific programmatic calls against the instance that could adversely affect the server.Which of the following STS API should you use?",D,D,GetFederationToken,AssumeRoleWithSAML,AssumeRoleWithWebIdentity,GetSessionToken,,,,한 회사가 Amazon EC2 인스턴스에서 전자상거래 애플리케이션을 실행하고 있습니다. 새로 고용된 개발자는 매주 토요일 EC2 인스턴스에 필요한 업데이트를 모니터링하고 처리하는 임무를 맡았습니다. 개발자는 집에서 작업 중이며 웹 서버에 대한 원격 액세스가 필요합니다. 시스템 관리자는 AWS STS API를 사용하여 개발자에게 임시 자격 증명을 제공하고 다중 요소 인증(MFA)을 시행하여 서버에 부정적인 영향을 미칠 수 있는 인스턴스에 대해 특정 프로그래밍 호출을 보호하려고 합니다.다음 중 어떤 STS API를 사용해야 합니까?,GetFederationToken,AssumeRoleWithSAML,AssumeRoleWithWebIdentity,GetSessionToken,,,0,,
udemy,DVA-02,354,A Software Engineer is developing a Node.js application that will be deployed using Elastic Beanstalk. The application source code is currently inside a folder called MyApp. He wants to add a configuration file named tutorialsdojo.config to the application.Where should the file be placed?,C,C,Inside the .elasticbeanstalk folder,Inside the MyApp folder at the root level,Inside the .ebextensions folder,Inside the package.json,,,,소프트웨어 엔지니어가 Elastic Beanstalk를 사용하여 배포할 Node.js 애플리케이션을 개발하고 있습니다. 애플리케이션 소스 코드는 현재 MyApp이라는 폴더 안에 있습니다. tutorialsdojo.config그는 애플리케이션에 이름이 지정된 구성 파일을 추가하려고 합니다 .파일을 어디에 배치해야 합니까?,.elasticbeanstalk 폴더 내부,루트 수준의 MyApp 폴더 내부,.ebextensions 폴더 내부,package.json 내부,,,0,,
udemy,DVA-02,355,"An application uses the PutObject operation in parallel to upload hundreds of thousands of objects per second to an S3 bucket. To meet security compliance, the developer uses the server-side encryption in AWS KMS (SSE-KMS) to encrypt objects as they get stored in the S3 bucket. There is a noticeable performance degradation after making the change.Which of the following is the most likely cause of the problem?",D,D,The KMS is not using an alias to easily identify the CMK required for the server-side encryption with AWS KMS (SSE-KMS.),"The CMK is using an AES 256 algorithm, which makes the encryption process slower. AES 128 should be used instead.",The Amazon S3 throttles the PutObject operation for objects encrypted with SSE-KMS.,The API request rate has exceeded the quota for AWS KMS API operations.,,,,애플리케이션은 이 PutObject작업을 병렬로 사용하여 초당 수십만 개의 객체를 S3 버킷에 업로드합니다. 보안 규정 준수를 충족하기 위해 개발자는 AWS KMS(SSE-KMS)의 서버 측 암호화를 사용하여 S3 버킷에 저장되는 객체를 암호화합니다. 변경 후 성능이 눈에 띄게 저하됩니다.다음 중 문제의 원인일 가능성이 가장 높은 것은 무엇입니까?,KMS는 AWS KMS(SSE-KMS)를 사용한 서버 측 암호화에 필요한 CMK를 쉽게 식별하기 위해 별칭을 사용하지 않습니다.,CMK는 AES 256 알고리즘을 사용하므로 암호화 프로세스가 느려집니다. 대신 AES 128을 사용해야 합니다.,Amazon S3는 PutObjectSSE-KMS로 암호화된 객체에 대한 작업을 제한합니다.,API 요청 비율이 AWS KMS API 작업 할당량을 초과했습니다.,,,0,,
udemy,DVA-02,356,A developer is building a serverless application that will send out a newsletter to customers using AWS Lambda. The Lambda function will be invoked at a 7-day interval.Which method will provide an automated and serverless approach to trigger the function?,B,B,Add an environment variable named DAYS for the Lambda function and set its value to 7.,Configure an Amazon CloudWatch Events rule that triggers every week to invoke the Lambda function.,Implement a task timer using Step Functions that will send a newsletter every week.,Run a cron job in an Amazon EC2 instance that will trigger the Lambda function every week.,,,,개발자는 AWS Lambda를 사용하여 고객에게 뉴스레터를 보내는 서버리스 애플리케이션을 구축하고 있습니다. Lambda 함수는 7일 간격으로 호출됩니다.기능을 트리거하기 위해 자동화된 서버리스 접근 방식을 제공하는 방법은 무엇입니까?,Lambda 함수에 DAYS라는 환경 변수를 추가하고 해당 값을 7로 설정합니다.,매주 트리거되어 Lambda 함수를 호출하는 Amazon CloudWatch Events 규칙을 구성합니다.,매주 뉴스레터를 보내는 Step Functions를 사용하여 작업 타이머를 구현합니다.,매주 Lambda 함수를 트리거하는 Amazon EC2 인스턴스에서 cron 작업을 실행합니다.,,,0,,
udemy,DVA-02,357,"A developer is using the AWS CLI to interact with different AWS services. An UnauthorizedOperation error, as shown below, is received after running the stop-instance command:Along with the response is an additional failure message displayed in ciphertext format.How can the developer decode the message?",C,C,Decode the message by calling the AWS KMS decrypt command.,Decode the message using an external cryptography library.,Decode the message by calling the AWS STS decode-authorization-message command.,Decode the message by calling the AWS IAM decode-authorization-message command.,,,,개발자는 AWS CLI를 사용하여 다양한 AWS 서비스와 상호 작용합니다. UnauthorizedOperation명령 을 실행한 후 아래와 같은 오류가 수신 됩니다 stop-instance.응답과 함께 암호문 형식으로 추가 실패 메시지가 표시됩니다.개발자는 메시지를 어떻게 해독할 수 있나요?,AWS KMS decrypt명령을 호출하여 메시지를 디코딩합니다.,외부 암호화 라이브러리를 사용하여 메시지를 디코딩합니다.,AWS STS decode-authorization-message명령을 호출하여 메시지를 디코딩합니다.,AWS IAM decode-authorization-message명령을 호출하여 메시지를 디코딩합니다.,,,0,,
udemy,DVA-02,358,A developer is writing a web application that will allow users to save and retrieve images in an Amazon S3 bucket. The users are required to register and log in to access the application.Which combination of AWS Services should the Developer utilize for implementing the user authentication module of the application?,D,D,Amazon Cognito User Pools and AWS Key Management Service (KMS),Amazon User Pools and AWS Security Token Service (STS),Amazon Cognito Identity Pools and IAM Role.,Amazon Cognito Identity Pools and User Pools.,,,,개발자는 사용자가 Amazon S3 버킷에 이미지를 저장하고 검색할 수 있는 웹 애플리케이션을 작성하고 있습니다. 사용자는 애플리케이션에 액세스하려면 등록하고 로그인해야 합니다.개발자는 애플리케이션의 사용자 인증 모듈을 구현하기 위해 어떤 AWS 서비스 조합을 활용해야 합니까?,Amazon Cognito 사용자 풀 및 AWS Key Management Service(KMS),Amazon 사용자 풀 및 AWS Security Token Service(STS),Amazon Cognito 자격 증명 풀 및 IAM 역할.,Amazon Cognito 자격 증명 풀 및 사용자 풀.,,,0,,
udemy,DVA-02,359,A company plans to conduct an online survey to distinguish the users who bought its product from those who didn't. The survey will be processed by Step Functions which comprises four states that will manage the application logic and error handling of the state machine. It is required to aggregate all the data that passes through the nodes if the process fails.What should the company do to meet the requirements?,A,A,"Include a Catch field in the state machine definition to capture the error. Then, use ResultPath to include each node’s input data with its output.","Include a Parameters field in the state machine definition to capture the errors. Then, use ResultPath to include each node's input data with its output.","Include a Catch field in the state machine definition to capture the errors. Then, use ItemsPath to include each node's input data with its output.","Include a Parameters field in the state machine definition to capture the errors. Then, use ItemsPath to include each node's input data with its output.",,,,한 회사는 자사 제품을 구매한 사용자와 구매하지 않은 사용자를 구별하기 위해 온라인 설문조사를 실시할 계획입니다. 설문 조사는 상태 시스템의 오류 처리 및 애플리케이션 논리를 관리하는 4가지 상태로 구성된 Step Functions에 의해 처리됩니다. 프로세스가 실패할 경우 노드를 통과하는 모든 데이터를 집계해야 합니다.요구 사항을 충족하려면 회사에서 무엇을 해야 합니까?,Catch오류를 캡처하려면 상태 머신 정의에 필드를 포함하세요 . 그런 다음 ResultPath각 노드의 입력 데이터를 출력에 포함하는 데 사용합니다.,Parameters오류를 캡처하려면 상태 머신 정의에 필드를 포함하세요 . 그런 다음 ResultPath각 노드의 입력 데이터를 출력에 포함하는 데 사용합니다.,Catch오류를 캡처하려면 상태 머신 정의에 필드를 포함하세요 . 그런 다음 ItemsPath각 노드의 입력 데이터를 출력에 포함하는 데 사용합니다.,Parameters오류를 캡처하려면 상태 머신 정의에 필드를 포함하세요 . 그런 다음 ItemsPath각 노드의 입력 데이터를 출력에 포함하는 데 사용합니다.,,,0,,
udemy,DVA-02,360,A company is planning to launch an online cross-platform game that expects millions of users. The developer wants to use an in-house authentication system for user identification. Each user identifier must be kept consistent across devices and platforms.How can the developer achieve this?,C,C,Generate a universally unique identifier (UUID) for each device. Store the UUID with the user in a DynamoDB table.,Generate a unique IAM access key for each user and use the access key ID as the unique identifier.,Use developer-authenticated identities in Amazon Cognito to generate unique identifiers for the users.,Create an IAM Role for each user and use its Amazon Resource Name (ARN) as unique identifiers.,,,,한 회사가 수백만 명의 사용자를 기대하는 온라인 크로스 플랫폼 게임을 출시할 계획입니다. 개발자는 사용자 식별을 위해 사내 인증 시스템을 사용하려고 합니다. 각 사용자 식별자는 기기와 플랫폼 전반에서 일관되게 유지되어야 합니다.개발자는 이를 어떻게 달성할 수 있나요?,각 장치에 대해 UUID(Universally Unique Identifier)를 생성합니다. DynamoDB 테이블에 사용자와 함께 UUID를 저장합니다.,각 사용자에 대해 고유한 IAM 액세스 키를 생성하고 액세스 키 ID를 고유 식별자로 사용합니다.,Amazon Cognito에서 개발자 인증 자격 증명을 사용하여 사용자에 대한 고유 식별자를 생성합니다.,각 사용자에 대한 IAM 역할을 생성하고 Amazon 리소스 이름(ARN)을 고유 식별자로 사용합니다.,,,0,,
udemy,DVA-02,361,"A company uses a Linux, Apache, MySQL, and PHP (LAMP) web service stack to host an on-premises application for its car rental business. The manager wants to move its operation into the Cloud using Amazon Web Services.Which combination of services could be used to run the application that will require the least amount of configuration?",B,B,Amazon ECS and Amazon EFS,Amazon EC2 and Amazon Aurora,Amazon S3 and Amazon CloudFront,Amazon API Gateway and Amazon RDS,,,,"회사는 Linux, Apache, MySQL 및 PHP(LAMP) 웹 서비스 스택을 사용하여 자동차 렌탈 사업을 위한 온프레미스 애플리케이션을 호스팅합니다. 관리자는 Amazon Web Services를 사용하여 운영을 클라우드로 이전하려고 합니다.최소한의 구성이 필요한 애플리케이션을 실행하는 데 사용할 수 있는 서비스 조합은 무엇입니까?",Amazon ECS 및 Amazon EFS,아마존 EC2와 아마존 오로라,Amazon S3 및 Amazon CloudFront,Amazon API Gateway 및 Amazon RDS,,,0,,
udemy,DVA-02,362,A developer is writing an application that will download hundreds of media files. Each file must be encrypted with a unique encryption key within the application before storing it in an S3 bucket. The developer needs a cost-effective solution with low management overhead.Which of the following is the most suitable solution?,A,A,Use the GenerateDataKey API command to generate a data key for each file to encrypt them. Store the encrypted data key and the file.,Use the CreateKey API command to generate a CMK for each file to encrypt them.,Enable the SSE-S3 for the S3 bucket. Directly store the files in the bucket.,Use an open-source key generator to produce a unique key. Use the key to encrypt the files.,,,,개발자가 수백 개의 미디어 파일을 다운로드하는 애플리케이션을 작성하고 있습니다. 각 파일은 S3 버킷에 저장하기 전에 애플리케이션 내에서 고유한 암호화 키로 암호화되어야 합니다. 개발자에게는 관리 오버헤드가 낮은 비용 효율적인 솔루션이 필요합니다.다음 중 가장 적합한 솔루션은 무엇입니까?,API 명령을 사용하여 GenerateDataKey각 파일에 대한 데이터 키를 생성하여 암호화합니다. 암호화된 데이터 키와 파일을 저장합니다.,API 명령을 사용하여 CreateKey각 파일에 대한 CMK를 생성하여 암호화합니다.,S3 버킷에 대해 SSE-S3를 활성화합니다. 버킷에 파일을 직접 저장합니다.,오픈 소스 키 생성기를 사용하여 고유 키를 생성하세요. 키를 사용하여 파일을 암호화합니다.,,,0,,
udemy,DVA-02,363,A developer has an EC2 instance deployed in a test environment. He will be using the aws iam create-role command to create a role that will allow the instance to list the S3 buckets in a particular region.What should be done first before the EC2 instance can assume the role?,D,D,Add the EC2 service under the Resource field of the Inline policy,Add the EC2 service under the Principal field of the Inline policy,Add the EC2 service under the Resource field of the Trust policy,Add the EC2 service under the Principal field of the Trust policy,,,,개발자는 테스트 환경에 EC2 인스턴스를 배포했습니다. 그는 명령을 사용하여 aws iam create-role인스턴스가 특정 지역의 S3 버킷을 나열할 수 있도록 하는 역할을 생성할 것입니다.EC2 인스턴스가 역할을 맡으려면 먼저 무엇을 수행해야 합니까?,인라인 정책의 리소스 필드 아래에 EC2 서비스를 추가합니다.,인라인 정책의 Principal 필드 아래에 EC2 서비스를 추가합니다.,신뢰 정책의 리소스 필드 아래에 EC2 서비스를 추가합니다.,신뢰 정책의 Principal 필드 아래에 EC2 서비스를 추가합니다.,,,0,,
udemy,DVA-02,364,A developer is managing several microservices built using API Gateway and AWS Lambda. The Developer wants to deploy new updates to one of the APIs. He wants to ensure a smooth transition between the versions by giving users enough time to migrate to the new version before retiring the previous one.Which solution should the developer implement?,C,C,"Implement the updates and publish a new version of the Lambda function. Then, issue the new Lambda invocation URL.",Implement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target resource then redeploy it to the same stage.,Implement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target resource then redeploy it to a new stage.,Implement the updates on the Lambda function. Create a CloudFront distribution and use the function as the origin.,,,,개발자는 API Gateway 및 AWS Lambda를 사용하여 구축된 여러 마이크로서비스를 관리하고 있습니다. 개발자는 API 중 하나에 새 업데이트를 배포하려고 합니다. 그는 이전 버전을 사용 중지하기 전에 사용자에게 새 버전으로 마이그레이션할 수 있는 충분한 시간을 제공하여 버전 간 원활한 전환을 보장하려고 합니다.개발자는 어떤 솔루션을 구현해야 합니까?,업데이트를 구현하고 Lambda 함수의 새 버전을 게시합니다. 그런 다음 새 Lambda 호출 URL을 발급합니다.,업데이트를 구현하고 Lambda 함수의 새 버전을 게시합니다. API Gateway 대상 리소스에 새 버전을 지정한 후 동일한 단계에 다시 배포합니다.,업데이트를 구현하고 Lambda 함수의 새 버전을 게시합니다. API Gateway 대상 리소스에 새 버전을 지정한 후 새 단계에 다시 배포합니다.,Lambda 함수에 대한 업데이트를 구현합니다. CloudFront 배포를 생성하고 해당 함수를 오리진으로 사용합니다.,,,0,,
udemy,DVA-02,365,"A serverless application is created to process numerous files. Each invocation takes 5 minutes to complete. The Lambda function's execution time is too slow for the application.Considering that the Lambda function does not return any important data, which method will accelerate data processing the most?",D,D,Use synchronous RequestResponse Lambda invocations. Process the files one by one.,"Compress the files to reduce their size, then process them with synchronous RequestResponse Lambda invocations.",Merge all of the files into a single file then process them all at once with an asynchronous Event Lambda invocation.,Use asynchronous Event Lambda invocations. Configure the function to process the files in parallel.,,,,수많은 파일을 처리하기 위해 서버리스 애플리케이션이 생성됩니다. 각 호출을 완료하는 데 5분이 소요됩니다. 애플리케이션에 비해 Lambda 함수의 실행 시간이 너무 느립니다.Lambda 함수가 중요한 데이터를 반환하지 않는다는 점을 고려하면 어떤 방법이 데이터 처리를 가장 가속화합니까?,동기식 RequestResponseLambda 호출을 사용합니다. 파일을 하나씩 처리합니다.,파일을 압축하여 크기를 줄인 다음 동기식 RequestResponseLambda 호출로 처리합니다.,모든 파일을 단일 파일로 병합한 다음 비동기식 Event Lambda 호출을 통해 한 번에 처리합니다.,비동기식 Event Lambda 호출을 사용합니다. 파일을 병렬로 처리하도록 기능을 구성합니다.,,,0,,
udemy,DVA-02,366,"A developer uses AWS Serverless Application Model (SAM) in a local machine to create a serverless Python application. After defining the required dependencies in the requirements.txt file, the developer is now ready to test and deploy.What are the steps to successfully deploy the application?",A,A,Build the SAM template in the local machine and call the sam deploy command to package and deploy the SAM template from an S3 bucket.,Upload and build the SAM template in an EC2 instance. Run the sam deploy command to package and deploy the SAM template.,Build the SAM template in the local machine. Run the sam deploy command to package and deploy the SAM template from AWS CodeCommit.,Run the sam init command. Build the SAM template in the local machine and call the sam deploy command to package and deploy the SAM template from an S3 bucket.,,,,개발자는 로컬 시스템에서 AWS Serverless Application Model(SAM)을 사용하여 서버리스 Python 애플리케이션을 생성합니다. 파일 에 필요한 종속성을 정의한 후 requirements.txt개발자는 이제 테스트하고 배포할 준비가 되었습니다.애플리케이션을 성공적으로 배포하기 위한 단계는 무엇입니까?,로컬 시스템에서 SAM 템플릿을 빌드하고 sam deploy명령을 호출하여 S3 버킷에서 SAM 템플릿을 패키징하고 배포합니다.,EC2 인스턴스에 SAM 템플릿을 업로드하고 빌드합니다. 명령을 실행하여 sam deploySAM 템플릿을 패키징하고 배포합니다.,로컬 시스템에서 SAM 템플릿을 빌드합니다. 명령을 실행하여 sam deployAWS CodeCommit에서 SAM 템플릿을 패키징하고 배포합니다.,명령을 실행하십시오 sam init. 로컬 시스템에서 SAM 템플릿을 빌드하고 sam deploy명령을 호출하여 S3 버킷에서 SAM 템플릿을 패키징하고 배포합니다.,,,0,,
udemy,DVA-02,367,"A Lambda function is being developed to process a 50MB gzip-compressed file that will be uploaded to an S3 bucket on a daily basis. The function must have access to a storage location where it can load and unzip the file. After processing, the file will be delivered to another S3 bucket.Which solution can the developer implement that requires the LEAST effort and cost?",D,D,"Download the file to an Amazon Elastic Block Store (EBS) volume. From there, consume and process the data before sending it to the S3 bucket.","Download the file to a separate S3 bucket. From there, consume and process the data before sending it to the S3 bucket that contains the logs.","Download the file to Amazon Elastic File System. From there, consume and process the data before sending it to the S3 bucket.","Download the file to the /tmp directory. From there, consume and process the data before sending it to the S3 bucket.",,,,매일 S3 버킷에 업로드되는 50MB gzip 압축 파일을 처리하기 위해 Lambda 기능이 개발되고 있습니다. 함수는 파일을 로드하고 압축을 풀 수 있는 저장 위치에 액세스할 수 있어야 합니다. 처리 후 파일은 다른 S3 버킷으로 전달됩니다.개발자가 최소한의 노력과 비용으로 구현할 수 있는 솔루션은 무엇입니까?,파일을 Amazon Elastic Block Store(EBS) 볼륨에 다운로드합니다. 여기에서 데이터를 S3 버킷으로 보내기 전에 소비하고 처리합니다.,파일을 별도의 S3 버킷에 다운로드합니다. 여기에서 로그가 포함된 S3 버킷으로 데이터를 보내기 전에 데이터를 소비하고 처리합니다.,Amazon Elastic File System에 파일을 다운로드합니다. 여기에서 데이터를 S3 버킷으로 보내기 전에 소비하고 처리합니다.,해당 디렉터리에 파일을 다운로드합니다 /tmp. 여기에서 데이터를 S3 버킷으로 보내기 전에 소비하고 처리합니다.,,,0,,
udemy,DVA-02,368,"A developer wants to expose a legacy web service that uses an XML-based Simple Object Access Protocol (SOAP) interface through API Gateway. However, there is a compatibility issue since most modern applications communicate data in JSON format.Which is the most cost-effective method that will overcome this issue?",C,C,Use API Gateway to create a RESTful API. Send the incoming JSON to an HTTP server hosted on an EC2 instance and have it transform the data into XML and vice versa before sending it to the legacy application.,Use API Gateway to create a RESTful API. Transform the incoming JSON into XML for the SOAP interface through an Application Load Balancer and vice versa. Put the legacy web service behind the ALB.,Use API Gateway to create a RESTful API. Transform the incoming JSON into XML using mapping templates. Forward the request into the SOAP interface by using a Lambda function and parse the response (XML) into JSON before sending back to API Gateway.,Use API Gateway to create a WebSocket API. Transform the incoming JSON into XML using mapping templates. Forward the request into the SOAP interface by using a Lambda function and parse the response (XML) into JSON before sending back to API Gateway.,,,,개발자는 API 게이트웨이를 통해 XML 기반 SOAP(Simple Object Access Protocol) 인터페이스를 사용하는 레거시 웹 서비스를 노출하려고 합니다. 그러나 대부분의 최신 애플리케이션은 JSON 형식으로 데이터를 통신하므로 호환성 문제가 있습니다.이 문제를 극복할 수 있는 가장 비용 효율적인 방법은 무엇입니까?,API 게이트웨이를 사용하여 RESTful API를 생성합니다. 수신 JSON을 EC2 인스턴스에 호스팅된 HTTP 서버로 보내고 데이터를 XML로 변환하거나 그 반대로 변환한 후 레거시 애플리케이션으로 전송합니다.,API 게이트웨이를 사용하여 RESTful API를 생성합니다. Application Load Balancer를 통해 수신 JSON을 SOAP 인터페이스용 XML로 변환하거나 그 반대로 변환합니다. ALB 뒤에 레거시 웹 서비스를 배치합니다.,API 게이트웨이를 사용하여 RESTful API를 생성합니다. .NET을 사용하여 수신 JSON을 XML로 변환합니다 mapping templates. API 게이트웨이로 다시 보내기 전에 Lambda 함수를 사용하여 요청을 SOAP 인터페이스로 전달하고 응답(XML)을 JSON으로 구문 분석합니다.,API 게이트웨이를 사용하여 WebSocket API를 생성합니다. .NET을 사용하여 수신 JSON을 XML로 변환합니다 mapping templates. API 게이트웨이로 다시 보내기 전에 Lambda 함수를 사용하여 요청을 SOAP 인터페이스로 전달하고 응답(XML)을 JSON으로 구문 분석합니다.,,,0,,
udemy,DVA-02,369,"A developer needs to view the percentage of used memory and the number of TCP connections of instances inside an Auto Scaling Group. To achieve this, the developer must send the metrics to Amazon CloudWatch.Which approach provides the MOST secure way of authenticating a CloudWatch PUT request?",D,D,Create an IAM user with programmatic access. Attach a cloudwatch:PutMetricDatapermission and store the access key and secret key in the instance’s configuration file.,Create an IAM user with programmatic access. Attach a cloudwatch:PutMetricData permission and update the Auto Scaling launch configuration to insert the access key and secret key into the instance user data.,Modify the existing Auto Scaling launch configuration to use an IAM role with the cloudwatch:PutMetricData permission for the instances.,Create an IAM role with cloudwatch:PutMetricData permission for the new Auto Scaling launch configuration from which you launch instances.,,,,개발자는 Auto Scaling 그룹 내 인스턴스의 사용된 메모리 비율과 인스턴스의 TCP 연결 수를 확인해야 합니다. 이를 달성하려면 개발자는 지표를 Amazon CloudWatch로 보내야 합니다.CloudWatch PUT 요청을 인증하는 가장 안전한 방법을 제공하는 접근 방식은 무엇입니까?,프로그래밍 방식으로 액세스할 수 있는 IAM 사용자를 생성합니다. 권한 을 연결 cloudwatch:PutMetricData하고 인스턴스 구성 파일에 액세스 키와 비밀 키를 저장합니다.,프로그래밍 방식으로 액세스할 수 있는 IAM 사용자를 생성합니다. 권한 을 연결 cloudwatch:PutMetricData하고 Auto Scaling 시작 구성을 업데이트하여 액세스 키와 비밀 키를 인스턴스 사용자 데이터에 삽입합니다.,cloudwatch:PutMetricData인스턴스에 대한 권한이 있는 IAM 역할을 사용하도록 기존 Auto Scaling 시작 구성을 수정합니다 .,cloudwatch:PutMetricData인스턴스를 시작하는 새로운 Auto Scaling 시작 구성에 대한 권한이 있는 IAM 역할을 생성합니다 .,,,0,,
udemy,DVA-02,370,Several development teams worldwide will be collaboratively working on a project hosted on an AWS Elastic Beanstalk environment. The developers need to be able to deploy incremental code updates without re-uploading the entire project.Which of the following actions will reduce the upload and deployment time with the LEAST amount of effort?,B,B,Host the code repository on an EC2 instance and allow access to all the developers. Write a script that will automate the deployment process to Elastic Beanstalk.,Create an AWS CodeCommit repository and allow access to all developers. Deploy the code to Elastic Beanstalk.,Upload the code to an Amazon EFS mounted on an EC2 instance. Write a script that will automate the deployment process to Elastic Beanstalk.,Configure event notifications on a central S3 bucket and allow access to all developers. Invoke a Lambda Function that will deploy the code to Elastic Beanstalk when a PUT event occurs.,,,,전 세계 여러 개발 팀이 AWS Elastic Beanstalk 환경에서 호스팅되는 프로젝트를 공동으로 작업할 예정입니다. 개발자는 전체 프로젝트를 다시 업로드하지 않고도 증분 코드 업데이트를 배포할 수 있어야 합니다.다음 중 최소한의 노력으로 업로드 및 배포 시간을 줄이는 작업은 무엇입니까?,EC2 인스턴스에 코드 리포지토리를 호스팅하고 모든 개발자에게 액세스를 허용합니다. Elastic Beanstalk에 대한 배포 프로세스를 자동화하는 스크립트를 작성합니다.,AWS CodeCommit 리포지토리를 생성하고 모든 개발자에게 액세스를 허용합니다. Elastic Beanstalk에 코드를 배포합니다.,EC2 인스턴스에 탑재된 Amazon EFS에 코드를 업로드합니다. Elastic Beanstalk에 대한 배포 프로세스를 자동화하는 스크립트를 작성합니다.,중앙 S3 버킷에 이벤트 알림을 구성하고 모든 개발자에게 액세스를 허용합니다. PUT 이벤트가 발생할 때 Elastic Beanstalk에 코드를 배포하는 Lambda 함수를 호출합니다.,,,0,,
udemy,DVA-02,371,"A Development team is building a fault-tolerant solution for a web application hosted on Amazon EC2. The application’s session data is primarily stored in an Amazon ElastiCache for Redis cluster. Whenever session data is requested, the application initially checks the local cache on the EC2 instance. If the data isn't available locally, the application fetches it from the Redis cluster and then caches it locally to reduce latency for subsequent requests.The solution aims to ensure that no user requests are lost during a session in case an EC2 instance is terminated or has failed a health check.Which solution best fits the requirement with the least effort?",B,B,Use the DynamoDB Session Handler to save session data.,Use an Elastic Load Balancer and configure sticky sessions.,Use an Elastic Load Balancer and configure connection draining.,Create an SQS queue to store session data.,,,,개발 팀은 Amazon EC2에서 호스팅되는 웹 애플리케이션을 위한 내결함성 솔루션을 구축하고 있습니다. 애플리케이션의 세션 데이터는 주로 Redis용 Amazon ElastiCache 클러스터에 저장됩니다. 세션 데이터가 요청될 때마다 애플리케이션은 처음에 EC2 인스턴스의 로컬 캐시를 확인합니다. 데이터를 로컬에서 사용할 수 없는 경우 애플리케이션은 Redis 클러스터에서 데이터를 가져온 다음 로컬로 캐시하여 후속 요청에 대한 대기 시간을 줄입니다.이 솔루션은 EC2 인스턴스가 종료되거나 상태 확인에 실패한 경우 세션 중에 사용자 요청이 손실되지 않도록 하는 것을 목표로 합니다.최소한의 노력으로 요구 사항에 가장 적합한 솔루션은 무엇입니까?,DynamoDB 세션 핸들러를 사용하여 세션 데이터를 저장합니다.,Elastic Load Balancer를 사용하고 고정 세션을 구성하세요.,Elastic Load Balancer를 사용하고 연결 드레이닝을 구성합니다.,세션 데이터를 저장할 SQS 대기열을 생성합니다.,,,0,,
udemy,DVA-02,372,"A startup plans to use Amazon Cognito User Pools to easily manage their users' sign-up and sign-in workflows to an application. To save time from designing the User Interface (UI) for the login page, the development team has decided to use Cognito's built-in UI. However, the product manager finds the UI bland and instructed the developer to include the product logo on the web page.How should the developer meet the above requirements?",C,C,Create a login page with the product logo and upload it to an S3 bucket. Point the S3 endpoint in the Cognito app settings.,Create a login page with the product logo and upload it to Amazon Cognito.,Upload the logo to the Amazon Cognito app settings and use that logo on the custom login page.,Upload the logo to an S3 bucket and point the S3 endpoint on the custom login page.,,,,한 스타트업에서는 Amazon Cognito 사용자 풀을 사용하여 사용자의 애플리케이션 가입 및 로그인 워크플로를 쉽게 관리할 계획입니다. 로그인 페이지의 사용자 인터페이스(UI) 설계 시간을 절약하기 위해 개발팀은 Cognito에 내장된 UI를 사용하기로 결정했습니다. 그러나 제품 관리자는 UI가 단조롭다고 판단하고 개발자에게 웹 페이지에 제품 로고를 포함하도록 지시했습니다.개발자는 위의 요구 사항을 어떻게 충족해야 합니까?,제품 로고가 포함된 로그인 페이지를 생성하고 S3 버킷에 업로드합니다. Cognito 앱 설정에서 S3 엔드포인트를 가리킵니다.,제품 로고가 포함된 로그인 페이지를 생성하고 Amazon Cognito에 업로드합니다.,Amazon Cognito 앱 설정에 로고를 업로드하고 사용자 정의 로그인 페이지에서 해당 로고를 사용하십시오.,S3 버킷에 로고를 업로드하고 사용자 정의 로그인 페이지에서 S3 엔드포인트를 가리킵니다.,,,0,,
udemy,DVA-02,373,"An application executes GET operations to various AWS services. The development team is using AWS X-Ray to trace all the calls made to AWS. As one of the developers, you are responsible for maintaining a particular block of code on the application. To save time, you only want to record data associated with the code to group the traces in the AWS console.Which of the following X-Ray features should you use?",A,A,Annotations,Subsegment,Metadata,Sampling,,,,애플리케이션은 다양한 AWS 서비스에 대해 GET 작업을 실행합니다. 개발팀은 AWS X-Ray를 사용하여 AWS에 대한 모든 호출을 추적하고 있습니다. 개발자 중 한 명으로서 귀하는 애플리케이션의 특정 코드 블록을 유지 관리할 책임이 있습니다. 시간을 절약하려면 코드와 관련된 데이터만 기록하여 AWS 콘솔에서 추적을 그룹화하려고 합니다.다음 X-Ray 기능 중 어떤 기능을 사용해야 합니까?,주석,하위 세그먼트,Metadata,견본 추출,,,0,,
udemy,DVA-02,374,"A Ruby developer is looking to offload some of the processing on his application to the AWS cloud without managing any servers. The submodules must be written in Ruby, which mainly invokes API calls to an external web service. The response from the API call is parsed and stored in a MongoDB database.What should he do to develop the Lambda function in his preferred programming language?",B,B,Create a Lambda function with a custom runtime to use Ruby. Then include the runtime in the function's deployment package. Migrate it to a layer that you manage independently from the function.,Create a Lambda function with a supported runtime version for Ruby.,Create a Lambda function on Ruby with a custom runtime and use the AWS SDK for Ruby.,Create a Lambda function using the AWS SDK for Ruby.,,,,Ruby 개발자는 서버를 관리하지 않고 자신의 애플리케이션 처리 중 일부를 AWS 클라우드로 오프로드하려고 합니다. 하위 모듈은 주로 외부 웹 서비스에 대한 API 호출을 호출하는 Ruby로 작성되어야 합니다. API 호출의 응답은 구문 분석되어 MongoDB 데이터베이스에 저장됩니다.그가 선호하는 프로그래밍 언어로 Lambda 함수를 개발하려면 어떻게 해야 합니까?,Ruby를 사용하려면 사용자 지정 런타임으로 Lambda 함수를 생성하세요. 그런 다음 함수의 배포 패키지에 런타임을 포함합니다. 기능과 독립적으로 관리하는 레이어로 마이그레이션하세요.,지원되는 Ruby용 런타임 버전으로 Lambda 함수를 생성합니다.,사용자 지정 런타임을 사용하여 Ruby에서 Lambda 함수를 생성하고 Ruby용 AWS SDK를 사용합니다.,Ruby용 AWS SDK를 사용하여 Lambda 함수를 생성합니다.,,,0,,
udemy,DVA-02,375,"A development team needs to deploy an application revision into three environments: Test, Staging, and Production. The application should be deployed into the Test environment first, then Staging, and then Production.Which approach will conveniently allow the team to deploy the application into different environments?",C,C,"Create, configure, and deploy multiple application projects for each environment using CodeBuild.",Create multiple data pipeline provisions for each environment to deploy the application using the AWS Data Pipeline.,Create multiple deployment groups for each environment using AWS CodeDeploy.,Create a repository for each environment in AWS CodeCommit to deploy the application.,,,,"개발 팀은 테스트, 준비, 프로덕션의 세 가지 환경에 애플리케이션 개정을 배포해야 합니다. 애플리케이션을 먼저 테스트 환경에 배포한 다음 스테이징, 프로덕션 순으로 배포해야 합니다.팀이 애플리케이션을 다양한 환경에 편리하게 배포할 수 있는 접근 방식은 무엇입니까?","CodeBuild를 사용하여 각 환경에 대한 여러 애플리케이션 프로젝트를 생성, 구성 및 배포합니다.",AWS Data Pipeline을 사용하여 애플리케이션을 배포하려면 각 환경에 대해 여러 데이터 파이프라인 프로비저닝을 생성하세요.,AWS CodeDeploy를 사용하여 각 환경에 대해 여러 배포 그룹을 생성합니다.,AWS CodeCommit에서 각 환경에 대한 리포지토리를 생성하여 애플리케이션을 배포합니다.,,,0,,
udemy,DVA-02,376,A developer is managing an Application Load Balancer that targets a Lambda function. The developer needs to obtain all values of identical query parameters key that is supplied in a request.How can the developer implement this?,A,A,Enable the multi-value headers on the Application Load Balancer.,Decode the URL encoded query string values in the Lambda function.,Replace the Application Load Balancer with a Classical Load Balancer and enable multi-value headers.,Set a custom HTTP response header in the Lambda function.,,,,개발자는 Lambda 함수를 대상으로 하는 Application Load Balancer를 관리하고 있습니다. 개발자는 요청에 제공된 동일한 쿼리 매개변수 키의 모든 값을 얻어야 합니다.개발자는 이를 어떻게 구현할 수 있나요?,Application Load Balancer에서 다중 값 헤더를 활성화합니다.,Lambda 함수에서 URL로 인코딩된 쿼리 문자열 값을 디코딩합니다.,Application Load Balancer를 Classical Load Balancer로 교체하고 다중 값 헤더를 활성화합니다.,Lambda 함수에서 사용자 지정 HTTP 응답 헤더를 설정합니다.,,,0,,
udemy,DVA-02,377,"An application is used to upload images to an Amazon S3 bucket. Once an event occurs, a Lambda function is triggered to compress the photos. However, it has been discovered that the processing time of the function is longer than expected.Which change will improve the processing time of the function most effectively?",D,D,Configure the S3 bucket to send notifications to an SQS queue. Use the SQS queue with the Lambda function to process the image.,Increase the timeout value of the function.,"Run the function with Lambda@Edge which will run the code closer to the users of your application, reducing your application’s latency.",Increase the memory allocation of the function.,,,,애플리케이션은 Amazon S3 버킷에 이미지를 업로드하는 데 사용됩니다. 이벤트가 발생하면 Lambda 함수가 트리거되어 사진을 압축합니다. 그러나 함수 처리 시간이 예상보다 길어지는 것으로 확인되었습니다.함수 처리 시간을 가장 효과적으로 개선할 수 있는 변경 사항은 무엇입니까?,SQS 대기열에 알림을 보내도록 S3 버킷을 구성합니다. Lambda 함수와 함께 SQS 대기열을 사용하여 이미지를 처리합니다.,함수의 시간 초과 값을 늘립니다.,애플리케이션 사용자에게 더 가까운 곳에서 코드를 실행하는 Lambda@Edge로 함수를 실행하여 애플리케이션의 지연 시간을 줄이세요.,함수의 메모리 할당을 늘립니다.,,,0,,
udemy,DVA-02,378,"Private documents have to be securely stored in an S3 Standard-IA bucket. These documents must be encrypted at rest, and the encryption keys should be rotated every 365 days.Which encryption method is the easiest to implement?",A,A,Use a customer managed KMS key and enable automatic annual key rotation.,Generate a symmetric key using an external library and use that to encrypt the data before sending it to the S3 bucket. Write a script that will automate the key rotation.,Use OpenSSL to generate an encryption key and import it into AWS KMS with automatic annual key rotation enabled.,Use the AWS owned key for S3 to encrypt data.,,,,개인 문서는 S3 Standard-IA 버킷에 안전하게 저장되어야 합니다. 이러한 문서는 저장 시 암호화되어야 하며 암호화 키는 365일마다 교체되어야 합니다.구현하기 가장 쉬운 암호화 방법은 무엇입니까?,고객 관리 KMS 키를 사용하고 자동 연간 키 순환을 활성화합니다.,외부 라이브러리를 사용하여 대칭 키를 생성하고 이를 사용하여 데이터를 S3 버킷으로 보내기 전에 암호화합니다. 키 순환을 자동화하는 스크립트를 작성하세요.,OpenSSL을 사용하여 암호화 키를 생성하고 자동 연간 키 교체가 활성화된 상태에서 AWS KMS로 가져옵니다.,S3용 AWS 소유 키를 사용하여 데이터를 암호화합니다.,,,0,,
udemy,DVA-02,379,A developer is building an application that uses Amazon CloudFront to distribute thousands of images stored in an S3 bucket. The developer needs a fast and cost-efficient solution that will allow him to update the images immediately without waiting for the object’s expiration date.Which solution meets the requirements?,D,D,Upload the new images in the S3 bucket and wait for the objects in the edge locations to expire to reflect the changes.,Update the images by invalidating them from the edge caches.,Disable the CloudFront distribution and re-enable it to update the images in all edge locations.,Update the images by using versioned file names.,,,,개발자는 Amazon CloudFront를 사용하여 S3 버킷에 저장된 수천 개의 이미지를 배포하는 애플리케이션을 구축하고 있습니다. 개발자에게는 객체의 만료 날짜를 기다리지 않고 즉시 이미지를 업데이트할 수 있는 빠르고 비용 효율적인 솔루션이 필요합니다.어떤 솔루션이 요구 사항을 충족합니까?,S3 버킷에 새 이미지를 업로드하고 엣지 로케이션의 객체가 만료되어 변경 사항이 반영될 때까지 기다립니다.,엣지 캐시에서 이미지를 무효화하여 업데이트합니다.,CloudFront 배포를 비활성화하고 다시 활성화하여 모든 엣지 로케이션의 이미지를 업데이트합니다.,버전이 지정된 파일 이름을 사용하여 이미지를 업데이트합니다.,,,0,,
udemy,DVA-02,380,"A San Francisco-based tech startup is building a cross-platform mobile app that can notify the user of upcoming astronomical events. Your mobile app authenticates with the Identity Provider (IdP) using the provider's SDK and Amazon Cognito. Once the end-user is authenticated with the IdP, the OAuth or OpenID Connect token returned from the IdP is passed by your app to Amazon Cognito.Which of the following is returned for the user to provide a set of temporary, limited-privilege AWS credentials?",D,D,Cognito API,Cognito Key Pair,Cognito SDK,Cognito ID,,,,샌프란시스코에 본사를 둔 한 기술 스타트업은 사용자에게 다가오는 천문학적 사건을 알릴 수 있는 크로스 플랫폼 모바일 앱을 구축하고 있습니다. 모바일 앱은 공급자의 SDK 및 Amazon Cognito를 사용하여 ID 공급자(IdP)를 인증합니다. 최종 사용자가 IdP로 인증되면 IdP에서 반환된 OAuth 또는 OpenID Connect 토큰이 앱을 통해 Amazon Cognito로 전달됩니다.다음 중 사용자가 권한이 제한된 임시 AWS 자격 증명 세트를 제공하기 위해 반환되는 것은 무엇입니까?,API 알기,코그니토 키 쌍,SDK 알아보기,아이디를 알면,,,0,,
udemy,DVA-02,381,"A developer is building an AWS Lambda-based Java application that optimizes pictures uploaded to an S3 bucket. Upon running several tests, the Lambda function shows a cold start of about 5 seconds.Which of the following could the developer do to reduce the cold start time? (Select TWO.)",AB,AB,Increase the memory allocation setting for the Lambda function.,Reduce the deployment package’s size by including only the needed modules from the AWS SDK for Java.,Run the Lambda function in a VPC to gain access to Amazon’s high-end infrastructure.,Increase the timeout setting for the Lambda function.,Add the Spring Framework to the project and enable dependency injection.,,,개발자는 S3 버킷에 업로드된 사진을 최적화하는 AWS Lambda 기반 Java 애플리케이션을 구축하고 있습니다. 여러 테스트를 실행하면 Lambda 함수는 약 5초의 콜드 스타트를 보여줍니다.다음 중 콜드 스타트 ​​시간을 줄이기 위해 개발자가 수행할 수 있는 작업은 무엇입니까? (2개를 선택하세요.),Lambda 함수에 대한 메모리 할당 설정을 늘립니다.,Java용 AWS SDK에서 필요한 모듈만 포함하여 배포 패키지의 크기를 줄입니다.,Amazon의 고급 인프라에 액세스하려면 VPC에서 Lambda 함수를 실행하세요.,Lambda 함수에 대한 제한 시간 설정을 늘립니다.,프로젝트에 Spring Framework를 추가하고 종속성 주입을 활성화합니다.,,0,,
udemy,DVA-02,382,A university is gradually migrating some of its physical documents to the AWS cloud. They will start by moving their alumnus' historical records to Amazon S3. The storage solution should provide a secure and durable object storage with the lowest cost.Which of the following types of S3 storage should you recommend?,B,B,Amazon S3 One-Zone,Amazon S3 Glacier Deep Archive,Amazon S3 Glacier,Amazon S3 Infrequent Access,,,,한 대학에서는 물리적 문서 중 일부를 AWS 클라우드로 점진적으로 마이그레이션하고 있습니다. 졸업생의 기록 기록을 Amazon S3로 옮기는 것부터 시작합니다. 스토리지 솔루션은 최저 비용으로 안전하고 내구성이 뛰어난 개체 스토리지를 제공해야 합니다.다음 중 어떤 유형의 S3 스토리지를 권장해야 합니까?,Amazon S3 원존,Amazon S3 Glacier Deep 아카이브,아마존 S3 빙하,Amazon S3 자주 액세스하지 않음,,,0,,
udemy,DVA-02,383,"A developer is building an application with Amazon DynamoDB as its database. The application needs to group the PUT, UPDATE, and DELETE actions into a single all-or-nothing operation to make changes against multiple items in the DynamoDB table.Which DynamoDB operation should the developer use?",D,D,BatchWriteItem,Scan,Query,TransactWriteItems,,,,"개발자는 Amazon DynamoDB를 데이터베이스로 사용하여 애플리케이션을 구축하고 있습니다. DynamoDB 테이블의 여러 항목을 변경하려면 애플리케이션이 , 및 작업을 하나의 전부 아니면 전무 작업으로 그룹화 PUT해야 UPDATE합니다 DELETE.개발자는 어떤 DynamoDB 작업을 사용해야 합니까?",BatchWriteItem,Scan,Query,TransactWriteItems,,,0,,
udemy,DVA-02,384,"An EC2 instance has an IAM role that explicitly denies all S3 API Write operations. Moreover, the instance has access key credentials configured to gain full access to S3 operations.Which statement is correct for this scenario?",C,C,The instance can perform all S3 operations except for write operations on any S3 bucket.,The instance can list all S3 buckets but will not be able to delete them.,The instance can perform all S3 operations on any S3 bucket.,The instance cannot upload objects to S3 buckets.,,,,EC2 인스턴스에는 모든 S3 API 쓰기 작업을 명시적으로 거부하는 IAM 역할이 있습니다. 또한 인스턴스에는 S3 작업에 대한 전체 액세스 권한을 얻도록 구성된 액세스 키 자격 증명이 있습니다.이 시나리오에 대한 설명으로 옳은 것은 무엇입니까?,인스턴스는 S3 버킷에 대한 쓰기 작업을 제외한 모든 S3 작업을 수행할 수 있습니다.,인스턴스는 모든 S3 버킷을 나열할 수 있지만 삭제할 수는 없습니다.,인스턴스는 모든 S3 버킷에서 모든 S3 작업을 수행할 수 있습니다.,인스턴스는 S3 버킷에 객체를 업로드할 수 없습니다.,,,0,,
udemy,DVA-02,385,"Some static assets stored in an S3 bucket need to be accessed by a user on the development account. The S3 bucket is in the production account. According to the company policy, the sharing of full credentials between accounts is prohibited.What steps should be done to delegate access across the two accounts? (Select THREE.)",CDF,CDF,Log in to the production account and create a policy that will use STS to assume the IAM role in the development account. Attach the policy to the IAM users.,"On the development account, create an IAM role and specify the production account as a trusted entity.","On the production account, create an IAM role and specify the development account as a trusted entity.",Set the policy that will grant access to S3 for the IAM role created in the production account.,Set the policy that will grant access to S3 for the IAM role created in the development account.,Log in to the development account and create a policy that will use STS to assume the IAM role in the production account. Attach the policy to the IAM users.,,S3 버킷에 저장된 일부 정적 자산은 개발 계정의 사용자가 액세스해야 합니다. S3 버킷은 프로덕션 계정에 있습니다. 회사 정책에 따라 계정 간 전체 자격 증명을 공유하는 것은 금지되어 있습니다.두 계정에 걸쳐 액세스 권한을 위임하려면 어떤 단계를 수행해야 합니까? (3개를 선택하세요.),프로덕션 계정에 로그인하고 STS를 사용하여 개발 계정에서 IAM 역할을 맡는 정책을 생성합니다. IAM 사용자에게 정책을 연결합니다.,개발 계정에서 IAM 역할을 생성하고 프로덕션 계정을 신뢰할 수 있는 엔터티로 지정합니다.,프로덕션 계정에서 IAM 역할을 생성하고 개발 계정을 신뢰할 수 있는 엔터티로 지정합니다.,프로덕션 계정에서 생성된 IAM 역할에 대해 S3에 대한 액세스 권한을 부여하는 정책을 설정합니다.,개발 계정에서 생성된 IAM 역할에 대해 S3에 대한 액세스 권한을 부여하는 정책을 설정합니다.,,0,,개발 계정에 로그인하고 STS를 사용하여 프로덕션 계정에서 IAM 역할을 맡는 정책을 생성합니다. IAM 사용자에게 정책을 연결합니다.
udemy,DVA-02,386,"An update was made on an AWS Lambda-based application. It is invoked by an API Gateway endpoint with caching enabled to improve latency requests. The developer expected to get the latest data as a response when he tested the application. However, he kept getting stale data upon trying many times.What should the developer do that will require the LEAST amount of effort to resolve the issue? (Select TWO.)",BE,BE,Include Cache-Control: no-cache HTTP header on the API request.,Include Cache-Control: max-age=0 HTTP header on the API request.,Set the new endpoint as a trigger for the lambda function.,Create a new REST API endpoint and disable caching.,Grant permission to the client to invalidate caching when there’s a request using the IAM execution role.,,,AWS Lambda 기반 애플리케이션이 업데이트되었습니다. 대기 시간 요청을 개선하기 위해 캐싱이 활성화된 API 게이트웨이 엔드포인트에 의해 호출됩니다. 개발자는 애플리케이션을 테스트할 때 응답으로 최신 데이터를 얻을 것으로 기대했습니다. 그러나 그는 여러 번 시도한 결과 계속해서 오래된 데이터를 얻었습니다.문제를 해결하기 위해 최소한의 노력이 필요한 개발자는 무엇을 해야 합니까? (2개를 선택하세요.),Cache-Control: no-cacheAPI 요청에 HTTP 헤더를 포함합니다 .,Cache-Control: max-age=0API 요청에 HTTP 헤더를 포함합니다 .,새 엔드포인트를 람다 함수의 트리거로 설정합니다.,새 REST API 엔드포인트를 생성하고 캐싱을 비활성화합니다.,IAM 실행 역할을 사용하여 요청이 있는 경우 캐싱을 무효화할 수 있는 권한을 클라이언트에 부여합니다.,,0,,
udemy,DVA-02,387,"A mobile game developer is using DynamoDB as a data store and a Web Identity Federation for authorization and authentication. Each item in the DynamoDB table contains the attributes for individual user's game data such as user ID, game scores, and top score where the user ID is the partition key. The developer must control user access to specific data items based on their IDs. In doing so, users will only be able to obtain items that they own.Which of the following solutions must be implemented by the developer?",D,D,Modify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:Select condition key to the user IDs.,Modify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:ReturnValues condition key to the user IDs.,Modify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:Attributes condition key to the user IDs.,Modify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:LeadingKeys condition key to the user IDs.,,,,"모바일 게임 개발자는 DynamoDB를 데이터 스토어로 사용하고, 권한 부여 및 인증을 위해 웹 자격 증명 연동을 사용하고 있습니다. DynamoDB 테이블의 각 항목에는 사용자 ID, 게임 점수, 사용자 ID가 파티션 키인 최고 점수 등 개별 사용자의 게임 데이터에 대한 속성이 포함되어 있습니다. 개발자는 ID를 기반으로 특정 데이터 항목에 대한 사용자 액세스를 제어해야 합니다. 이를 통해 사용자는 자신이 소유한 아이템만 얻을 수 있습니다.다음 중 개발자가 구현해야 하는 솔루션은 무엇입니까?",dynamodb:Select조건 키 값을 사용자 ID로 설정하여 ID 공급자의 역할과 연결된 IAM 정책을 수정합니다 .,dynamodb:ReturnValues조건 키 값을 사용자 ID로 설정하여 ID 공급자의 역할과 연결된 IAM 정책을 수정합니다 .,dynamodb:Attributes조건 키 값을 사용자 ID로 설정하여 ID 공급자의 역할과 연결된 IAM 정책을 수정합니다 .,dynamodb:LeadingKeys조건 키 값을 사용자 ID로 설정하여 ID 공급자의 역할과 연결된 IAM 정책을 수정합니다 .,,,0,,
udemy,DVA-02,388,"An application has a feature that displays GIFs based on keyword inputs. The code streams random GIF links from an external API to your local machine. When run, the application's process takes longer than expected. You are suspecting that the new function sendRequest() you added is the culprit.Which of the following actions should you do to determine the latency of the function?",C,C,Use CloudTrail to record and store event logs for actions made by your function.,"Using CloudWatch, troubleshoot the issue by checking the logs.","Using AWS X-Ray, define an arbitrary subsegment inside the code to instrument the function.","Using AWS X-Ray, disable sampling to efficiently trace all requests for calls.",,,,애플리케이션에는 키워드 입력에 따라 GIF를 표시하는 기능이 있습니다. 코드는 외부 API에서 로컬 시스템으로 임의의 GIF 링크를 스트리밍합니다. 실행 시 애플리케이션 프로세스가 예상보다 오래 걸립니다. 당신은 당신이 추가한 새로운 기능이 sendRequest()원인이라고 의심하고 있습니다.다음 중 함수의 지연 시간을 확인하려면 어떤 조치를 취해야 합니까?,CloudTrail을 사용하여 함수에서 수행한 작업에 대한 이벤트 로그를 기록하고 저장합니다.,CloudWatch를 사용하여 로그를 확인하여 문제를 해결합니다.,AWS X-Ray를 사용하여 코드 내에 임의의 하위 세그먼트를 정의하여 함수를 계측합니다.,AWS X-Ray를 사용하면 샘플링을 비활성화하여 모든 호출 요청을 효율적으로 추적할 수 있습니다.,,,0,,
udemy,DVA-02,389,"An application that runs on a local Linux server is migrated to a Lambda function to save costs. The Lambda function shows an Unable to import module error when invoked.As the developer, how can you fix the error?",C,C,Import the missing modules in the Lambda code. The Lambda function will automatically install them when invoked.,Download the missing modules in the lib directory. Package all files under the lib directory into a ZIP file and upload it to AWS Lambda.,Install the missing modules locally to your application’s folder. Package the folder into a ZIP file and upload it to AWS Lambda.,Run a Linux command inside the Lambda function to install the missing modules.,,,,로컬 Linux 서버에서 실행되는 애플리케이션은 비용 절감을 위해 Lambda 함수로 마이그레이션됩니다. Lambda 함수를 Unable to import module호출하면 오류가 표시됩니다.개발자로서 오류를 어떻게 수정할 수 있나요?,Lambda 코드에서 누락된 모듈을 가져옵니다. Lambda 함수가 호출되면 자동으로 설치됩니다.,lib 디렉터리에서 누락된 모듈을 다운로드합니다. lib 디렉터리 아래의 모든 파일을 ZIP 파일로 패키징하고 AWS Lambda에 업로드합니다.,누락된 모듈을 애플리케이션 폴더에 로컬로 설치하십시오. 폴더를 ZIP 파일로 패키징하고 AWS Lambda에 업로드합니다.,Lambda 함수 내에서 Linux 명령을 실행하여 누락된 모듈을 설치합니다.,,,0,,
udemy,DVA-02,390,"A developer is building a serverless URL shortener using Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. The application code as well as the stack that defines the cloud resources should be written in Python. The code should also be reusable in case an update must be done to the stack.Which of the following actions must be done by the developer to meet the requirements above?",A,A,"Use AWS CDK to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.","Use AWS CloudFormation to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.","Use AWS Cloud9 to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.","Use AWS SDK for Python (boto3) to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.",,,,"개발자는 Amazon API Gateway, Amazon DynamoDB 및 AWS Lambda를 사용하여 서버리스 URL 단축기를 구축하고 있습니다. 애플리케이션 코드와 클라우드 리소스를 정의하는 스택은 Python으로 작성되어야 합니다. 스택을 업데이트해야 하는 경우에도 코드를 재사용할 수 있어야 합니다.위의 요구 사항을 충족하려면 다음 중 개발자가 수행해야 하는 작업은 무엇입니까?",AWS CDK를 사용하여 스택을 구축합니다. 그런 다음 Lambda에서 애플리케이션 로직을 작성할 때 Python을 런타임 환경으로 사용합니다.,AWS CloudFormation을 사용하여 스택을 구축합니다. 그런 다음 Lambda에서 애플리케이션 로직을 작성할 때 Python을 런타임 환경으로 사용합니다.,AWS Cloud9을 사용하여 스택을 구축하십시오. 그런 다음 Lambda에서 애플리케이션 로직을 작성할 때 Python을 런타임 환경으로 사용합니다.,Python용 AWS SDK(boto3)를 사용하여 스택을 빌드합니다. 그런 다음 Lambda에서 애플리케이션 로직을 작성할 때 Python을 런타임 환경으로 사용합니다.,,,0,,
